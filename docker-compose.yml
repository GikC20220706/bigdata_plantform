version: '3.8'

services:
  # Redis for caching
  redis:
    image: redis:7-alpine
    container_name: bigdata-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - bigdata-network

  # Main application
  bigdata-platform:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: bigdata-platform
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      # Application settings
      - DEBUG=false
      - HOST=0.0.0.0
      - PORT=8000
      - IS_LOCAL_DEV=false
      - MOCK_DATA_MODE=${MOCK_DATA_MODE:-false}

      # Redis configuration
      - REDIS_URL=redis://redis:6379/0

      # Cache configuration
      - CACHE_TTL=${CACHE_TTL:-300}
      - METRICS_CACHE_TTL=${METRICS_CACHE_TTL:-30}
      - METRICS_SSH_TIMEOUT=${METRICS_SSH_TIMEOUT:-5}
      - METRICS_MAX_CONCURRENT=${METRICS_MAX_CONCURRENT:-5}

      # Database configuration
      - DATABASE_URL=${DATABASE_URL:-sqlite:///./data/bigdata_platform.db}

      # Hadoop cluster configuration
      - HADOOP_HOME=${HADOOP_HOME:-/opt/hadoop}
      - HDFS_NAMENODE=${HDFS_NAMENODE:-hdfs://hadoop101:8020}
      - HDFS_USER=${HDFS_USER:-bigdata}

      # Hive configuration
      - HIVE_SERVER_HOST=${HIVE_SERVER_HOST:-hadoop101}
      - HIVE_SERVER_PORT=${HIVE_SERVER_PORT:-10000}
      - HIVE_USERNAME=${HIVE_USERNAME:-bigdata}
      - HIVE_PASSWORD=${HIVE_PASSWORD:-gqdw8862}

      # Flink configuration
      - FLINK_JOBMANAGER_HOST=${FLINK_JOBMANAGER_HOST:-hadoop101}
      - FLINK_JOBMANAGER_PORT=${FLINK_JOBMANAGER_PORT:-8081}

      # Doris configuration
      - DORIS_FE_HOST=${DORIS_FE_HOST:-hadoop101}
      - DORIS_FE_PORT=${DORIS_FE_PORT:-8060}
      - DORIS_USERNAME=${DORIS_USERNAME:-root}
      - DORIS_PASSWORD=${DORIS_PASSWORD:-1qaz@WSX3edc}

      # SSH and cluster access configuration
      - SSH_KEY_PATH=/app/ssh_keys/id_rsa
      - SSH_USERNAME=${SSH_USERNAME:-bigdata}
      - SSH_PASSWORD=${SSH_PASSWORD:-}
      - SSH_PORT=${SSH_PORT:-22}
      - SSH_TIMEOUT=${SSH_TIMEOUT:-10}

      # Node configuration
      - CLUSTER_NODES_CONFIG_FILE=/app/config/nodes.json
      - CLUSTER_NODES_CONFIG_JSON=${CLUSTER_NODES_CONFIG_JSON:-}

      # Security
      - SECRET_KEY=${SECRET_KEY:-your-secret-key-change-in-production}

      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FILE=/app/logs/bigdata_platform.log

      # File uploads
      - UPLOAD_DIR=/app/uploads
      - MAX_UPLOAD_SIZE=${MAX_UPLOAD_SIZE:-52428800}

    volumes:
      # Persistent data
      - ./logs:/app/logs
      - ./uploads:/app/uploads
      - ./data:/app/data

      # Configuration files
      - ./config/nodes.json:/app/config/nodes.json:ro
      - ./config:/app/config:ro

      # SSH keys for cluster access (create this directory and add your keys)
      - ./ssh_keys:/app/ssh_keys:ro

      # Optional: Mount additional configuration
      - ./.env:/app/.env:ro

    depends_on:
      redis:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/overview/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    networks:
      - bigdata-network

  # Nginx reverse proxy for production
  nginx:
    image: nginx:alpine
    container_name: bigdata-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - bigdata-platform
    networks:
      - bigdata-network

volumes:
  redis_data:
    driver: local

networks:
  bigdata-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16