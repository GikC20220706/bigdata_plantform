services:
  # MySQL数据库
  mysql:
    image: mysql:8.0
    container_name: bigdata-mysql
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: bigdata_platform
      MYSQL_USER: bigdata
      MYSQL_PASSWORD: bigdata123
      # 优化MySQL配置
      MYSQL_INNODB_BUFFER_POOL_SIZE: 256M
      MYSQL_INNODB_LOG_FILE_SIZE: 64M
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./config/mysql/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    command: >
      --default-authentication-plugin=mysql_native_password
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_unicode_ci
      --innodb-buffer-pool-size=256M
      --innodb-log-file-size=64M
      --max-connections=200
      --max-allowed-packet=64M
    healthcheck:
      test: [ "CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "bigdata", "-pbigdata123" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - bigdata-network

  # Redis缓存
  redis:
    image: redis:7-alpine
    container_name: bigdata-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      - ./config/redis/redis.conf:/etc/redis/redis.conf:ro
    command: >
      redis-server /etc/redis/redis.conf
      --appendonly yes
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - bigdata-network

  # 主应用
  bigdata-platform:
    image: bigdata-platform:3.2.1-aarch64
    user: "1000:1000"
    container_name: bigdata-platform
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      # 应用基础设置
      - DEBUG=false
      - HOST=0.0.0.0
      - PORT=8000
      - IS_LOCAL_DEV=false
      - MOCK_DATA_MODE=${MOCK_DATA_MODE:-false}

      # 数据库配置
      - DATABASE_URL=mysql+aiomysql://bigdata:bigdata123@mysql:3306/bigdata_platform?charset=utf8mb4
      - DAG_FOLDER=/opt/airflow/dags
      - DAG_TEMPLATES_FOLDER=/opt/airflow/dag_templates
      # Redis配置
      - REDIS_URL=redis://redis:6379/0
      - REDIS_HOST=redis
      - REDIS_PORT=6379

      # 缓存配置
      - CACHE_TTL=${CACHE_TTL:-300}
      - CACHE_OVERVIEW_EXPIRE=${CACHE_OVERVIEW_EXPIRE:-60}
      - CACHE_CLUSTER_EXPIRE=${CACHE_CLUSTER_EXPIRE:-30}
      - CACHE_METRICS_EXPIRE=${CACHE_METRICS_EXPIRE:-15}
      - METRICS_CACHE_TTL=${METRICS_CACHE_TTL:-30}
      - METRICS_SSH_TIMEOUT=${METRICS_SSH_TIMEOUT:-10}
      - METRICS_MAX_CONCURRENT=${METRICS_MAX_CONCURRENT:-5}

      # Hadoop集群配置
      - HADOOP_HOME=${HADOOP_HOME:-/opt/hadoop}
      - HDFS_NAMENODE=${HDFS_NAMENODE:-hdfs://hadoop101:8020}
      - HDFS_USER=${HDFS_USER:-bigdata}
      - HADOOP_NAMENODE_HOST=${HADOOP_NAMENODE_HOST:-192.168.1.100}
      - HADOOP_NAMENODE_PORT=${HADOOP_NAMENODE_PORT:-9000}

      # Hive配置
      - HIVE_SERVER_HOST=${HIVE_SERVER_HOST:-hadoop101}
      - HIVE_SERVER_PORT=${HIVE_SERVER_PORT:-10000}
      - HIVE_USERNAME=${HIVE_USERNAME:-bigdata}
      - HIVE_PASSWORD=${HIVE_PASSWORD:-gqdw8862}
      - HIVE_HOST=${HIVE_HOST:-192.168.1.100}
      - HIVE_PORT=${HIVE_PORT:-10000}

      # Flink配置
      - FLINK_JOBMANAGER_HOST=${FLINK_JOBMANAGER_HOST:-hadoop101}
      - FLINK_JOBMANAGER_PORT=${FLINK_JOBMANAGER_PORT:-8081}

      # Doris配置
      - DORIS_FE_HOST=${DORIS_FE_HOST:-hadoop101}
      - DORIS_FE_PORT=${DORIS_FE_PORT:-8060}
      - DORIS_USERNAME=${DORIS_USERNAME:-root}
      - DORIS_PASSWORD=${DORIS_PASSWORD:-1qaz@WSX3edc}

      # SSH配置
      - SSH_KEY_PATH=/app/ssh_keys/id_rsa
      - SSH_USERNAME=${SSH_USERNAME:-bigdata}
      - SSH_PASSWORD=${SSH_PASSWORD:-}
      - SSH_PORT=${SSH_PORT:-22}
      - SSH_TIMEOUT=${SSH_TIMEOUT:-10}

      # 集群节点配置
      - CLUSTER_NODES_CONFIG_FILE=/app/config/nodes.json
      - CLUSTER_NODES_CONFIG_JSON=${CLUSTER_NODES_CONFIG_JSON:-}
      - CLUSTER_AUTO_DISCOVERY=${CLUSTER_AUTO_DISCOVERY:-false}

      # 安全配置
      - SECRET_KEY=${SECRET_KEY:-your-secret-key-change-in-production}

      # 日志配置
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FILE=/app/logs/bigdata_platform.log

      # 文件上传配置
      - UPLOAD_DIR=/app/uploads
      - MAX_UPLOAD_SIZE=${MAX_UPLOAD_SIZE:-52428800}

      # 时区设置
      - TZ=Asia/Shanghai

      # Hadoop环境变量（匹配你的服务器配置）
      #- JAVA_HOME=/opt/jdk
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/etc/hadoop
      - DATAX_HOME=/opt/datax
      - PATH=/opt/hadoop/bin:/opt/hadoop/sbin:/opt/jdk/bin:/usr/local/bin:/usr/bin:/bin

      # JVM 优化参数
      - JAVA_OPTS=-Xss8m -Xms4g -Xmx16g -Dfastjson.parser.safeMode=true -Dfastjson2.parser.safeMode=true
      - DATAX_JVM_OPTS=-Xss8m -Xms4g -Xmx16g -Dfastjson.parser.safeMode=true


      # FastJSON 安全配置
      #- FASTJSON_PARSER_SAFEMODE=true
      #- FASTJSON2_PARSER_SAFEMODE=true
      #- JVM_OPTS=-Dfastjson.parser.safeMode=true -Dfastjson2.parser.safeMode=true
    volumes:
      # 持久化数据
      - ./logs:/app/logs
      - ./uploads:/app/uploads
      - ./data:/app/data

      # 配置文件（只读）
      - ./config/nodes.json:/app/config/nodes.json:ro
      - ./config:/app/config:ro

      # SSH密钥（只读，严格权限）
      - ./ssh_keys:/app/ssh_keys:ro

      # 环境配置（只读）
      - ./.env:/app/.env:ro

      # 时区数据
      - /etc/localtime:/etc/localtime:ro

      # DataX相关挂载 - 移到这里
      - ./datax/configs:/app/datax/jobs
      - ./datax/logs:/app/datax/logs
      - ./datax/scripts:/app/datax/scripts:ro
      - ./datax/datax:/opt/datax:ro
      # 使用服务器datax挂载
      #- /opt/module/datax:/opt/datax:ro
      # 添加以下 Hadoop 挂载
      - /opt/module/hadoop:/opt/hadoop:ro
      - /opt/module/hadoop/etc/hadoop:/etc/hadoop:ro
      #- /opt/module/jdk:/opt/jdk:ro

      - /etc/hosts:/etc/hosts:ro
      # 添加离线swagger ui
      - ./static:/app/static:ro
      - ./airflow/dags:/opt/airflow/dags:rw
      - ./airflow/dag_templates:/opt/airflow/dag_templates:rw

    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy

    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/api/v1/overview/health" ]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 120s

    # 资源限制（防止在生产环境中过度消耗资源）
    deploy:
      resources:
        limits:
          #cpus: '16.0'
          cpus: '2.0'
          #memory: 32G
          memory: 2G
        reservations:
          #cpus: '4.0'
          cpus: '0.5'
          #memory: 8G
          memory: 512M

    networks:
      - bigdata-network

  # Nginx反向代理（生产环境）
  nginx:
    image: nginx:alpine
    container_name: bigdata-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./nginx/logs:/var/log/nginx
    depends_on:
      bigdata-platform:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - bigdata-network

  # DataX执行引擎 - 修正镜像版本
  datax-engine:
    image: openjdk:11-jre-slim  # 改为Java 17
    container_name: bigdata-datax
    restart: unless-stopped
    volumes:
      - ./datax:/opt/datax:ro  # 修正路径
      - ./datax/configs:/opt/datax/configs
      - ./datax/logs:/opt/datax/logs
    environment:
      - DATAX_HOME=/opt/datax
      - JAVA_HOME=/opt/java/openjdk  # 修正Java路径
    networks:
      - bigdata-network
    command: >
      bash -c "
        echo 'Checking Java installation...' &&
        java -version &&
        echo 'Checking DataX installation...' &&
        ls -la /opt/datax/bin/datax.py &&
        chmod +x /opt/datax/bin/datax.py 2>/dev/null || true &&
        echo 'DataX engine ready' &&
        tail -f /dev/null
      "

  

  airflow-init:
    image: bigdata-airflow:latest
    container_name: airflow-init
    depends_on:
      mysql:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://airflow:airflow123@mysql:3306/airflow_db?charset=utf8mb4
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+pymysql://airflow:airflow123@mysql:3306/airflow_db?charset=utf8mb4
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
      TZ: Asia/Shanghai
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/config:/opt/airflow/config
    command: |
      bash -c "
        echo '=== Airflow 初始化开始 ===' &&
        echo '验证 MySQL 驱动...' &&
        python -c 'import pymysql; print(\"✓ PyMySQL 可用\")' &&
        python -c 'import MySQLdb; print(\"✓ MySQLdb 可用\")' &&
        echo '测试数据库连接...' &&
        python -c 'import pymysql; conn = pymysql.connect(host=\"mysql\", user=\"airflow\", password=\"airflow123\", database=\"airflow_db\"); print(\"✓ 数据库连接成功\"); conn.close()' &&
        echo '初始化 Airflow 数据库...' &&
        airflow db init &&
        echo '创建管理员用户...' &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin &&
        echo '=== Airflow 初始化完成 ==='
      "
    networks:
      - bigdata-network

    # Airflow Webserver - 生产优化配置
  airflow-webserver:
    image: bigdata-airflow:latest
    container_name: airflow-webserver
    restart: unless-stopped
    depends_on:
      mysql:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://airflow:airflow123@mysql:3306/airflow_db?charset=utf8mb4
      AIRFLOW__CORE__LOAD_EXAMPLES: false
      AIRFLOW__CORE__FERNET_KEY: ''
      DEPLOYMENT_ENV: ${DEPLOYMENT_ENV:-development}
      TZ: Asia/Shanghai
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      AIRFLOW__WEBSERVER__AUTHENTICATE: 'true'
      AIRFLOW__WEBSERVER__AUTH_BACKEND: 'airflow.auth.backends.password_auth'
      AIRFLOW__API__ENABLE_EXPERIMENTAL_API: 'false'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    ports:
      - "8080:8080"
    command: >
      bash -c "
        if [ \"\$DEPLOYMENT_ENV\" = \"production\" ]; then
          echo '=== 生产环境：启动 Gunicorn webserver ===' &&
          airflow webserver --port 8080 --workers 4 --timeout 600
        else
          echo '=== 开发环境：启动 Flask 开发服务器 ===' &&
          airflow webserver --port 8080 --debug
        fi
      "
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:8080/health || exit 1" ]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s
    networks:
      - bigdata-network
  airflow-scheduler:
    image: bigdata-airflow:latest
    container_name: airflow-scheduler
    restart: unless-stopped
    depends_on:
      mysql:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://airflow:airflow123@mysql:3306/airflow_db?charset=utf8mb4
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+pymysql://airflow:airflow123@mysql:3306/airflow_db?charset=utf8mb4
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
      # Scheduler 优化配置
      AIRFLOW__SCHEDULER__HEARTBEAT_SEC: 5
      AIRFLOW__SCHEDULER__MAX_THREADS: 2
      AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 300
      # 数据库连接池优化
      AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE: 5
      AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_RECYCLE: 3600
      AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_PRE_PING: 'true'
      # 数据底座集成配置
      BIGDATA_PLATFORM_API: http://bigdata-platform:8000
      TZ: Asia/Shanghai
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/config:/opt/airflow/config
      - ./airflow/plugins:/opt/airflow/plugins
      - ./datax:/opt/datax:ro
      - ./scripts:/opt/scripts:ro
    command: >
      bash -c "
        echo '=== 启动 Airflow Scheduler ===' &&
        echo '等待 webserver 就绪...' &&
        sleep 30 &&
        echo '启动 scheduler...' &&
        airflow scheduler
      "
    healthcheck:
      test: [ "CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob", "--hostname", "$(hostname)" ]
      interval: 60s
      timeout: 20s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 512M
    networks:
      - bigdata-network
# 数据卷 - 修正格式
volumes:
  mysql_data:
    driver: local
  redis_data:
    driver: local

# 网络配置
networks:
  bigdata-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
    driver_opts:
      com.docker.network.bridge.name: br-bigdata
      com.docker.network.driver.mtu: 1500
