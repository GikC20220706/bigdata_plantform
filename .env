# 首信云数据底座 - 环境配置文件

# 应用基础配置
DEBUG=false
HOST=0.0.0.0
PORT=8000
APP_NAME=首信云数据底座
VERSION=3.2.1

# 环境标识
IS_LOCAL_DEV=false
MOCK_DATA_MODE=false

# 数据库配置
#DATABASE_URL=sqlite:///./data/bigdata_platform.db
DATABASE_URL=mysql+aiomysql://bigdata:bigdata123@mysql:3306/bigdata_platform?charset=utf8mb4
# 数据库连接池配置
DATABASE_POOL_SIZE=20
DATABASE_POOL_RECYCLE=3600
DATABASE_POOL_PRE_PING=true

# Hadoop配置
HADOOP_HOME=/opt/module/hadoop
HADOOP_CONF_DIR=/opt/module/hadoop/etc/hadoop
HDFS_NAMENODE=hdfs://192.142.76.242:8020
HDFS_USER=bigdata

# Hive配置
HIVE_SERVER_HOST=192.142.76.242
HIVE_SERVER_PORT=10000
HIVE_DATABASE=default
HIVE_USERNAME=bigdata
HIVE_PASSWORD=gqdw8862

# Flink配置
FLINK_JOBMANAGER_HOST=192.142.76.242
FLINK_JOBMANAGER_PORT=8081

# Doris配置
DORIS_FE_HOST=192.142.76.242
DORIS_FE_PORT=8060
DORIS_USERNAME=root
DORIS_PASSWORD=1qaz@WSX3edc

# Redis配置 - 使用内置Redis容器
REDIS_URL=redis://redis:6379/0

# 缓存配置
CACHE_TTL=300
METRICS_CACHE_TTL=30
METRICS_SSH_TIMEOUT=5
METRICS_MAX_CONCURRENT=5
METRICS_ENABLE_CACHE=true

# SSH配置 - 用于连接集群节点
SSH_KEY_PATH=/app/ssh_keys/id_rsa
SSH_USERNAME=bigdata
SSH_PASSWORD=
SSH_PORT=22
SSH_TIMEOUT=10

# 集群节点配置
CLUSTER_NODES_CONFIG_FILE=/app/config/nodes.json

# 数据集成配置
DATA_INTEGRATION_ENABLED=true
MAX_QUERY_RESULTS=1000
CONNECTION_TIMEOUT=30
DEFAULT_POOL_SIZE=10

# 文件上传配置
UPLOAD_DIR=/app/uploads
MAX_UPLOAD_SIZE=52428800

# 日志配置
LOG_LEVEL=INFO
LOG_FILE=/app/logs/bigdata_platform.log

# 安全配置
SECRET_KEY=your-production-secret-key-change-this-in-production

# 监控配置
METRICS_COLLECTION_INTERVAL=60

# Java相关配置
#JAVA_HOME=/usr/lib/jvm/java-11-openjdk-arm64
#DATAX_HOME=/opt/datax

# DataX配置
#DATAX_PYTHON_PATH=/usr/bin/python3

# ==================== Airflow调度配置 ====================

# Airflow连接配置
AIRFLOW_HOST=airflow-webserver
AIRFLOW_PORT=8080
AIRFLOW_USERNAME=admin
AIRFLOW_PASSWORD=admin
AIRFLOW_API_VERSION=v1

# Airflow数据库配置（使用统一的MySQL）
AIRFLOW_DB_HOST=mysql
AIRFLOW_DB_PORT=3306
AIRFLOW_DB_USER=airflow
AIRFLOW_DB_PASSWORD=airflow123
AIRFLOW_DB_NAME=airflow_db

# DAG配置
DAG_FOLDER=./airflow/dags
DAG_TEMPLATES_FOLDER=./airflow/dag_templates
DEFAULT_TASK_TIMEOUT=3600
DEFAULT_RETRY_COUNT=3
DEFAULT_RETRY_DELAY=300

# DataX配置
DATAX_HOME=/opt/datax
DATAX_JOB_PATH=./datax/jobs
DATAX_LOG_PATH=./datax/logs

# 脚本路径配置
SQL_SCRIPTS_PATH=./scripts/sql
SHELL_SCRIPTS_PATH=./scripts/shell

# 数据底座API配置（用于Airflow回调）
BIGDATA_PLATFORM_API=http://bigdata-platform:8000
BIGDATA_PLATFORM_AUTH_TOKEN=your-auth-token-here

# 调度器配置
SCHEDULER_HEARTBEAT_SEC=5
SCHEDULER_MAX_THREADS=2
ENABLE_TASK_LOGS=true
LOG_RETENTION_DAYS=30