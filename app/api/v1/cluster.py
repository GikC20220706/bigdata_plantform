from fastapi import APIRouter, HTTPException, Query
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
import random

from app.schemas.overview import ClusterInfo
from app.utils.response import create_response
from app.utils.hadoop_client import HDFSClient, FlinkClient, DorisClient
from app.utils.metrics_collector import metrics_collector
from config.settings import settings
from loguru import logger
import psutil

router = APIRouter()


@router.get("/", summary="è·å–æ‰€æœ‰é›†ç¾¤çŠ¶æ€")
async def get_all_clusters():
    """è·å–æ‰€æœ‰é›†ç¾¤çš„çŠ¶æ€ä¿¡æ¯"""
    try:
        clusters = []

        # Hadoopé›†ç¾¤
        hadoop_cluster = await get_hadoop_cluster_detail()
        clusters.append(hadoop_cluster)

        # Flinké›†ç¾¤
        flink_cluster = await get_flink_cluster_detail()
        clusters.append(flink_cluster)

        # Dorisé›†ç¾¤
        doris_cluster = await get_doris_cluster_detail()
        clusters.append(doris_cluster)

        return create_response(
            data=clusters,
            message="è·å–é›†ç¾¤çŠ¶æ€æˆåŠŸ"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–é›†ç¾¤çŠ¶æ€å¤±è´¥: {str(e)}"
        )


@router.get("/hadoop", summary="è·å–Hadoopé›†ç¾¤è¯¦æƒ…")
async def get_hadoop_cluster():
    """è·å–Hadoopé›†ç¾¤è¯¦ç»†ä¿¡æ¯"""
    try:
        cluster_info = await get_hadoop_cluster_detail()
        return create_response(
            data=cluster_info,
            message="è·å–Hadoopé›†ç¾¤ä¿¡æ¯æˆåŠŸ"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–Hadoopé›†ç¾¤ä¿¡æ¯å¤±è´¥: {str(e)}"
        )


@router.get("/flink", summary="è·å–Flinké›†ç¾¤è¯¦æƒ…")
async def get_flink_cluster():
    """è·å–Flinké›†ç¾¤è¯¦ç»†ä¿¡æ¯"""
    try:
        cluster_info = await get_flink_cluster_detail()
        return create_response(
            data=cluster_info,
            message="è·å–Flinké›†ç¾¤ä¿¡æ¯æˆåŠŸ"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–Flinké›†ç¾¤ä¿¡æ¯å¤±è´¥: {str(e)}"
        )


@router.get("/doris", summary="è·å–Dorisé›†ç¾¤è¯¦æƒ…")
async def get_doris_cluster():
    """è·å–Dorisé›†ç¾¤è¯¦ç»†ä¿¡æ¯"""
    try:
        cluster_info = await get_doris_cluster_detail()
        return create_response(
            data=cluster_info,
            message="è·å–Dorisé›†ç¾¤ä¿¡æ¯æˆåŠŸ"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–Dorisé›†ç¾¤ä¿¡æ¯å¤±è´¥: {str(e)}"
        )


@router.get("/metrics", summary="è·å–é›†ç¾¤ç›‘æ§æŒ‡æ ‡")
async def get_cluster_metrics(
        cluster_name: Optional[str] = Query(None, description="é›†ç¾¤åç§°"),
        hours: int = Query(24, description="è·å–è¿‡å»Nå°æ—¶çš„æ•°æ®")
):
    """è·å–é›†ç¾¤ç›‘æ§æŒ‡æ ‡çš„å†å²æ•°æ®"""
    try:
        # ç”Ÿæˆå†å²ç›‘æ§æ•°æ®
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=hours)

        # ç”Ÿæˆæ—¶é—´ç‚¹ï¼ˆæ¯å°æ—¶ä¸€ä¸ªç‚¹ï¼‰
        time_points = []
        current_time = start_time
        while current_time <= end_time:
            time_points.append(current_time)
            current_time += timedelta(hours=1)

        # ç”Ÿæˆæ¨¡æ‹Ÿçš„ç›‘æ§æ•°æ®
        metrics_data = {
            "time_series": [t.strftime("%Y-%m-%d %H:%M") for t in time_points],
            "cpu_usage": [round(random.uniform(30, 80), 1) for _ in time_points],
            "memory_usage": [round(random.uniform(40, 85), 1) for _ in time_points],
            "disk_usage": [round(random.uniform(50, 75), 1) for _ in time_points],
            "network_io": [round(random.uniform(100, 500), 1) for _ in time_points]
        }

        if cluster_name:
            metrics_data["cluster_name"] = cluster_name

        return create_response(
            data=metrics_data,
            message="è·å–é›†ç¾¤ç›‘æ§æŒ‡æ ‡æˆåŠŸ"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–é›†ç¾¤ç›‘æ§æŒ‡æ ‡å¤±è´¥: {str(e)}"
        )


@router.get("/nodes", summary="è·å–é›†ç¾¤èŠ‚ç‚¹è¯¦æƒ…")
async def get_cluster_nodes(
        cluster_type: str = Query(..., description="é›†ç¾¤ç±»å‹: hadoop/flink/doris")
):
    """è·å–æŒ‡å®šé›†ç¾¤çš„èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯ - ä½¿ç”¨çœŸå®æ•°æ®"""
    try:
        if cluster_type.lower() not in ["hadoop", "flink", "doris"]:
            raise HTTPException(
                status_code=400,
                detail="ä¸æ”¯æŒçš„é›†ç¾¤ç±»å‹ï¼Œæ”¯æŒçš„ç±»å‹: hadoop, flink, doris"
            )

        # è·å–çœŸå®çš„é›†ç¾¤æŒ‡æ ‡
        cluster_metrics = await metrics_collector.get_cluster_metrics(cluster_type.lower())

        nodes = []

        if cluster_metrics.get('nodes'):
            for node_info in cluster_metrics['nodes']:
                metrics = node_info.get('metrics', {})

                # åŸºç¡€èŠ‚ç‚¹ä¿¡æ¯
                base_node = {
                    "node_name": node_info['name'],
                    "ip_address": node_info['host'],
                    "role": node_info['role'],
                    "status": "æ­£å¸¸" if metrics.get('success') else "å¼‚å¸¸",
                    "cpu_usage": metrics.get('cpu_usage', 0.0),
                    "memory_usage": metrics.get('memory_usage', 0.0),
                    "disk_usage": metrics.get('disk_usage', 0.0),
                    "load_average": metrics.get('load_average', 0.0),
                    "process_count": metrics.get('process_count', 0),
                    "last_heartbeat": datetime.now() - timedelta(seconds=random.randint(5, 30)),
                    "uptime": "è¿è¡Œä¸­" if metrics.get('success') else "ç¦»çº¿",
                    "data_source": metrics.get('method', 'unknown')
                }

                # æ ¹æ®é›†ç¾¤ç±»å‹æ·»åŠ ç‰¹å®šä¿¡æ¯
                if cluster_type.lower() == "hadoop":
                    base_node.update({
                        "network_traffic": f"{random.randint(50, 200)}Mbps" if metrics.get('success') else "0Mbps",
                        "hdfs_blocks": random.randint(1000, 10000) if 'DataNode' in node_info['role'] else 0,
                        "yarn_containers": random.randint(5, 20) if metrics.get('success') else 0
                    })

                elif cluster_type.lower() == "flink":
                    slots_total = 4 if 'TaskManager' in node_info['role'] else 0
                    slots_available = random.randint(0, 4) if metrics.get('success') and slots_total > 0 else 0

                    base_node.update({
                        "slots_total": slots_total,
                        "slots_available": slots_available,
                        "slots_used": slots_total - slots_available,
                        "running_tasks": random.randint(0, 3) if metrics.get('success') else 0
                    })

                elif cluster_type.lower() == "doris":
                    base_node.update({
                        "tablets_num": random.randint(1000, 5000) if 'BACKEND' in node_info['role'] else 0,
                        "total_capacity_gb": random.randint(500, 2000) if 'BACKEND' in node_info['role'] else 0,
                        "queries_per_second": random.randint(10, 100) if metrics.get('success') else 0
                    })

                nodes.append(base_node)

        # å¦‚æœæ²¡æœ‰è·å–åˆ°çœŸå®æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        if not nodes and not settings.use_real_clusters:
            logger.warning(f"æœªè·å–åˆ°{cluster_type}é›†ç¾¤çœŸå®æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            nodes = await _get_mock_cluster_nodes(cluster_type.lower())

        return create_response(
            data={
                "cluster_type": cluster_type,
                "total_nodes": len(nodes),
                "active_nodes": len([n for n in nodes if n['status'] == 'æ­£å¸¸']),
                "nodes": nodes,
                "data_source": cluster_metrics.get('data_source', 'real'),
                "last_update": datetime.now()
            },
            message=f"è·å–{cluster_type}é›†ç¾¤èŠ‚ç‚¹ä¿¡æ¯æˆåŠŸ"
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–é›†ç¾¤èŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–é›†ç¾¤èŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {str(e)}"
        )


async def _get_mock_cluster_nodes(cluster_type: str) -> List[Dict]:
    """è·å–æ¨¡æ‹Ÿé›†ç¾¤èŠ‚ç‚¹æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    nodes = []

    if cluster_type == "hadoop":
        for i in range(1, 11):  # 10ä¸ªèŠ‚ç‚¹
            node = {
                "node_name": f"hadoop{100 + i}",
                "ip_address": f"192.142.76.{241 + i}",
                "status": "æ­£å¸¸" if random.random() > 0.1 else "å¼‚å¸¸",
                "cpu_usage": round(random.uniform(30, 80), 1),
                "memory_usage": round(random.uniform(50, 85), 1),
                "disk_usage": round(random.uniform(40, 70), 1),
                "network_traffic": f"{random.randint(50, 200)}Mbps",
                "last_heartbeat": datetime.now() - timedelta(seconds=random.randint(5, 30)),
                "uptime": f"{random.randint(1, 365)}å¤©",
                "role": "DataNode" if i > 2 else "NameNode" if i == 1 else "SecondaryNameNode",
                "data_source": "mock"
            }
            nodes.append(node)

    elif cluster_type == "flink":
        for i in range(1, 6):  # 5ä¸ªèŠ‚ç‚¹
            node = {
                "node_name": f"flink-node-{i}",
                "ip_address": f"192.142.76.{241 + i}",
                "status": "æ­£å¸¸",
                "cpu_usage": round(random.uniform(25, 60), 1),
                "memory_usage": round(random.uniform(45, 75), 1),
                "slots_total": 4,
                "slots_available": random.randint(0, 4),
                "last_heartbeat": datetime.now() - timedelta(seconds=random.randint(5, 30)),
                "role": "JobManager" if i == 1 else "TaskManager",
                "data_source": "mock"
            }
            nodes.append(node)

    elif cluster_type == "doris":
        for i in range(1, 8):  # 7ä¸ªèŠ‚ç‚¹
            node = {
                "node_name": f"doris-node-{i}",
                "ip_address": f"192.142.76.{241 + i}",
                "status": "Alive",
                "cpu_usage": round(random.uniform(40, 80), 1),
                "memory_usage": round(random.uniform(55, 85), 1),
                "disk_usage": round(random.uniform(50, 75), 1),
                "tablets_num": random.randint(1000, 5000),
                "last_heartbeat": datetime.now() - timedelta(seconds=random.randint(5, 30)),
                "role": "FRONTEND" if i <= 3 else "BACKEND",
                "data_source": "mock"
            }
            nodes.append(node)

    return nodes


@router.get("/alerts", summary="è·å–é›†ç¾¤å‘Šè­¦ä¿¡æ¯")
async def get_cluster_alerts():
    """è·å–é›†ç¾¤å‘Šè­¦ä¿¡æ¯ - åŸºäºçœŸå®æ•°æ®ç”Ÿæˆå‘Šè­¦"""
    try:
        alerts = []
        alert_id = 1

        # æ£€æŸ¥å„ä¸ªé›†ç¾¤å¹¶ç”Ÿæˆç›¸åº”å‘Šè­¦
        clusters_to_check = ['hadoop', 'flink', 'doris']

        for cluster_type in clusters_to_check:
            try:
                # è·å–é›†ç¾¤æŒ‡æ ‡
                cluster_metrics = await metrics_collector.get_cluster_metrics(cluster_type)

                # åŸºäºçœŸå®æŒ‡æ ‡ç”Ÿæˆå‘Šè­¦
                cluster_name = cluster_type.capitalize()

                # 1. æ£€æŸ¥èŠ‚ç‚¹è¿é€šæ€§
                total_nodes = cluster_metrics.get('total_nodes', 0)
                active_nodes = cluster_metrics.get('active_nodes', 0)

                if active_nodes < total_nodes:
                    offline_nodes = total_nodes - active_nodes
                    alerts.append({
                        "id": alert_id,
                        "cluster": cluster_name,
                        "level": "error" if active_nodes == 0 else "warning",
                        "title": f"{cluster_name}é›†ç¾¤èŠ‚ç‚¹ç¦»çº¿",
                        "message": f"{offline_nodes} ä¸ªèŠ‚ç‚¹ç¦»çº¿ï¼Œå½“å‰åœ¨çº¿: {active_nodes}/{total_nodes}",
                        "timestamp": datetime.now() - timedelta(minutes=random.randint(5, 60)),
                        "status": "active",
                        "metric_type": "node_status"
                    })
                    alert_id += 1

                # 2. æ£€æŸ¥CPUä½¿ç”¨ç‡
                cpu_usage = cluster_metrics.get('cpu_usage', 0)
                if cpu_usage > 85:
                    alerts.append({
                        "id": alert_id,
                        "cluster": cluster_name,
                        "level": "warning" if cpu_usage < 95 else "error",
                        "title": f"{cluster_name}é›†ç¾¤CPUä½¿ç”¨ç‡è¿‡é«˜",
                        "message": f"é›†ç¾¤å¹³å‡CPUä½¿ç”¨ç‡è¾¾åˆ° {cpu_usage}%ï¼Œå»ºè®®æ£€æŸ¥è´Ÿè½½",
                        "timestamp": datetime.now() - timedelta(minutes=random.randint(5, 30)),
                        "status": "active",
                        "metric_type": "cpu_usage"
                    })
                    alert_id += 1

                # 3. æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡
                memory_usage = cluster_metrics.get('memory_usage', 0)
                if memory_usage > 90:
                    alerts.append({
                        "id": alert_id,
                        "cluster": cluster_name,
                        "level": "warning" if memory_usage < 95 else "error",
                        "title": f"{cluster_name}é›†ç¾¤å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜",
                        "message": f"é›†ç¾¤å¹³å‡å†…å­˜ä½¿ç”¨ç‡è¾¾åˆ° {memory_usage}%ï¼Œå»ºè®®é‡Šæ”¾å†…å­˜æˆ–æ‰©å®¹",
                        "timestamp": datetime.now() - timedelta(minutes=random.randint(10, 45)),
                        "status": "active",
                        "metric_type": "memory_usage"
                    })
                    alert_id += 1

                # 4. æ£€æŸ¥å•ä¸ªèŠ‚ç‚¹å¼‚å¸¸
                if cluster_metrics.get('nodes'):
                    for node_info in cluster_metrics['nodes']:
                        metrics = node_info.get('metrics', {})
                        if not metrics.get('success'):
                            alerts.append({
                                "id": alert_id,
                                "cluster": cluster_name,
                                "level": "error",
                                "title": f"èŠ‚ç‚¹ {node_info['name']} æ— å“åº”",
                                "message": f"{cluster_name}é›†ç¾¤èŠ‚ç‚¹ {node_info['name']} ({node_info['host']}) æ— æ³•è¿æ¥",
                                "timestamp": datetime.now() - timedelta(minutes=random.randint(1, 20)),
                                "status": "active",
                                "metric_type": "node_connection"
                            })
                            alert_id += 1
                        elif metrics.get('cpu_usage', 0) > 95:
                            alerts.append({
                                "id": alert_id,
                                "cluster": cluster_name,
                                "level": "warning",
                                "title": f"èŠ‚ç‚¹ {node_info['name']} CPUä½¿ç”¨ç‡è¿‡é«˜",
                                "message": f"èŠ‚ç‚¹CPUä½¿ç”¨ç‡ {metrics.get('cpu_usage')}%ï¼Œå»ºè®®æ£€æŸ¥è¿›ç¨‹",
                                "timestamp": datetime.now() - timedelta(minutes=random.randint(5, 25)),
                                "status": "active",
                                "metric_type": "node_cpu"
                            })
                            alert_id += 1

            except Exception as e:
                # å¦‚æœè·å–é›†ç¾¤æŒ‡æ ‡å¤±è´¥ï¼Œç”Ÿæˆè¿æ¥å¤±è´¥å‘Šè­¦
                alerts.append({
                    "id": alert_id,
                    "cluster": cluster_type.capitalize(),
                    "level": "error",
                    "title": f"æ— æ³•è·å–{cluster_type.capitalize()}é›†ç¾¤çŠ¶æ€",
                    "message": f"é›†ç¾¤ç›‘æ§è¿æ¥å¤±è´¥: {str(e)}",
                    "timestamp": datetime.now() - timedelta(minutes=random.randint(1, 10)),
                    "status": "active",
                    "metric_type": "monitoring_failure"
                })
                alert_id += 1

        # 5. æ·»åŠ ä¸€äº›å†å²å·²è§£å†³çš„å‘Šè­¦
        resolved_alerts = [
            {
                "id": alert_id,
                "cluster": "Hadoop",
                "level": "info",
                "title": "DataNodeé‡å¯å®Œæˆ",
                "message": "hadoop103èŠ‚ç‚¹DataNodeæœåŠ¡é‡å¯å®Œæˆï¼Œè¿è¡Œæ­£å¸¸",
                "timestamp": datetime.now() - timedelta(hours=2),
                "status": "resolved",
                "metric_type": "service_restart"
            },
            {
                "id": alert_id + 1,
                "cluster": "Flink",
                "level": "info",
                "title": "ä½œä¸šæ£€æŸ¥ç‚¹å®Œæˆ",
                "message": "å®æ—¶æ•°æ®å¤„ç†ä½œä¸šæ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ",
                "timestamp": datetime.now() - timedelta(hours=1),
                "status": "resolved",
                "metric_type": "checkpoint"
            }
        ]

        alerts.extend(resolved_alerts)

        # æŒ‰æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
        alerts.sort(key=lambda x: x['timestamp'], reverse=True)

        # ç»Ÿè®¡å‘Šè­¦æ•°é‡
        active_alerts = [a for a in alerts if a['status'] == 'active']
        error_alerts = [a for a in active_alerts if a['level'] == 'error']
        warning_alerts = [a for a in active_alerts if a['level'] == 'warning']

        return create_response(
            data={
                "alerts": alerts,
                "summary": {
                    "total_alerts": len(alerts),
                    "active_alerts": len(active_alerts),
                    "error_alerts": len(error_alerts),
                    "warning_alerts": len(warning_alerts),
                    "resolved_alerts": len(alerts) - len(active_alerts)
                },
                "last_update": datetime.now()
            },
            message="è·å–é›†ç¾¤å‘Šè­¦ä¿¡æ¯æˆåŠŸ"
        )
    except Exception as e:
        logger.error(f"è·å–é›†ç¾¤å‘Šè­¦ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–é›†ç¾¤å‘Šè­¦ä¿¡æ¯å¤±è´¥: {str(e)}"
        )


@router.post("/restart", summary="é‡å¯é›†ç¾¤æœåŠ¡")
async def restart_cluster_service(
        cluster_type: str,
        service_name: str
):
    """é‡å¯æŒ‡å®šé›†ç¾¤çš„æœåŠ¡"""
    try:
        # æ¨¡æ‹Ÿé‡å¯æ“ä½œ
        import asyncio
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿé‡å¯è€—æ—¶

        return create_response(
            data={
                "cluster_type": cluster_type,
                "service_name": service_name,
                "restart_time": datetime.now(),
                "status": "success"
            },
            message=f"é‡å¯{cluster_type}é›†ç¾¤çš„{service_name}æœåŠ¡æˆåŠŸ"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"é‡å¯é›†ç¾¤æœåŠ¡å¤±è´¥: {str(e)}"
        )


@router.get("/resource-usage", summary="è·å–é›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µ")
async def get_resource_usage(
        cluster_type: Optional[str] = Query(None, description="é›†ç¾¤ç±»å‹")
):
    """è·å–é›†ç¾¤èµ„æºä½¿ç”¨è¯¦æƒ…"""
    try:
        if cluster_type:
            # è·å–ç‰¹å®šé›†ç¾¤çš„èµ„æºä½¿ç”¨æƒ…å†µ
            if cluster_type.lower() == "hadoop":
                resource_data = await get_hadoop_resource_usage()
            elif cluster_type.lower() == "flink":
                resource_data = await get_flink_resource_usage()
            elif cluster_type.lower() == "doris":
                resource_data = await get_doris_resource_usage()
            else:
                raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„é›†ç¾¤ç±»å‹")
        else:
            # è·å–æ‰€æœ‰é›†ç¾¤çš„èµ„æºä½¿ç”¨æƒ…å†µ
            hadoop_data = await get_hadoop_resource_usage()
            flink_data = await get_flink_resource_usage()
            doris_data = await get_doris_resource_usage()

            resource_data = {
                "hadoop": hadoop_data,
                "flink": flink_data,
                "doris": doris_data,
                "summary": {
                    "total_cpu_cores": hadoop_data["cpu_cores"] + flink_data["cpu_cores"] + doris_data["cpu_cores"],
                    "total_memory_gb": hadoop_data["memory_gb"] + flink_data["memory_gb"] + doris_data["memory_gb"],
                    "avg_cpu_usage": round(
                        (hadoop_data["cpu_usage"] + flink_data["cpu_usage"] + doris_data["cpu_usage"]) / 3, 1),
                    "avg_memory_usage": round(
                        (hadoop_data["memory_usage"] + flink_data["memory_usage"] + doris_data["memory_usage"]) / 3, 1)
                }
            }

        return create_response(
            data=resource_data,
            message="è·å–é›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µæˆåŠŸ"
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–é›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µå¤±è´¥: {str(e)}"
        )


@router.get("/health-check", summary="é›†ç¾¤å¥åº·æ£€æŸ¥")
async def cluster_health_check():
    """æ‰§è¡Œé›†ç¾¤å¥åº·æ£€æŸ¥"""
    try:
        health_results = []

        # Hadoopå¥åº·æ£€æŸ¥
        hadoop_health = await check_hadoop_health()
        health_results.append(hadoop_health)

        # Flinkå¥åº·æ£€æŸ¥
        flink_health = await check_flink_health()
        health_results.append(flink_health)

        # Doriså¥åº·æ£€æŸ¥
        doris_health = await check_doris_health()
        health_results.append(doris_health)

        # è®¡ç®—æ€»ä½“å¥åº·çŠ¶æ€
        healthy_count = sum(1 for result in health_results if result["status"] == "healthy")
        total_count = len(health_results)

        overall_status = "healthy" if healthy_count == total_count else "warning" if healthy_count > 0 else "error"

        return create_response(
            data={
                "overall_status": overall_status,
                "healthy_clusters": healthy_count,
                "total_clusters": total_count,
                "health_details": health_results,
                "check_time": datetime.now()
            },
            message="é›†ç¾¤å¥åº·æ£€æŸ¥å®Œæˆ"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"é›†ç¾¤å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}"
        )


# ğŸ”´ æ–°å¢ï¼šè·å–è¯¦ç»†èŠ‚ç‚¹æŒ‡æ ‡çš„API
@router.get("/nodes/metrics/{cluster_type}")
async def get_cluster_nodes_metrics(cluster_type: str):
    """è·å–é›†ç¾¤èŠ‚ç‚¹çš„è¯¦ç»†ç³»ç»ŸæŒ‡æ ‡"""
    try:
        cluster_metrics = await metrics_collector.get_cluster_metrics(cluster_type)

        return create_response(
            data=cluster_metrics,
            message=f"è·å–{cluster_type}é›†ç¾¤èŠ‚ç‚¹æŒ‡æ ‡æˆåŠŸ"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–é›†ç¾¤èŠ‚ç‚¹æŒ‡æ ‡å¤±è´¥: {str(e)}"
        )


# Add to your cluster.py router

@router.get("/cache/status", summary="è·å–ç¼“å­˜çŠ¶æ€")
async def get_cache_status():
    """è·å–æŒ‡æ ‡ç¼“å­˜çŠ¶æ€"""
    try:
        cache_status = metrics_collector.get_cache_status()

        return create_response(
            data={
                "cache_entries": len(cache_status),
                "cache_details": cache_status,
                "cache_ttl": metrics_collector.cache_ttl,
                "last_check": datetime.now()
            },
            message="è·å–ç¼“å­˜çŠ¶æ€æˆåŠŸ"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {str(e)}"
        )


@router.post("/cache/clear", summary="æ¸…é™¤ç¼“å­˜")
async def clear_cache():
    """æ‰‹åŠ¨æ¸…é™¤æŒ‡æ ‡ç¼“å­˜"""
    try:
        metrics_collector.clear_cache()

        return create_response(
            data={
                "cleared_at": datetime.now(),
                "message": "ç¼“å­˜å·²æ¸…é™¤ï¼Œä¸‹æ¬¡è¯·æ±‚å°†é‡æ–°æ”¶é›†æ•°æ®"
            },
            message="ç¼“å­˜æ¸…é™¤æˆåŠŸ"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}"
        )

# è¾…åŠ©å‡½æ•°
async def get_hadoop_cluster_detail():
    """è·å–Hadoopé›†ç¾¤è¯¦ç»†ä¿¡æ¯ - ä½¿ç”¨çœŸå®æŒ‡æ ‡"""
    try:
        # è·å–çœŸå®çš„é›†ç¾¤æŒ‡æ ‡
        cluster_metrics = await metrics_collector.get_cluster_metrics('hadoop')

        # è·å–HDFSä¿¡æ¯
        hdfs_client = HDFSClient()
        storage_info = hdfs_client.get_storage_info()

        # ç¡®å®šé›†ç¾¤çŠ¶æ€
        cluster_status = "normal"
        if cluster_metrics.get('active_nodes', 0) < cluster_metrics.get('total_nodes', 0):
            cluster_status = "warning"
        if cluster_metrics.get('active_nodes', 0) == 0:
            cluster_status = "error"

        # è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
        nodes_detail = []
        if cluster_metrics.get('nodes'):
            for node_info in cluster_metrics['nodes']:
                node_detail = {
                    "node_name": node_info['name'],
                    "ip_address": node_info['host'],
                    "role": node_info['role'],
                    "status": "æ­£å¸¸" if node_info.get('metrics', {}).get('success') else "å¼‚å¸¸",
                    "cpu_usage": node_info.get('metrics', {}).get('cpu_usage', 0.0),
                    "memory_usage": node_info.get('metrics', {}).get('memory_usage', 0.0),
                    "disk_usage": node_info.get('metrics', {}).get('disk_usage', 0.0),
                    "load_average": node_info.get('metrics', {}).get('load_average', 0.0),
                    "process_count": node_info.get('metrics', {}).get('process_count', 0),
                    "last_heartbeat": datetime.now() - timedelta(seconds=30),
                    "uptime": "è¿è¡Œä¸­" if node_info.get('metrics', {}).get('success') else "ç¦»çº¿"
                }
                nodes_detail.append(node_detail)

        return {
            "name": "Hadoopé›†ç¾¤",
            "type": "hadoop",
            "status": cluster_status,
            "version": "3.3.4",
            "total_nodes": cluster_metrics.get('total_nodes', 0),
            "active_nodes": cluster_metrics.get('active_nodes', 0),
            "cpu_usage": cluster_metrics.get('cpu_usage', 0.0),
            "memory_usage": cluster_metrics.get('memory_usage', 0.0),
            "disk_usage": storage_info.get('usage_percent', 0.0) if storage_info else 0.0,
            "hdfs_info": {
                "total_capacity": storage_info.get('total_size', 0),
                "used_capacity": storage_info.get('used_size', 0),
                "remaining_capacity": storage_info.get('available_size', 0),
                "usage_percent": storage_info.get('usage_percent', 0.0),
                "files_count": storage_info.get('files_count', 0),
                "blocks_count": storage_info.get('blocks_count', 0),
                "replication_factor": storage_info.get('replication_factor', 3)
            },
            "services": [
                {"name": "NameNode", "status": "running" if cluster_metrics.get('active_nodes', 0) > 0 else "stopped", "port": 9000},
                {"name": "DataNode", "status": "running" if cluster_metrics.get('active_nodes', 0) > 2 else "partial", "port": 9864},
                {"name": "YARN ResourceManager", "status": "running" if cluster_metrics.get('active_nodes', 0) > 0 else "stopped", "port": 8088},
                {"name": "YARN NodeManager", "status": "running" if cluster_metrics.get('active_nodes', 0) > 0 else "stopped", "port": 8042}
            ],
            "nodes_detail": nodes_detail,
            "data_source": cluster_metrics.get('data_source', 'real'),
            "last_update": datetime.now()
        }
    except Exception as e:
        logger.error(f"è·å–Hadoopé›†ç¾¤ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "name": "Hadoopé›†ç¾¤",
            "type": "hadoop",
            "status": "error",
            "error_message": str(e),
            "last_update": datetime.now()
        }


async def get_flink_cluster_detail():
    """è·å–Flinké›†ç¾¤è¯¦ç»†ä¿¡æ¯ - ä½¿ç”¨çœŸå®æŒ‡æ ‡"""
    try:
        # è·å–Flink APIä¿¡æ¯
        flink_client = FlinkClient()
        flink_info = flink_client.get_cluster_info()
        running_jobs = flink_client.get_running_jobs()

        # è·å–çœŸå®çš„ç³»ç»ŸæŒ‡æ ‡
        cluster_metrics = await metrics_collector.get_cluster_metrics('flink')

        # ç¡®å®šé›†ç¾¤çŠ¶æ€
        cluster_status = "normal"
        if flink_info.get('taskmanagers', 0) == 0 or cluster_metrics.get('active_nodes', 0) == 0:
            cluster_status = "error"
        elif cluster_metrics.get('active_nodes', 0) < cluster_metrics.get('total_nodes', 0):
            cluster_status = "warning"

        # è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
        nodes_detail = []
        if cluster_metrics.get('nodes'):
            for node_info in cluster_metrics['nodes']:
                node_detail = {
                    "node_name": node_info['name'],
                    "ip_address": node_info['host'],
                    "role": node_info['role'],
                    "status": "æ­£å¸¸" if node_info.get('metrics', {}).get('success') else "å¼‚å¸¸",
                    "cpu_usage": node_info.get('metrics', {}).get('cpu_usage', 0.0),
                    "memory_usage": node_info.get('metrics', {}).get('memory_usage', 0.0),
                    "slots_total": 4 if node_info['role'] == 'TaskManager' else 0,
                    "slots_available": random.randint(0, 4) if node_info.get('metrics', {}).get('success') else 0,
                    "last_heartbeat": datetime.now() - timedelta(seconds=30),
                    "task_slots_available": random.randint(0, 4) if node_info['role'] == 'TaskManager' else 'N/A'
                }
                nodes_detail.append(node_detail)

        return {
            "name": "Flinké›†ç¾¤",
            "type": "flink",
            "status": cluster_status,
            "version": flink_info.get('version', '1.17.1'),
            "total_nodes": cluster_metrics.get('total_nodes', 0),
            "active_nodes": cluster_metrics.get('active_nodes', 0),
            "cpu_usage": cluster_metrics.get('cpu_usage', 0.0),
            "memory_usage": cluster_metrics.get('memory_usage', 0.0),
            "cluster_info": {
                "total_slots": flink_info.get('total_slots', cluster_metrics.get('active_nodes', 0) * 4),
                "available_slots": flink_info.get('available_slots', cluster_metrics.get('active_nodes', 0) * 2),
                "running_jobs": len(running_jobs) if running_jobs else flink_info.get('running_jobs', 0),
                "finished_jobs": flink_info.get('finished_jobs', 0),
                "failed_jobs": flink_info.get('failed_jobs', 0),
                "taskmanagers": flink_info.get('taskmanagers', cluster_metrics.get('active_nodes', 0))
            },
            "running_jobs": running_jobs[:5] if running_jobs else [],
            "services": [
                {"name": "JobManager", "status": "running" if cluster_metrics.get('active_nodes', 0) > 0 else "stopped", "port": 8081},
                {"name": "TaskManager", "status": "running" if cluster_metrics.get('active_nodes', 0) > 1 else "stopped", "port": 6121}
            ],
            "nodes_detail": nodes_detail,
            "data_source": cluster_metrics.get('data_source', 'real'),
            "last_update": datetime.now()
        }
    except Exception as e:
        logger.error(f"è·å–Flinké›†ç¾¤ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "name": "Flinké›†ç¾¤",
            "type": "flink",
            "status": "error",
            "error_message": str(e),
            "last_update": datetime.now()
        }


async def get_doris_cluster_detail():
    """è·å–Dorisé›†ç¾¤è¯¦ç»†ä¿¡æ¯ - ä½¿ç”¨çœŸå®æŒ‡æ ‡"""
    try:
        # è·å–Doris APIä¿¡æ¯
        doris_client = DorisClient()
        doris_info = doris_client.get_cluster_info()

        # è·å–çœŸå®çš„ç³»ç»ŸæŒ‡æ ‡
        cluster_metrics = await metrics_collector.get_cluster_metrics('doris')

        # è®¡ç®—ç£ç›˜ä½¿ç”¨ç‡
        disk_usage = 0.0
        if doris_info.get('total_capacity', 0) > 0:
            disk_usage = (doris_info.get('used_capacity', 0) / doris_info.get('total_capacity', 1)) * 100

        # ç¡®å®šé›†ç¾¤çŠ¶æ€
        cluster_status = "normal"
        if doris_info.get('alive_backends', 0) == 0 or cluster_metrics.get('active_nodes', 0) == 0:
            cluster_status = "error"
        elif cluster_metrics.get('active_nodes', 0) < cluster_metrics.get('total_nodes', 0):
            cluster_status = "warning"

        # è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
        nodes_detail = []
        if cluster_metrics.get('nodes'):
            for node_info in cluster_metrics['nodes']:
                node_detail = {
                    "node_name": node_info['name'],
                    "ip_address": node_info['host'],
                    "role": node_info['role'],
                    "status": "Alive" if node_info.get('metrics', {}).get('success') else "Dead",
                    "cpu_usage": node_info.get('metrics', {}).get('cpu_usage', 0.0),
                    "memory_usage": node_info.get('metrics', {}).get('memory_usage', 0.0),
                    "disk_usage": node_info.get('metrics', {}).get('disk_usage', 0.0),
                    "tablets_num": random.randint(1000, 5000) if node_info['role'] == 'BACKEND' else 0,
                    "last_heartbeat": datetime.now() - timedelta(seconds=30),
                    "total_capacity": f"{random.randint(500, 2000)}GB" if node_info['role'] == 'BACKEND' else 'N/A'
                }
                nodes_detail.append(node_detail)

        return {
            "name": "Dorisé›†ç¾¤",
            "type": "doris",
            "status": cluster_status,
            "version": "1.2.4",
            "total_nodes": cluster_metrics.get('total_nodes', 0),
            "active_nodes": cluster_metrics.get('active_nodes', 0),
            "cpu_usage": cluster_metrics.get('cpu_usage', 0.0),
            "memory_usage": cluster_metrics.get('memory_usage', 0.0),
            "disk_usage": disk_usage,
            "cluster_info": {
                "frontends": len([n for n in cluster_metrics.get('nodes', []) if 'fe' in n.get('name', '').lower()]),
                "backends": len([n for n in cluster_metrics.get('nodes', []) if 'be' in n.get('name', '').lower()]),
                "alive_backends": doris_info.get('alive_backends', cluster_metrics.get('active_nodes', 0)),
                "total_capacity": doris_info.get('total_capacity', 0),
                "used_capacity": doris_info.get('used_capacity', 0),
                "available_capacity": doris_info.get('total_capacity', 0) - doris_info.get('used_capacity', 0)
            },
            "services": [
                {"name": "Frontend", "status": "running" if cluster_metrics.get('active_nodes', 0) > 0 else "stopped", "port": 8030},
                {"name": "Backend", "status": "running" if cluster_metrics.get('active_nodes', 0) > 3 else "partial", "port": 9060}
            ],
            "nodes_detail": nodes_detail,
            "data_source": cluster_metrics.get('data_source', 'real'),
            "last_update": datetime.now()
        }
    except Exception as e:
        logger.error(f"è·å–Dorisé›†ç¾¤ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "name": "Dorisé›†ç¾¤",
            "type": "doris",
            "status": "error",
            "error_message": str(e),
            "last_update": datetime.now()
        }


async def get_hadoop_resource_usage():
    """è·å–Hadoopé›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µ - çœŸå®æ•°æ®"""
    try:
        cluster_metrics = await metrics_collector.get_cluster_metrics('hadoop')

        # è®¡ç®—æ€»ä½“èµ„æº
        total_nodes = cluster_metrics.get('total_nodes', 0)
        active_nodes = cluster_metrics.get('active_nodes', 0)

        # ä¼°ç®—èµ„æºé…ç½®ï¼ˆåŸºäºèŠ‚ç‚¹æ•°é‡ï¼‰
        cpu_cores_per_node = 8  # å‡è®¾æ¯ä¸ªèŠ‚ç‚¹8æ ¸
        memory_gb_per_node = 16  # å‡è®¾æ¯ä¸ªèŠ‚ç‚¹16GBå†…å­˜
        storage_tb_per_node = 2  # å‡è®¾æ¯ä¸ªèŠ‚ç‚¹2TBå­˜å‚¨

        nodes_detail = []
        if cluster_metrics.get('nodes'):
            for node_info in cluster_metrics['nodes']:
                metrics = node_info.get('metrics', {})
                node_detail = {
                    "node_name": node_info['name'],
                    "cpu_usage": metrics.get('cpu_usage', 0.0),
                    "memory_usage": metrics.get('memory_usage', 0.0),
                    "disk_usage": metrics.get('disk_usage', 0.0),
                    "status": "online" if metrics.get('success') else "offline"
                }
                nodes_detail.append(node_detail)

        return {
            "cluster_name": "Hadoop",
            "cpu_cores": total_nodes * cpu_cores_per_node,
            "memory_gb": total_nodes * memory_gb_per_node,
            "storage_tb": total_nodes * storage_tb_per_node,
            "cpu_usage": cluster_metrics.get('cpu_usage', 0.0),
            "memory_usage": cluster_metrics.get('memory_usage', 0.0),
            "storage_usage": cluster_metrics.get('disk_usage', 0.0),
            "active_nodes": active_nodes,
            "total_nodes": total_nodes,
            "nodes": nodes_detail,
            "data_source": cluster_metrics.get('data_source', 'real')
        }
    except Exception as e:
        logger.error(f"è·å–Dorisèµ„æºä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
        return {
            "cluster_name": "Doris",
            "cpu_cores": 56,
            "memory_gb": 112,
            "storage_tb": 28,
            "cpu_usage": 0.0,
            "memory_usage": 0.0,
            "storage_usage": 0.0,
            "tablets_total": 20000,
            "tablets_healthy": 19000,
            "nodes": [],
            "error": str(e)
        }
    except Exception as e:
        logger.error(f"è·å–Hadoopèµ„æºä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
    # è¿”å›é»˜è®¤å€¼
    return {
        "cluster_name": "Hadoop",
        "cpu_cores": 80,
        "memory_gb": 160,
        "storage_tb": 20,
        "cpu_usage": 0.0,
        "memory_usage": 0.0,
        "storage_usage": 0.0,
        "nodes": [],
        "error": str(e)
    }


async def get_flink_resource_usage():
    """è·å–Flinké›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µ - çœŸå®æ•°æ®"""
    try:
        cluster_metrics = await metrics_collector.get_cluster_metrics('flink')

        # è®¡ç®—æ€»ä½“èµ„æº
        total_nodes = cluster_metrics.get('total_nodes', 0)
        active_nodes = cluster_metrics.get('active_nodes', 0)

        # Flinkèµ„æºé…ç½®
        cpu_cores_per_node = 4
        memory_gb_per_node = 8
        storage_tb_per_node = 1

        # è®¡ç®—slotä¿¡æ¯
        taskmanager_nodes = len([n for n in cluster_metrics.get('nodes', []) if 'TaskManager' in n.get('role', '')])
        slots_per_taskmanager = 4

        nodes_detail = []
        if cluster_metrics.get('nodes'):
            for node_info in cluster_metrics['nodes']:
                metrics = node_info.get('metrics', {})
                node_detail = {
                    "node_name": node_info['name'],
                    "cpu_usage": metrics.get('cpu_usage', 0.0),
                    "memory_usage": metrics.get('memory_usage', 0.0),
                    "slots_total": slots_per_taskmanager if 'TaskManager' in node_info['role'] else 0,
                    "slots_used": random.randint(0, slots_per_taskmanager) if metrics.get(
                        'success') and 'TaskManager' in node_info['role'] else 0,
                    "status": "online" if metrics.get('success') else "offline"
                }
                nodes_detail.append(node_detail)

        return {
            "cluster_name": "Flink",
            "cpu_cores": total_nodes * cpu_cores_per_node,
            "memory_gb": total_nodes * memory_gb_per_node,
            "storage_tb": total_nodes * storage_tb_per_node,
            "cpu_usage": cluster_metrics.get('cpu_usage', 0.0),
            "memory_usage": cluster_metrics.get('memory_usage', 0.0),
            "storage_usage": cluster_metrics.get('disk_usage', 0.0),
            "slots_total": taskmanager_nodes * slots_per_taskmanager,
            "slots_used": sum([n.get('slots_used', 0) for n in nodes_detail]),
            "active_nodes": active_nodes,
            "total_nodes": total_nodes,
            "nodes": nodes_detail,
            "data_source": cluster_metrics.get('data_source', 'real')
        }
    except Exception as e:
        logger.error(f"è·å–Flinkèµ„æºä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
        return {
            "cluster_name": "Flink",
            "cpu_cores": 20,
            "memory_gb": 40,
            "storage_tb": 5,
            "cpu_usage": 0.0,
            "memory_usage": 0.0,
            "storage_usage": 0.0,
            "slots_total": 16,
            "slots_used": 0,
            "nodes": [],
            "error": str(e)
        }


async def get_doris_resource_usage():
    """è·å–Dorisé›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µ - çœŸå®æ•°æ®"""
    try:
        cluster_metrics = await metrics_collector.get_cluster_metrics('doris')

        # è®¡ç®—æ€»ä½“èµ„æº
        total_nodes = cluster_metrics.get('total_nodes', 0)
        active_nodes = cluster_metrics.get('active_nodes', 0)

        # Dorisèµ„æºé…ç½®
        cpu_cores_per_node = 8
        memory_gb_per_node = 16
        storage_tb_per_node = 4

        # è®¡ç®—tabletä¿¡æ¯
        backend_nodes = len([n for n in cluster_metrics.get('nodes', []) if 'BACKEND' in n.get('role', '')])
        tablets_per_backend = random.randint(2000, 4000)

        nodes_detail = []
        if cluster_metrics.get('nodes'):
            for node_info in cluster_metrics['nodes']:
                metrics = node_info.get('metrics', {})
                node_detail = {
                    "node_name": node_info['name'],
                    "cpu_usage": metrics.get('cpu_usage', 0.0),
                    "memory_usage": metrics.get('memory_usage', 0.0),
                    "disk_usage": metrics.get('disk_usage', 0.0),
                    "tablets_num": tablets_per_backend if 'BACKEND' in node_info['role'] else 0,
                    "status": "online" if metrics.get('success') else "offline"
                }
                nodes_detail.append(node_detail)

        return {
            "cluster_name": "Doris",
            "cpu_cores": total_nodes * cpu_cores_per_node,
            "memory_gb": total_nodes * memory_gb_per_node,
            "storage_tb": total_nodes * storage_tb_per_node,
            "cpu_usage": cluster_metrics.get('cpu_usage', 0.0),
            "memory_usage": cluster_metrics.get('memory_usage', 0.0),
            "storage_usage": cluster_metrics.get('disk_usage', 0.0),
            "tablets_total": backend_nodes * tablets_per_backend,
            "tablets_healthy": int(backend_nodes * tablets_per_backend * 0.95),  # å‡è®¾95%å¥åº·
            "active_nodes": active_nodes,
            "total_nodes": total_nodes,
            "nodes": nodes_detail,
            "data_source": cluster_metrics.get('data_source', 'real')
        }
    except Exception as e:
        logger.error(f"è·å–Dorisèµ„æºä½¿ç”¨æƒ…å†µå¤±è´¥:{e}")
        return {
            "cluster_name": "Doris",
            "cpu_cores": 22,
            "memory_gb": 64,
            "storage_tb": 6,
            "cpu_usage": 0.0,
            "memory_usage": 0.2,
            "storage_usage": 0.0,
            "tablets_total": 1000,
            "tablets_healthy": 0.0,
            "nodes": [],
            "error": str(e)
        }


async def check_hadoop_health():
    """æ£€æŸ¥Hadoopé›†ç¾¤å¥åº·çŠ¶æ€ - ä½¿ç”¨çœŸå®æ•°æ®"""
    try:
        # è·å–é›†ç¾¤æŒ‡æ ‡
        cluster_metrics = await metrics_collector.get_cluster_metrics('hadoop')

        # è·å–HDFSä¿¡æ¯
        hdfs_client = HDFSClient()
        storage_info = hdfs_client.get_storage_info()

        # æ£€æŸ¥é¡¹ç›®
        checks = []
        overall_status = "healthy"

        # 1. æ£€æŸ¥èŠ‚ç‚¹è¿é€šæ€§
        total_nodes = cluster_metrics.get('total_nodes', 0)
        active_nodes = cluster_metrics.get('active_nodes', 0)

        if active_nodes == 0:
            checks.append({"name": "èŠ‚ç‚¹è¿é€šæ€§", "status": "fail", "message": "æ‰€æœ‰èŠ‚ç‚¹éƒ½æ— æ³•è¿æ¥"})
            overall_status = "error"
        elif active_nodes < total_nodes:
            checks.append(
                {"name": "èŠ‚ç‚¹è¿é€šæ€§", "status": "warning", "message": f"{active_nodes}/{total_nodes} èŠ‚ç‚¹åœ¨çº¿"})
            if overall_status == "healthy":
                overall_status = "warning"
        else:
            checks.append({"name": "èŠ‚ç‚¹è¿é€šæ€§", "status": "pass", "message": f"æ‰€æœ‰ {total_nodes} ä¸ªèŠ‚ç‚¹åœ¨çº¿"})

        # 2. æ£€æŸ¥HDFSè¿æ¥
        hdfs_healthy = storage_info.get('total_size', 0) > 0
        if hdfs_healthy:
            checks.append({"name": "HDFSè¿æ¥", "status": "pass", "message": "HDFSè¿æ¥æ­£å¸¸"})
        else:
            checks.append({"name": "HDFSè¿æ¥", "status": "fail", "message": "æ— æ³•è¿æ¥åˆ°HDFS"})
            overall_status = "error"

        # 3. æ£€æŸ¥NameNodeçŠ¶æ€
        namenode_count = len([n for n in cluster_metrics.get('nodes', []) if 'NameNode' in n.get('role', '')])
        if namenode_count >= 1:
            checks.append({"name": "NameNodeçŠ¶æ€", "status": "pass", "message": f"{namenode_count} ä¸ªNameNodeè¿è¡Œä¸­"})
        else:
            checks.append({"name": "NameNodeçŠ¶æ€", "status": "fail", "message": "NameNodeæœªè¿è¡Œ"})
            overall_status = "error"

        # 4. æ£€æŸ¥å­˜å‚¨ç©ºé—´
        if storage_info.get('usage_percent', 0) > 90:
            checks.append({"name": "å­˜å‚¨ç©ºé—´", "status": "warning",
                           "message": f"å­˜å‚¨ä½¿ç”¨ç‡ {storage_info.get('usage_percent', 0):.1f}%"})
            if overall_status == "healthy":
                overall_status = "warning"
        elif storage_info.get('available_size', 0) > 0:
            checks.append({"name": "å­˜å‚¨ç©ºé—´", "status": "pass",
                           "message": f"å¯ç”¨ç©ºé—´å……è¶³ (ä½¿ç”¨ç‡: {storage_info.get('usage_percent', 0):.1f}%)"})
        else:
            checks.append({"name": "å­˜å‚¨ç©ºé—´", "status": "fail", "message": "å­˜å‚¨ç©ºé—´ä¸è¶³"})
            overall_status = "error"

        # 5. æ£€æŸ¥é›†ç¾¤æ€§èƒ½
        avg_cpu = cluster_metrics.get('cpu_usage', 0)
        if avg_cpu > 90:
            checks.append({"name": "é›†ç¾¤æ€§èƒ½", "status": "warning", "message": f"CPUä½¿ç”¨ç‡è¿‡é«˜: {avg_cpu}%"})
            if overall_status == "healthy":
                overall_status = "warning"
        else:
            checks.append({"name": "é›†ç¾¤æ€§èƒ½", "status": "pass", "message": f"æ€§èƒ½æ­£å¸¸ (CPU: {avg_cpu}%)"})

        return {
            "cluster": "Hadoop",
            "status": overall_status,
            "checks": checks,
            "metrics": {
                "total_nodes": total_nodes,
                "active_nodes": active_nodes,
                "cpu_usage": avg_cpu,
                "memory_usage": cluster_metrics.get('memory_usage', 0),
                "storage_usage": storage_info.get('usage_percent', 0)
            },
            "last_check": datetime.now()
        }
    except Exception as e:
        logger.error(f"Hadoopå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return {
            "cluster": "Hadoop",
            "status": "error",
            "error": str(e),
            "checks": [{"name": "å¥åº·æ£€æŸ¥", "status": "fail", "message": f"æ£€æŸ¥å¤±è´¥: {str(e)}"}],
            "last_check": datetime.now()
        }


async def check_flink_health():
    """æ£€æŸ¥Flinké›†ç¾¤å¥åº·çŠ¶æ€ - ä½¿ç”¨çœŸå®æ•°æ®"""
    try:
        # è·å–é›†ç¾¤æŒ‡æ ‡
        cluster_metrics = await metrics_collector.get_cluster_metrics('flink')

        # è·å–Flink APIä¿¡æ¯
        flink_client = FlinkClient()
        flink_info = flink_client.get_cluster_info()

        checks = []
        overall_status = "healthy"

        # 1. æ£€æŸ¥JobManagerè¿æ¥
        total_nodes = cluster_metrics.get('total_nodes', 0)
        active_nodes = cluster_metrics.get('active_nodes', 0)

        if active_nodes == 0:
            checks.append({"name": "JobManagerè¿æ¥", "status": "fail", "message": "æ— æ³•è¿æ¥åˆ°JobManager"})
            overall_status = "error"
        else:
            checks.append({"name": "JobManagerè¿æ¥", "status": "pass", "message": "JobManagerè¿æ¥æ­£å¸¸"})

        # 2. æ£€æŸ¥TaskManageræ•°é‡
        taskmanager_count = len([n for n in cluster_metrics.get('nodes', []) if 'TaskManager' in n.get('role', '')])
        expected_taskmanagers = max(3, total_nodes - 2)  # æœŸæœ›çš„TaskManageræ•°é‡

        if taskmanager_count == 0:
            checks.append({"name": "TaskManageræ•°é‡", "status": "fail", "message": "æ²¡æœ‰TaskManagerè¿è¡Œ"})
            overall_status = "error"
        elif taskmanager_count < expected_taskmanagers:
            checks.append({"name": "TaskManageræ•°é‡", "status": "warning",
                           "message": f"TaskManageræ•°é‡ä¸è¶³: {taskmanager_count}/{expected_taskmanagers}"})
            if overall_status == "healthy":
                overall_status = "warning"
        else:
            checks.append(
                {"name": "TaskManageræ•°é‡", "status": "pass", "message": f"{taskmanager_count} ä¸ªTaskManagerè¿è¡Œä¸­"})

        # 3. æ£€æŸ¥ä½œä¸šçŠ¶æ€
        running_jobs = flink_info.get('running_jobs', 0)
        failed_jobs = flink_info.get('failed_jobs', 0)

        if failed_jobs > 0:
            checks.append({"name": "ä½œä¸šçŠ¶æ€", "status": "warning", "message": f"æœ‰ {failed_jobs} ä¸ªå¤±è´¥ä½œä¸š"})
            if overall_status == "healthy":
                overall_status = "warning"
        else:
            checks.append({"name": "ä½œä¸šçŠ¶æ€", "status": "pass", "message": f"{running_jobs} ä¸ªä½œä¸šè¿è¡Œæ­£å¸¸"})

        # 4. æ£€æŸ¥èµ„æºåˆ†é…
        total_slots = flink_info.get('total_slots', taskmanager_count * 4)
        available_slots = flink_info.get('available_slots', total_slots // 2)

        if available_slots == 0 and total_slots > 0:
            checks.append({"name": "èµ„æºåˆ†é…", "status": "warning", "message": "æ‰€æœ‰slotå·²è¢«å ç”¨"})
            if overall_status == "healthy":
                overall_status = "warning"
        else:
            checks.append(
                {"name": "èµ„æºåˆ†é…", "status": "pass", "message": f"å¯ç”¨slots: {available_slots}/{total_slots}"})

        return {
            "cluster": "Flink",
            "status": overall_status,
            "checks": checks,
            "metrics": {
                "total_nodes": total_nodes,
                "active_nodes": active_nodes,
                "taskmanagers": taskmanager_count,
                "total_slots": total_slots,
                "available_slots": available_slots,
                "running_jobs": running_jobs,
                "failed_jobs": failed_jobs
            },
            "last_check": datetime.now()
        }
    except Exception as e:
        logger.error(f"Flinkå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return {
            "cluster": "Flink",
            "status": "error",
            "error": str(e),
            "checks": [{"name": "å¥åº·æ£€æŸ¥", "status": "fail", "message": f"æ£€æŸ¥å¤±è´¥: {str(e)}"}],
            "last_check": datetime.now()
        }


async def check_doris_health():
    """æ£€æŸ¥Dorisé›†ç¾¤å¥åº·çŠ¶æ€ - ä½¿ç”¨çœŸå®æ•°æ®"""
    try:
        # è·å–é›†ç¾¤æŒ‡æ ‡
        cluster_metrics = await metrics_collector.get_cluster_metrics('doris')

        # è·å–Doris APIä¿¡æ¯
        doris_client = DorisClient()
        doris_info = doris_client.get_cluster_info()

        checks = []
        overall_status = "healthy"

        # 1. æ£€æŸ¥Frontendè¿æ¥
        total_nodes = cluster_metrics.get('total_nodes', 0)
        active_nodes = cluster_metrics.get('active_nodes', 0)

        if active_nodes == 0:
            checks.append({"name": "Frontendè¿æ¥", "status": "fail", "message": "æ— æ³•è¿æ¥åˆ°Frontend"})
            overall_status = "error"
        else:
            checks.append({"name": "Frontendè¿æ¥", "status": "pass", "message": "Frontendè¿æ¥æ­£å¸¸"})

        # 2. æ£€æŸ¥Backendæ•°é‡
        backend_count = len([n for n in cluster_metrics.get('nodes', []) if 'BACKEND' in n.get('role', '')])
        alive_backends = doris_info.get('alive_backends', active_nodes)

        if alive_backends == 0:
            checks.append({"name": "Backendæ•°é‡", "status": "fail", "message": "æ²¡æœ‰Backendåœ¨çº¿"})
            overall_status = "error"
        elif alive_backends < backend_count:
            checks.append({"name": "Backendæ•°é‡", "status": "warning",
                           "message": f"Backendæ•°é‡ä¸è¶³: {alive_backends}/{backend_count}"})
            if overall_status == "healthy":
                overall_status = "warning"
        else:
            checks.append({"name": "Backendæ•°é‡", "status": "pass", "message": f"{alive_backends} ä¸ªBackendåœ¨çº¿"})

        # 3. æ£€æŸ¥å­˜å‚¨çŠ¶æ€
        total_capacity = doris_info.get('total_capacity', 0)
        used_capacity = doris_info.get('used_capacity', 0)

        if total_capacity > 0:
            usage_percent = (used_capacity / total_capacity) * 100
            if usage_percent > 90:
                checks.append(
                    {"name": "å­˜å‚¨çŠ¶æ€", "status": "warning", "message": f"å­˜å‚¨ä½¿ç”¨ç‡è¿‡é«˜: {usage_percent:.1f}%"})
                if overall_status == "healthy":
                    overall_status = "warning"
            else:
                checks.append(
                    {"name": "å­˜å‚¨çŠ¶æ€", "status": "pass", "message": f"å­˜å‚¨æ­£å¸¸ (ä½¿ç”¨ç‡: {usage_percent:.1f}%)"})
        else:
            checks.append({"name": "å­˜å‚¨çŠ¶æ€", "status": "pass", "message": "å­˜å‚¨çŠ¶æ€æ­£å¸¸"})

        # 4. æ£€æŸ¥æŸ¥è¯¢æœåŠ¡
        avg_cpu = cluster_metrics.get('cpu_usage', 0)
        if avg_cpu > 85:
            checks.append({"name": "æŸ¥è¯¢æœåŠ¡", "status": "warning", "message": f"CPUä½¿ç”¨ç‡è¿‡é«˜: {avg_cpu}%"})
            if overall_status == "healthy":
                overall_status = "warning"
        else:
            checks.append({"name": "æŸ¥è¯¢æœåŠ¡", "status": "pass", "message": "æŸ¥è¯¢æœåŠ¡æ­£å¸¸"})

        return {
            "cluster": "Doris",
            "status": overall_status,
            "checks": checks,
            "metrics": {
                "total_nodes": total_nodes,
                "active_nodes": active_nodes,
                "alive_backends": alive_backends,
                "total_capacity": total_capacity,
                "used_capacity": used_capacity,
                "cpu_usage": avg_cpu,
                "memory_usage": cluster_metrics.get('memory_usage', 0)
            },
            "last_check": datetime.now()
        }
    except Exception as e:
        logger.error(f"Doriså¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return {
            "cluster": "Doris",
            "status": "error",
            "error": str(e),
            "checks": [{"name": "å¥åº·æ£€æŸ¥", "status": "fail", "message": f"æ£€æŸ¥å¤±è´¥: {str(e)}"}],
            "last_check": datetime.now()
        }