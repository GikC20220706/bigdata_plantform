# DataXé›†æˆæœåŠ¡
import json
import subprocess
import asyncio
import os
from typing import Dict, Any, Optional
from pathlib import Path
import tempfile
from datetime import datetime


class DataXIntegrationService:
    """DataXé›†æˆæœåŠ¡"""

    def __init__(self, datax_home: str = "/opt/datax"):
        self.datax_home = Path(datax_home)
        self.python_path = self.datax_home / "bin" / "datax.py"

    async def create_sync_task(self, sync_config: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºæ•°æ®åŒæ­¥ä»»åŠ¡"""
        try:
            # ç”ŸæˆDataXé…ç½®æ–‡ä»¶
            job_config = self._generate_datax_config(sync_config)

            # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(job_config, f, indent=2, ensure_ascii=False)
                config_file = f.name

            # æ‰§è¡ŒDataXä»»åŠ¡
            result = await self._execute_datax_job(config_file, sync_config.get('task_id'))

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(config_file)

            return result

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "task_id": sync_config.get('task_id')
            }

    def _generate_datax_config(self, sync_config: Dict[str, Any]) -> Dict[str, Any]:


        """ç”ŸæˆDataXé…ç½®"""
        source = sync_config['source']
        target = sync_config['target']
        if target.get('type', '').lower() == 'hive':
            # ä»ç¯å¢ƒé…ç½®æˆ–targeté…ç½®ä¸­è·å–namenodeä¿¡æ¯
            target['namenode_host'] = target.get('namenode_host', '192.142.76.242')  # ä»ä½ çš„.envçœ‹åˆ°çš„
            target['namenode_port'] = target.get('namenode_port', '8020')

            # åŠ¨æ€ç”ŸæˆHDFSè·¯å¾„
            database = target.get('database', 'default')
            table_name = target['table']
            base_path = target.get('base_path', '/user/hive/warehouse')

            # ç”Ÿæˆå®Œæ•´çš„HDFSè·¯å¾„
            target['hdfs_path'] = f"{base_path}/{database}.db/{table_name}"

            # è®¾ç½®é»˜è®¤æ–‡ä»¶å
            if 'file_name' not in target:
                target['file_name'] = f"{table_name}_data"

        # æ ¹æ®æ•°æ®æºç±»å‹ç”Ÿæˆreaderé…ç½®
        reader_config = self._get_reader_config(source)
        # æ ¹æ®ç›®æ ‡ç±»å‹ç”Ÿæˆwriteré…ç½®
        writer_config = self._get_writer_config(target)

        datax_config = {
            "job": {
                "setting": {
                    "speed": {
                        "channel": sync_config.get('parallel_jobs', 4)
                    },
                    "errorLimit": {
                        "record": sync_config.get('error_limit', 0),
                        "percentage": 0.02
                    }
                },
                "content": [{
                    "reader": reader_config,
                    "writer": writer_config
                }]
            }
        }

        return datax_config

    def _get_reader_config(self, source: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®æ•°æ®æºç±»å‹ç”Ÿæˆreaderé…ç½®"""
        db_type = source['type'].lower()

        if db_type == 'mysql':
            return {
                "name": "mysqlreader",
                "parameter": {
                    "username": source['username'],
                    "password": source['password'],
                    "connection": [{
                        "jdbcUrl": [f"jdbc:mysql://{source['host']}:{source['port']}/{source['database']}"],
                        "querySql": [source.get('query', f"SELECT * FROM {source['table']}")]
                    }]
                }
            }

        elif db_type == 'oracle':
            return {
                "name": "oraclereader",
                "parameter": {
                    "username": source['username'],
                    "password": source['password'],
                    "connection": [{
                        "jdbcUrl": [f"jdbc:oracle:thin:@{source['host']}:{source['port']}:{source['database']}"],
                        "querySql": [source.get('query', f"SELECT * FROM {source['table']}")]
                    }]
                }
            }

        elif db_type == 'kingbase':
            return {
                "name": "kingbaseesreader",
                "parameter": {
                    "username": source['username'],
                    "password": source['password'],
                    "connection": [{
                        "jdbcUrl": [f"jdbc:kingbase8://{source['host']}:{source['port']}/{source['database']}"],
                        "querySql": [source.get('query', f"SELECT * FROM {source['table']}")]
                    }]
                }
            }

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {db_type}")

    def _get_writer_config(self, target: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®ç›®æ ‡ç±»å‹ç”Ÿæˆwriteré…ç½®"""
        db_type = target['type'].lower()

        if db_type == 'mysql':
            return {
                "name": "mysqlwriter",
                "parameter": {
                    "writeMode": target.get('write_mode', 'insert'),
                    "username": target['username'],
                    "password": target['password'],
                    "column": target.get('columns', ["*"]),  # æ–°å¢ï¼šå­—æ®µåˆ—è¡¨
                    "connection": [{
                        "jdbcUrl": f"jdbc:mysql://{target['host']}:{target['port']}/{target['database']}?useUnicode=true&characterEncoding=utf8",
                        # ä¿®å¤ï¼šå®Œæ•´URLæ ¼å¼
                        "table": [target['table']]  # ä¿æŒæ•°ç»„æ ¼å¼
                    }],
                    # å¯é€‰é…ç½®
                    "preSql": target.get('pre_sql', []),  # æ–°å¢ï¼šå‰ç½®SQL
                    "postSql": target.get('post_sql', []),  # æ–°å¢ï¼šåç½®SQL
                    "session": target.get('session', [])  # æ–°å¢ï¼šä¼šè¯è®¾ç½®
                }
            }

        elif db_type == 'hive':
            return {
                "name": "hdfswriter",
                "parameter": {
                    "defaultFS": f"hdfs://{target['namenode_host']}:{target['namenode_port']}",
                    "fileType": "text",
                    "path": target['hdfs_path'],
                    "fileName": target.get('file_name', 'data'),
                    "column": target.get('columns', [{"name": "*", "type": "string"}]),
                    "fieldDelimiter": "\t",
                    "writeMode": "append"
                }
            }

        elif db_type == 'kingbase':
            return {
                "name": "kingbasewriter",
                "parameter": {
                    "username": target['username'],
                    "password": target['password'],
                    "column": target.get('columns', ["*"]),
                    "connection": [{
                        "jdbcUrl": f"jdbc:postgresql://{target['host']}:{target['port']}/{target['database']}",
                        "table": [target['table']]
                    }],
                    "writeMode": target.get('write_mode', 'insert'),
                    "preSql": target.get('pre_sql', []),
                    "postSql": target.get('post_sql', [])
                }
            }

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›®æ ‡ç±»å‹: {db_type}")

    async def _execute_datax_job(self, config_file: str, task_id: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡ŒDataXä»»åŠ¡"""
        try:
            # æ„å»ºDataXæ‰§è¡Œå‘½ä»¤
            cmd = [
                "python3",
                str(self.python_path),
                config_file
            ]

            # å¼‚æ­¥æ‰§è¡Œå‘½ä»¤
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate()

            # è§£ææ‰§è¡Œç»“æœ
            if process.returncode == 0:
                # è§£æDataXè¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                output = stdout.decode('utf-8')
                stats = self._parse_datax_output(output)

                return {
                    "success": True,
                    "task_id": task_id,
                    "statistics": stats,
                    "output": output
                }
            else:
                return {
                    "success": False,
                    "task_id": task_id,
                    "error": stderr.decode('utf-8'),
                    "exit_code": process.returncode
                }

        except Exception as e:
            return {
                "success": False,
                "task_id": task_id,
                "error": str(e)
            }

    def _parse_datax_output(self, output: str) -> Dict[str, Any]:
        """è§£æDataXè¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_records": 0,
            "total_bytes": 0,
            "speed_records": 0,
            "speed_bytes": 0,
            "duration": 0
        }

        try:
            # è§£æDataXçš„ç»Ÿè®¡è¾“å‡º
            lines = output.split('\n')
            for line in lines:
                if "æ€»è®¡è€—æ—¶" in line:
                    # è§£ææ‰§è¡Œæ—¶é—´
                    pass
                elif "åŒæ­¥é€Ÿåº¦" in line:
                    # è§£æåŒæ­¥é€Ÿåº¦
                    pass
                elif "è¯»å–è®°å½•æ•°" in line:
                    # è§£æè®°å½•æ•°
                    pass

        except Exception:
            pass

        return stats


# æ‰©å±•ç°æœ‰çš„sync API
class EnhancedSyncService:
    """å¢å¼ºçš„æ•°æ®åŒæ­¥æœåŠ¡"""

    def __init__(self):
        self.datax_service = DataXIntegrationService()

    async def execute_sync_task(self, task_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®åŒæ­¥ä»»åŠ¡"""
        try:
            """éªŒè¯åŒæ­¥é…ç½®"""
            required_fields = ['source', 'target', 'id']
            for field in required_fields:
                if field not in task_config:
                    raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")

            # éªŒè¯æ•°æ®æºé…ç½®
            source = task_config['source']
            if 'type' not in source:
                raise ValueError("æ•°æ®æºé…ç½®ä¸å®Œæ•´ï¼šç¼ºå°‘typeå­—æ®µ")

            # éªŒè¯ç›®æ ‡é…ç½®
            target = task_config['target']
            if 'type' not in target:
                raise ValueError("ç›®æ ‡é…ç½®ä¸å®Œæ•´ï¼šç¼ºå°‘typeå­—æ®µ")

            # ğŸ†• æ–°å¢ï¼šHiveç›®æ ‡çš„ç‰¹æ®ŠéªŒè¯
            if target.get('type', '').lower() == 'hive':
                if 'table' not in target:
                    raise ValueError("Hiveç›®æ ‡é…ç½®ç¼ºå°‘tableå­—æ®µ")
            # éªŒè¯é…ç½®
            self._validate_sync_config(task_config)

            # æ ¹æ®åŒæ­¥ç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹å¼
            sync_type = task_config.get('sync_type', 'full')

            if sync_type == 'incremental':
                # å¢é‡åŒæ­¥é€»è¾‘
                return await self._execute_incremental_sync(task_config)
            else:
                # å…¨é‡åŒæ­¥
                return await self._execute_full_sync(task_config)

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "task_id": task_config.get('id')
            }

    async def _execute_full_sync(self, task_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå…¨é‡æ•°æ®åŒæ­¥"""
        return await self.datax_service.create_sync_task(task_config)

    async def _execute_incremental_sync(self, task_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå¢é‡æ•°æ®åŒæ­¥"""
        # è·å–ä¸Šæ¬¡åŒæ­¥çš„æ°´ä½çº¿
        watermark = await self._get_sync_watermark(task_config['id'])

        # ä¿®æ”¹æŸ¥è¯¢æ¡ä»¶ä»¥æ”¯æŒå¢é‡
        if watermark:
            incremental_column = task_config.get('incremental_column', 'updated_at')
            original_query = task_config['source'].get('query', f"SELECT * FROM {task_config['source']['table']}")

            # æ·»åŠ å¢é‡æ¡ä»¶
            if 'WHERE' in original_query.upper():
                incremental_query = f"{original_query} AND {incremental_column} > '{watermark}'"
            else:
                incremental_query = f"{original_query} WHERE {incremental_column} > '{watermark}'"

            task_config['source']['query'] = incremental_query

        result = await self.datax_service.create_sync_task(task_config)

        # æ›´æ–°æ°´ä½çº¿
        if result.get('success'):
            await self._update_sync_watermark(task_config['id'], datetime.now())

        return result

    async def _get_sync_watermark(self, task_id: str) -> Optional[str]:
        """è·å–åŒæ­¥æ°´ä½çº¿"""
        # ä»Redisæˆ–æ•°æ®åº“è·å–ä¸Šæ¬¡åŒæ­¥çš„æ—¶é—´æˆ³
        # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„å­˜å‚¨æ–¹å¼å®ç°
        pass

    async def _update_sync_watermark(self, task_id: str, watermark: datetime):
        """æ›´æ–°åŒæ­¥æ°´ä½çº¿"""
        # ä¿å­˜æ–°çš„æ°´ä½çº¿åˆ°Redisæˆ–æ•°æ®åº“
        # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„å­˜å‚¨æ–¹å¼å®ç°
        pass

    def _validate_sync_config(self, config: Dict[str, Any]):
        """éªŒè¯åŒæ­¥é…ç½®"""
        required_fields = ['source', 'target', 'id']
        for field in required_fields:
            if field not in config:
                raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")

        # éªŒè¯æ•°æ®æºé…ç½®
        source = config['source']
        if 'type' not in source or 'host' not in source:
            raise ValueError("æ•°æ®æºé…ç½®ä¸å®Œæ•´")

        # éªŒè¯ç›®æ ‡é…ç½®
        target = config['target']
        if 'type' not in target:
            raise ValueError("ç›®æ ‡é…ç½®ä¸å®Œæ•´")