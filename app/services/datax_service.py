# DataXé›†æˆæœåŠ¡
import json
import subprocess
import asyncio
import os
import traceback
from typing import Dict, Any, Optional, List
from pathlib import Path
import tempfile
from datetime import datetime

from loguru import logger


class DataXIntegrationService:
    """DataXé›†æˆæœåŠ¡"""

    def __init__(self, datax_home: str = "/opt/datax"):
        self.datax_home = Path(datax_home)
        self.python_path = self.datax_home / "bin" / "datax.py"

    async def create_sync_task(self, sync_config: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºæ•°æ®åŒæ­¥ä»»åŠ¡"""
        try:
            # ç”ŸæˆDataXé…ç½®æ–‡ä»¶
            job_config = self._generate_datax_config(sync_config)

            # ğŸ”§ å¼ºåˆ¶JSONåºåˆ—åŒ–æ£€æŸ¥
            try:
                test_json = json.dumps(job_config, ensure_ascii=False, default=str)
                logger.info("JSONåºåˆ—åŒ–æ£€æŸ¥é€šè¿‡")
            except Exception as json_error:
                logger.error(f"JSONåºåˆ—åŒ–å¤±è´¥: {json_error}")
                # è¾“å‡ºé—®é¢˜é…ç½®
                logger.error(
                    f"é—®é¢˜é…ç½®keys: {list(job_config.keys()) if isinstance(job_config, dict) else type(job_config)}")
                raise ValueError(f"é…ç½®åºåˆ—åŒ–å¤±è´¥: {json_error}")

            # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
                json.dump(job_config, f, indent=2, ensure_ascii=False)
                config_file = f.name

            # æ‰§è¡ŒDataXä»»åŠ¡
            result = await self._execute_datax_job(config_file, sync_config.get('task_id'))

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(config_file)

            return result

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "task_id": sync_config.get('task_id')
            }

    def _generate_datax_config(self, sync_config: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆDataXé…ç½®"""
        import copy
        clean_config = self._clean_config_for_serialization(sync_config)
        source = copy.deepcopy(sync_config['source'])
        target = copy.deepcopy(sync_config['target'])

        keys_to_remove = ['_sa_instance_state', 'connection', 'client', 'metadata',
                          'schema_mapping', 'column_mappings']

        for key in keys_to_remove:
            source.pop(key, None)
            target.pop(key, None)

        # å¦‚æœsourceæ˜¯hiveç±»å‹ï¼Œè¿›ä¸€æ­¥æ¸…ç†
        if source.get('type', '').lower() == 'hive':
            # ç¡®ä¿columnsæ˜¯ç®€å•çš„å­—ç¬¦ä¸²åˆ—è¡¨
            if 'columns' in source:
                source['columns'] = [str(col) for col in source['columns']]

        if target.get('type', '').lower() == 'hive':
            # ä»ç¯å¢ƒé…ç½®æˆ–targeté…ç½®ä¸­è·å–namenodeä¿¡æ¯
            target['namenode_host'] = target.get('namenode_host', '192.142.76.242')
            target['namenode_port'] = target.get('namenode_port', '8020')

            # ğŸ”§ ä¿®å¤ï¼šåŠ¨æ€ç”Ÿæˆæ­£ç¡®çš„HDFSè·¯å¾„ï¼ŒåŒ…å«æ•°æ®åº“åå’Œåˆ†åŒº
            database = target.get('database', 'default')
            table_name = target['table']
            base_path = target.get('base_path', '/user/hive/warehouse')

            # ç”Ÿæˆå½“å‰æ—¥æœŸåˆ†åŒº
            from datetime import datetime
            current_date = datetime.now().strftime('%Y-%m-%d')
            partition_value = target.get('partition_value', current_date)

            # ç”Ÿæˆå®Œæ•´çš„HDFSè·¯å¾„ï¼ŒåŒ…å«åˆ†åŒº
            if database and database != 'default':
                target['hdfs_path'] = f"{base_path}/{database}.db/{table_name}/dt={partition_value}"
            else:
                target['hdfs_path'] = f"{base_path}/{table_name}/dt={partition_value}"

            # è®¾ç½®é»˜è®¤æ–‡ä»¶å
            if 'file_name' not in target:
                target['file_name'] = f"{table_name}_data"

            # ğŸ†• æ–°å¢ï¼šHiveç›¸å…³é…ç½®
            target['partition_column'] = 'dt'
            target['partition_value'] = partition_value
            target['storage_format'] = 'ORC'
            target['compression'] = 'snappy'

            logger.info(f"ç”ŸæˆHive HDFSè·¯å¾„: {target['hdfs_path']}")
            logger.info(f"åˆ†åŒºä¿¡æ¯: dt={partition_value}")

        # æ ¹æ®æ•°æ®æºç±»å‹ç”Ÿæˆreaderé…ç½®
        reader_config = self._get_reader_config(source)
        # æ ¹æ®ç›®æ ‡ç±»å‹ç”Ÿæˆwriteré…ç½®
        writer_config = self._get_writer_config(target)

        datax_config = {
            "job": {
                "setting": {
                    "speed": {
                        "channel": sync_config.get('parallel_jobs', 4)
                    },
                    "errorLimit": {
                        "record": sync_config.get('error_limit', 0),
                        "percentage": 0.02
                    }
                },
                "content": [{
                    "reader": reader_config,
                    "writer": writer_config
                }]
            }
        }

        return datax_config

    def _get_reader_config(self, source: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®æ•°æ®æºç±»å‹ç”Ÿæˆreaderé…ç½®"""
        db_type = source['type'].lower()

        if db_type == 'mysql':
            columns = source.get('columns', [])
            if not columns:
                raise ValueError("MySQL Readerç¼ºå°‘å­—æ®µé…ç½®")

            # ä¸ºMySQLæ„å»ºSELECTè¯­å¥
            columns_str = ', '.join(columns)
            select_sql = f"SELECT {columns_str} FROM {source['table']}"

            return {
                "name": "mysqlreader",
                "parameter": {
                    "username": source['username'],
                    "password": source['password'],
                    "connection": [{
                        "jdbcUrl": [f"jdbc:mysql://{source['host']}:{source['port']}/{source['database']}"],
                        "querySql": [select_sql]
                    }]
                }
            }

        elif db_type == 'doris':
            columns = source.get('columns', [])
            if not columns:
                raise ValueError("Doris Readerç¼ºå°‘å­—æ®µé…ç½®")

            return {
                "name": "dorisreader",
                "parameter": {
                    "username": source['username'],
                    "password": source['password'],
                    "column": columns,
                    "splitPk": source.get('split_pk', ''),
                    "connection": [{
                        "table": [source['table']],
                        "jdbcUrl": [f"jdbc:mysql://{source['host']}:{source['port']}/{source['database']}"]
                    }]
                }
            }
        elif db_type == 'hive':
            columns = source.get('columns', [])
            schema_mapping = source.get('schema_mapping', {})
            column_info = schema_mapping.get('columns', [])

            if not columns:
                raise ValueError("Hive Readerç¼ºå°‘å­—æ®µé…ç½®")

            # æ¸…ç†sourceä¸­å¯èƒ½çš„å¯¹è±¡å¼•ç”¨
            clean_source = {
                'database': source.get('database', 'default'),
                'table': source.get('table'),
                'namenode_host': source.get('namenode_host', '192.142.76.242'),
                'namenode_port': source.get('namenode_port', '8020'),
                'base_path': source.get('base_path', '/user/hive/warehouse'),
                'file_type': source.get('file_type', 'orc'),
                'field_delimiter': source.get('field_delimiter', '\t'),
                'partition_filter': source.get('partition_filter'),
                'add_ods_prefix': source.get('add_ods_prefix', False)
            }

            # ğŸ”§ ç”ŸæˆHDFSè·¯å¾„
            hdfs_path = self._generate_hive_read_path(source)

            # ğŸ”§ ç”Ÿæˆå­—æ®µé…ç½®
            #column_config = self._generate_hdfs_column_config(column_info, columns)
            column_config = []
            for i, col in enumerate(columns):
                if isinstance(col, str):
                    column_config.append({
                        "index": i,
                        "type": "string"  # ç»Ÿä¸€ä½¿ç”¨stringç±»å‹
                    })
                else:
                    column_config.append({
                        "index": i,
                        "type": "string"
                    })

            return {
                "name": "hdfsreader",
                "parameter": {
                    "path": hdfs_path,
                    "defaultFS": f"hdfs://{source['namenode_host']}:{source['namenode_port']}",
                    "column": column_config,
                    "fileType": source.get('file_type', 'orc'),  # æ”¯æŒorc, textç­‰
                    "encoding": "UTF-8",
                    "fieldDelimiter": source.get('field_delimiter', '\t')  # åˆ†éš”ç¬¦
                }
            }

        elif db_type == 'oracle':
            columns = source.get('columns', [])
            if not columns:
                raise ValueError("Oracle Readerç¼ºå°‘å­—æ®µé…ç½®")

            columns_str = ', '.join(columns)
            select_sql = f"SELECT {columns_str} FROM {source['table']}"

            return {
                "name": "oraclereader",
                "parameter": {
                    "username": source['username'],
                    "password": source['password'],
                    "connection": [{
                        "jdbcUrl": [f"jdbc:oracle:thin:@{source['host']}:{source['port']}:{source['database']}"],
                        "querySql": [select_sql]
                    }]
                }
            }

        elif db_type == 'kingbase':
            columns = source.get('columns', [])
            if not columns:
                raise ValueError("KingBase Readerç¼ºå°‘å­—æ®µé…ç½®")

            columns_str = ', '.join(columns)
            select_sql = f"SELECT {columns_str} FROM {source['table']}"

            return {
                "name": "kingbaseesreader",
                "parameter": {
                    "username": source['username'],
                    "password": source['password'],
                    "connection": [{
                        "jdbcUrl": [f"jdbc:kingbase8://{source['host']}:{source['port']}/{source['database']}"],
                        "querySql": [select_sql]
                    }]
                }
            }

        elif db_type == 'postgresql':
            columns = source.get('columns', [])
            if not columns:
                raise ValueError("PostgreSQL Readerç¼ºå°‘å­—æ®µé…ç½®")

            columns_str = ', '.join(columns)
            select_sql = f"SELECT {columns_str} FROM {source['table']}"

            return {
                "name": "postgresqlreader",
                "parameter": {
                    "username": source['username'],
                    "password": source['password'],
                    "connection": [{
                        "jdbcUrl": [f"jdbc:postgresql://{source['host']}:{source['port']}/{source['database']}"],
                        "querySql": [select_sql]
                    }]
                }
            }

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {db_type}")

    def _generate_hive_read_path(self, source: Dict[str, Any]) -> str:
        """ç”ŸæˆHiveè¯»å–è·¯å¾„"""
        database = source.get('database', 'default')
        table_name = source['table']
        base_path = source.get('base_path', '/user/hive/warehouse')

        table_name_upper = table_name.upper()

        if table_name_upper.startswith('ODS_'):
            # å·²ç»æœ‰ODSå‰ç¼€ï¼ˆä¸ç®¡å¤§å°å†™ï¼‰ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä½†ç»Ÿä¸€è½¬ä¸ºå¤§å†™
            final_table_name = table_name_upper
        elif table_name.lower().startswith('ods_'):
            # æœ‰å°å†™çš„odså‰ç¼€ï¼Œè½¬æ¢ä¸ºå¤§å†™ODS_
            final_table_name = 'ODS_' + table_name[4:]  # å»æ‰å°å†™ods_ï¼ŒåŠ ä¸Šå¤§å†™ODS_
        elif source.get('add_ods_prefix', False):
            # æ²¡æœ‰å‰ç¼€ä¸”éœ€è¦æ·»åŠ ï¼Œæ·»åŠ ODS_å‰ç¼€
            final_table_name = f"ODS_{table_name.upper()}"
        else:
            # ä¸éœ€è¦å‰ç¼€æˆ–å·²æœ‰å‰ç¼€ï¼Œç›´æ¥ä½¿ç”¨
            final_table_name = table_name

        # æ„å»ºåŸºç¡€è·¯å¾„
        if database != 'default':
            base_table_path = f"{base_path}/{database}.db/{final_table_name}"
        else:
            base_table_path = f"{base_path}/{final_table_name}"

        # å¤„ç†åˆ†åŒº
        partition_filter = source.get('partition_filter')
        if partition_filter:
            hdfs_path = f"{base_table_path}/{partition_filter}/*"
        else:
            hdfs_path = f"{base_table_path}/*"

        logger.info(f"åŸå§‹è¡¨å: {table_name}")
        logger.info(f"æœ€ç»ˆè¡¨å: {final_table_name}")
        logger.info(f"ç”ŸæˆHiveè¯»å–è·¯å¾„: {hdfs_path}")
        return hdfs_path

    # def _generate_hdfs_column_config(self, column_info: List[Dict], column_names: List[str]) -> List[Dict]:
    #     """ç”ŸæˆHDFS Readerçš„å­—æ®µé…ç½®"""
    #     column_config = []
    #
    #     if column_info:
    #         data_columns = [col for col in column_info if col.get('name', '').lower() != 'dt']
    #         # å¦‚æœæœ‰è¯¦ç»†çš„å­—æ®µä¿¡æ¯ï¼Œä½¿ç”¨ç±»å‹æ˜ å°„
    #         for i, col in enumerate(data_columns):
    #             col_name = col.get('name', f'column_{i}')
    #             col_type = col.get('target_type', 'STRING')
    #
    #             # è½¬æ¢ä¸ºhdfsreaderæ”¯æŒçš„ç±»å‹
    #             hdfs_type = self._convert_to_hdfs_reader_type(col_type)
    #
    #             column_config.append({
    #                 "index": i,
    #                 "type": hdfs_type
    #             })
    #     else:
    #         data_column_names = [col for col in column_names if col.lower() != 'dt']
    #         # å¦‚æœåªæœ‰å­—æ®µåï¼Œé»˜è®¤éƒ½æ˜¯stringç±»å‹
    #         for i, col_name in enumerate(data_column_names):
    #             column_config.append({
    #                 "index": i,
    #                 "type": "string"
    #             })
    #
    #     logger.info(f"ç”ŸæˆHDFSå­—æ®µé…ç½®: {len(column_config)}ä¸ªå­—æ®µï¼ˆå·²æ’é™¤åˆ†åŒºå­—æ®µï¼‰")
    #     return column_config
    def _generate_hdfs_column_config(self, column_info: List[Dict], column_names: List[str]) -> List[Dict]:
        """ç”ŸæˆHDFS Readerçš„å­—æ®µé…ç½®"""
        column_config = []

        # ç¡®ä¿åªä½¿ç”¨ç®€å•ç±»å‹
        if column_info:
            for i, col in enumerate(column_info):
                # åªæå–å¿…è¦çš„å­—æ®µï¼Œé¿å…å…¶ä»–å¯¹è±¡å¼•ç”¨
                if isinstance(col, dict):
                    col_name = str(col.get('name', f'column_{i}'))
                    if col_name.lower() != 'dt':  # æ’é™¤åˆ†åŒºå­—æ®µ
                        column_config.append({
                            "index": len(column_config),
                            "type": "string"  # ç®€åŒ–ç±»å‹å¤„ç†
                        })
        else:
            for i, col_name in enumerate(column_names):
                if col_name.lower() != 'dt':
                    column_config.append({
                        "index": len(column_config),
                        "type": "string"
                    })

        logger.info(f"ç”ŸæˆHDFSå­—æ®µé…ç½®: {len(column_config)}ä¸ªå­—æ®µ")
        return column_config

    def _convert_to_hdfs_reader_type(self, hive_type: str) -> str:
        """å°†Hiveç±»å‹è½¬æ¢ä¸ºHDFS Readeræ”¯æŒçš„ç±»å‹"""
        base_type = hive_type.split('(')[0].upper()

        type_mapping = {
            'STRING': 'string',
            'VARCHAR': 'string',
            'TEXT': 'string',
            'INT': 'long',
            'INTEGER': 'long',
            'BIGINT': 'long',
            'SMALLINT': 'long',
            'TINYINT': 'long',
            'DECIMAL': 'double',
            'DOUBLE': 'double',
            'FLOAT': 'double',
            'BOOLEAN': 'boolean',
            'DATE': 'date',
            'TIMESTAMP': 'date',
            'DATETIME': 'date'
        }

        return type_mapping.get(base_type, 'string')
    def _get_writer_config(self, target: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®ç›®æ ‡ç±»å‹ç”Ÿæˆwriteré…ç½®"""
        db_type = target['type'].lower()

        if db_type == 'mysql':
            columns = target.get('columns', [])
            if not columns:
                raise ValueError("MySQL Writerç¼ºå°‘å­—æ®µé…ç½®")

            return {
                "name": "mysqlwriter",
                "parameter": {
                    "writeMode": target.get('write_mode', 'insert'),
                    "username": target['username'],
                    "password": target['password'],
                    "column": columns,
                    "connection": [{
                        "jdbcUrl": f"jdbc:mysql://{target['host']}:{target['port']}/{target['database']}?useUnicode=true&characterEncoding=utf8",
                        "table": [target['table']]
                    }],
                    "preSql": target.get('pre_sql', []),
                    "postSql": target.get('post_sql', [])
                }
            }

        elif db_type == 'doris':
            columns = target.get('columns', [])
            if not columns:
                raise ValueError("Doris Writerç¼ºå°‘å­—æ®µé…ç½®")

            return {
                "name": "doriswriter",
                "parameter": {
                    "loadUrl": [f"{target['host']}:{target.get('http_port', 8060)}"],
                    "column": columns,
                    "username": target['username'],
                    "password": target['password'],
                    "postSql": target.get('post_sql', []),
                    "preSql": target.get('pre_sql', []),
                    "flushInterval": target.get('flush_interval', 30000),
                    "connection": [{
                        "jdbcUrl": f"jdbc:mysql://{target['host']}:{target['port']}/{target['database']}",
                        "selectedDatabase": target['database'],
                        "table": [target['table']]
                    }],
                    "loadProps": {
                        "format": target.get('format', 'json'),
                        "strip_outer_array": target.get('strip_outer_array', True)
                    }
                }
            }

        elif db_type == 'hive':
            columns = target.get('columns', [])
            if not columns:
                raise ValueError("Hive Writerç¼ºå°‘å­—æ®µé…ç½®")

            return {
                "name": "hdfswriter",
                "parameter": {
                    "defaultFS": f"hdfs://{target['namenode_host']}:{target['namenode_port']}",
                    "fileType": "orc",
                    "path": target['hdfs_path'],
                    "fileName": target.get('file_name', 'data'),
                    "column": [{"name": col, "type": "string"} for col in columns],
                    "fieldDelimiter": "\t",
                    "writeMode": "append",
                    "compress": target.get('compression', 'snappy'),
                    #"orcSchema": self._generate_orc_schema(columns),
                    # "hadoopConfig": {
                    #     "orc.compress": target.get('compression', 'snappy'),
                    #     "orc.create.index": "true"
                    # }
                }
            }


        elif db_type == 'kingbase':
            columns = target.get('columns', [])
            if not columns:
                raise ValueError("KingBase Writerç¼ºå°‘å­—æ®µé…ç½®")

            return {
                "name": "kingbaseeswriter",
                "parameter": {
                    "username": target['username'],
                    "password": target['password'],
                    "column": columns,
                    "connection": [{
                        "jdbcUrl": f"jdbc:kingbase8://{target['host']}:{target['port']}/{target['database']}",
                        "table": [target['table']]
                    }],
                    "preSql": target.get('pre_sql', []),
                    "postSql": target.get('post_sql', [])
                }
            }

        elif db_type == 'postgresql':
            columns = target.get('columns', [])
            if not columns:
                raise ValueError("PostgreSQL Writerç¼ºå°‘å­—æ®µé…ç½®")

            return {
                "name": "postgresqlwriter",
                "parameter": {
                    "username": target['username'],
                    "password": target['password'],
                    "column": columns,
                    "connection": [{
                        "jdbcUrl": f"jdbc:postgresql://{target['host']}:{target['port']}/{target['database']}",
                        "table": [target['table']]
                    }],
                    "writeMode": target.get('write_mode', 'insert'),
                    "preSql": target.get('pre_sql', []),
                    "postSql": target.get('post_sql', [])
                }
            }

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›®æ ‡ç±»å‹: {db_type}")

    async def _execute_datax_job(self, config_file: str, task_id: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡ŒDataXä»»åŠ¡"""
        try:
            env = os.environ.copy()

            # é¦–å…ˆæ£€æŸ¥DataXæ˜¯å¦å­˜åœ¨
            if not self.python_path.exists():
                return {
                    "success": False,
                    "task_id": task_id,
                    "error": f"DataXè„šæœ¬ä¸å­˜åœ¨: {self.python_path}",
                    "exit_code": -1
                }

            # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(config_file):
                return {
                    "success": False,
                    "task_id": task_id,
                    "error": f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}",
                    "exit_code": -1
                }

            # è¯»å–å¹¶éªŒè¯é…ç½®æ–‡ä»¶å†…å®¹
            try:
                with open(config_file, 'r', encoding='utf-8', errors='replace') as f:
                    config_content = f.read()
                    logger.info(f"DataXé…ç½®æ–‡ä»¶å†…å®¹:\n{config_content}")
                    # éªŒè¯JSONæ ¼å¼
                    json.loads(config_content)
            except Exception as e:
                return {
                    "success": False,
                    "task_id": task_id,
                    "error": f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {str(e)}",
                    "exit_code": -1
                }

            # è®¾ç½®JVMå‚æ•°ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ï¼‰
            jvm_opts = [
                "-Xms1G",
                "-Xmx4G",
                "-XX:+UseG1GC",
                "-Dfastjson.parser.safeMode=true",
                "-Dfastjson2.parser.safeMode=true"
            ]
            env['DATAX_JVM_OPTS'] = ' '.join(jvm_opts)

            # æ„å»ºDataXæ‰§è¡Œå‘½ä»¤ - è¿™ä¸ªå¿…é¡»åœ¨ifå—å¤–é¢ï¼
            cmd = [
                "python",
                str(self.python_path),
                config_file
            ]

            logger.info(f"æ‰§è¡ŒDataXå‘½ä»¤: {' '.join(cmd)}")
            logger.info(f"JVMå‚æ•°: {env.get('DATAX_JVM_OPTS', 'None')}")

            # å¼‚æ­¥æ‰§è¡Œå‘½ä»¤
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env
            )

            stdout, stderr = await process.communicate()

            # è§£ç è¾“å‡º
            stdout_text = stdout.decode('utf-8', errors='replace')
            stderr_text = stderr.decode('utf-8', errors='replace')

            logger.info(f"DataX stdout: {stdout_text}")
            if stderr_text:
                logger.error(f"DataX stderr: {stderr_text}")
            logger.info(f"DataXé€€å‡ºç : {process.returncode}")

            # è§£ææ‰§è¡Œç»“æœ
            if process.returncode == 0:
                # è§£æDataXè¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                stats = self._parse_datax_output(stdout_text)

                return {
                    "success": True,
                    "task_id": task_id,
                    "statistics": stats,
                    "output": stdout_text,
                    "stderr": stderr_text
                }
            else:
                # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                error_message = stderr_text or stdout_text or f"DataXæ‰§è¡Œå¤±è´¥ï¼Œé€€å‡ºç : {process.returncode}"

                # æ£€æŸ¥æ˜¯å¦æ˜¯StackOverflowError
                if "StackOverflowError" in error_message:
                    logger.error("æ£€æµ‹åˆ°StackOverflowErrorï¼Œè¿™æ˜¯FastJSONåºåˆ—åŒ–é—®é¢˜")
                    error_message = "FastJSONåºåˆ—åŒ–å¤±è´¥ï¼ˆStackOverflowErrorï¼‰ï¼Œè¯·æ£€æŸ¥Hive readeré…ç½®"

                return {
                    "success": False,
                    "task_id": task_id,
                    "error": error_message,
                    "stdout": stdout_text,
                    "stderr": stderr_text,
                    "exit_code": process.returncode
                }

        except FileNotFoundError as e:
            return {
                "success": False,
                "task_id": task_id,
                "error": f"å‘½ä»¤æœªæ‰¾åˆ°: {str(e)}ã€‚è¯·æ£€æŸ¥DataXæ˜¯å¦æ­£ç¡®å®‰è£…",
                "exit_code": -1
            }
        except Exception as e:
            logger.error(f"æ‰§è¡ŒDataXä»»åŠ¡å¼‚å¸¸: {str(e)}")
            return {
                "success": False,
                "task_id": task_id,
                "error": f"æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                "exit_code": -1
            }

    def _generate_orc_schema(self, columns: List[str]) -> str:
        """ç”ŸæˆORC Schema"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥æ ¹æ®å­—æ®µç±»å‹ç”Ÿæˆ
        orc_fields = [f"{col}:string" for col in columns]
        return f"struct<{','.join(orc_fields)}>"

    def _parse_datax_output(self, output: str) -> Dict[str, Any]:
        """è§£æDataXè¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_records": 0,
            "total_bytes": 0,
            "speed_records": 0,
            "speed_bytes": 0,
            "duration": 0
        }

        try:
            # è§£æDataXçš„ç»Ÿè®¡è¾“å‡º
            lines = output.split('\n')
            for line in lines:
                if "æ€»è®¡è€—æ—¶" in line:
                    # è§£ææ‰§è¡Œæ—¶é—´
                    pass
                elif "åŒæ­¥é€Ÿåº¦" in line:
                    # è§£æåŒæ­¥é€Ÿåº¦
                    pass
                elif "è¯»å–è®°å½•æ•°" in line:
                    # è§£æè®°å½•æ•°
                    pass

        except Exception:
            pass

        return stats

    def _clean_config_for_serialization(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…ç†é…ç½®ä¸­å¯èƒ½å¯¼è‡´å¾ªç¯å¼•ç”¨çš„å¯¹è±¡"""
        import copy
        clean_config = {}

        for key, value in config.items():
            if isinstance(value, dict):
                clean_config[key] = self._clean_dict(value)
            elif isinstance(value, list):
                clean_config[key] = [self._clean_dict(item) if isinstance(item, dict) else item for item in value]
            else:
                clean_config[key] = value

        return clean_config

    def _clean_dict(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…ç†å­—å…¸ä¸­çš„ä¸å¯åºåˆ—åŒ–å¯¹è±¡"""
        clean = {}
        exclude_keys = {'class', 'classLoader', 'module', '__class__', '__dict__'}

        for k, v in data.items():
            if k not in exclude_keys and not callable(v):
                try:
                    json.dumps(v)  # æµ‹è¯•æ˜¯å¦å¯åºåˆ—åŒ–
                    clean[k] = v
                except (TypeError, ValueError):
                    clean[k] = str(v)  # è½¬ä¸ºå­—ç¬¦ä¸²

        return clean


# æ‰©å±•ç°æœ‰çš„sync API
class EnhancedSyncService:
    """å¢å¼ºçš„æ•°æ®åŒæ­¥æœåŠ¡"""

    def __init__(self):
        self.datax_service = DataXIntegrationService()

    async def execute_sync_task(self, task_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®åŒæ­¥ä»»åŠ¡"""
        try:
            logger.info(f"éªŒè¯åŒæ­¥é…ç½®: {task_config}")
            """éªŒè¯åŒæ­¥é…ç½®"""
            required_fields = ['source', 'target', 'id']
            for field in required_fields:
                if field not in task_config:
                    raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")

            # éªŒè¯æ•°æ®æºé…ç½®
            source = task_config['source']
            if 'type' not in source:
                raise ValueError("æ•°æ®æºé…ç½®ä¸å®Œæ•´ï¼šç¼ºå°‘typeå­—æ®µ")

            # éªŒè¯ç›®æ ‡é…ç½®
            target = task_config['target']
            if 'type' not in target:
                raise ValueError("ç›®æ ‡é…ç½®ä¸å®Œæ•´ï¼šç¼ºå°‘typeå­—æ®µ")

            # ğŸ†• æ–°å¢ï¼šHiveç›®æ ‡çš„ç‰¹æ®ŠéªŒè¯
            if target.get('type', '').lower() == 'hive':
                if 'table' not in target:
                    raise ValueError("Hiveç›®æ ‡é…ç½®ç¼ºå°‘tableå­—æ®µ")
            # éªŒè¯é…ç½®
            self._validate_sync_config(task_config)
            logger.info("é…ç½®éªŒè¯é€šè¿‡")
            # æ ¹æ®åŒæ­¥ç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹å¼
            sync_type = task_config.get('sync_type', 'full')
            logger.info(f"åŒæ­¥ç±»å‹: {sync_type}")
            if sync_type == 'incremental':
                # å¢é‡åŒæ­¥é€»è¾‘
                return await self._execute_incremental_sync(task_config)
            else:
                # å…¨é‡åŒæ­¥
                return await self._execute_full_sync(task_config)

        except Exception as e:
            error_msg = f"åŒæ­¥ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {
                "success": False,
                "error": error_msg,
                "error_type": "sync_task_error",
                "task_id": task_config.get('id'),
                "traceback": traceback.format_exc()
            }

    async def _execute_full_sync(self, task_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå…¨é‡æ•°æ®åŒæ­¥"""
        return await self.datax_service.create_sync_task(task_config)

    async def _execute_incremental_sync(self, task_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå¢é‡æ•°æ®åŒæ­¥"""
        # è·å–ä¸Šæ¬¡åŒæ­¥çš„æ°´ä½çº¿
        watermark = await self._get_sync_watermark(task_config['id'])

        # ä¿®æ”¹æŸ¥è¯¢æ¡ä»¶ä»¥æ”¯æŒå¢é‡
        if watermark:
            incremental_column = task_config.get('incremental_column', 'updated_at')
            original_query = task_config['source'].get('query', f"SELECT * FROM {task_config['source']['table']}")

            # æ·»åŠ å¢é‡æ¡ä»¶
            if 'WHERE' in original_query.upper():
                incremental_query = f"{original_query} AND {incremental_column} > '{watermark}'"
            else:
                incremental_query = f"{original_query} WHERE {incremental_column} > '{watermark}'"

            task_config['source']['query'] = incremental_query

        result = await self.datax_service.create_sync_task(task_config)

        # æ›´æ–°æ°´ä½çº¿
        if result.get('success'):
            await self._update_sync_watermark(task_config['id'], datetime.now())

        return result

    async def _get_sync_watermark(self, task_id: str) -> Optional[str]:
        """è·å–åŒæ­¥æ°´ä½çº¿"""
        # ä»Redisæˆ–æ•°æ®åº“è·å–ä¸Šæ¬¡åŒæ­¥çš„æ—¶é—´æˆ³
        # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„å­˜å‚¨æ–¹å¼å®ç°
        pass

    async def _update_sync_watermark(self, task_id: str, watermark: datetime):
        """æ›´æ–°åŒæ­¥æ°´ä½çº¿"""
        # ä¿å­˜æ–°çš„æ°´ä½çº¿åˆ°Redisæˆ–æ•°æ®åº“
        # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„å­˜å‚¨æ–¹å¼å®ç°
        pass

    def _validate_sync_config(self, config: Dict[str, Any]):
        """éªŒè¯åŒæ­¥é…ç½®"""
        required_fields = ['source', 'target', 'id']
        for field in required_fields:
            if field not in config:
                raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")

        # éªŒè¯æ•°æ®æºé…ç½®
        source = config['source']
        if 'type' not in source or 'host' not in source:
            raise ValueError("æ•°æ®æºé…ç½®ä¸å®Œæ•´")

        # éªŒè¯ç›®æ ‡é…ç½®
        target = config['target']
        if 'type' not in target:
            raise ValueError("ç›®æ ‡é…ç½®ä¸å®Œæ•´")