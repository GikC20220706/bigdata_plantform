# app/services/smart_sync_service.py
"""
Êô∫ËÉΩÊï∞ÊçÆÂêåÊ≠•ÊúçÂä° - ÊîØÊåÅÊãñÊãΩÂºèÊìç‰ΩúÂíåËá™Âä®Âª∫Ë°®
"""

import asyncio
import json
import os
import tempfile
import traceback
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
from loguru import logger

from app.services.datax_service import EnhancedSyncService
from app.services.optimized_data_integration_service import get_optimized_data_integration_service
from app.utils.response import create_response
from app.models.sync_history import SyncHistory, SyncTableHistory
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_, or_, desc, func
from app.utils.database import get_async_db


class SmartSyncService:
    """Êô∫ËÉΩÊï∞ÊçÆÂêåÊ≠•ÊúçÂä°"""

    def __init__(self):
        self.datax_service = EnhancedSyncService()
        self.integration_service = get_optimized_data_integration_service()

    async def get_sync_history(
            self,
            page: int = 1,
            page_size: int = 20,
            status: Optional[str] = None,
            source_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """‰ªéÊï∞ÊçÆÂ∫ìËé∑ÂèñÁúüÂÆûÁöÑÂêåÊ≠•ÂéÜÂè≤ËÆ∞ÂΩï"""
        try:
            # Ëé∑ÂèñÊï∞ÊçÆÂ∫ìËøûÊé•
            async with get_async_db() as db:
                # ÊûÑÂª∫Êü•ËØ¢Êù°‰ª∂
                conditions = []
                if status:
                    conditions.append(SyncHistory.status == status)
                if source_name:
                    conditions.append(SyncHistory.source_name.like(f"%{source_name}%"))

                # Êü•ËØ¢ÊÄªÊï∞
                count_query = select(func.count(SyncHistory.id))
                if conditions:
                    count_query = count_query.where(and_(*conditions))

                total_result = await db.execute(count_query)
                total = total_result.scalar()

                # ÂàÜÈ°µÊü•ËØ¢
                query = select(SyncHistory).order_by(desc(SyncHistory.created_at))
                if conditions:
                    query = query.where(and_(*conditions))

                offset = (page - 1) * page_size
                query = query.offset(offset).limit(page_size)

                result = await db.execute(query)
                records = result.scalars().all()

                # ÊûÑÂª∫ÂìçÂ∫îÊï∞ÊçÆ
                history_list = []
                for record in records:
                    history_list.append({
                        "sync_id": record.sync_id,
                        "source_name": record.source_name,
                        "target_name": record.target_name,
                        "status": record.status,
                        "start_time": record.start_time,
                        "end_time": record.end_time,
                        "duration_seconds": record.duration_seconds,
                        "total_tables": record.total_tables,
                        "success_tables": record.success_tables,
                        "failed_tables": record.failed_tables,
                        "total_records": record.total_records,
                        "created_by": record.created_by,
                        "current_step": record.current_step,
                        "progress": record.progress,
                        "error_message": record.error_message
                    })

                return {
                    "history": history_list,
                    "total": total,
                    "page": page,
                    "page_size": page_size,
                    "has_more": offset + page_size < total,
                    "total_pages": (total + page_size - 1) // page_size
                }

        except Exception as e:
            logger.error(f"‰ªéÊï∞ÊçÆÂ∫ìËé∑ÂèñÂêåÊ≠•ÂéÜÂè≤Â§±Ë¥•: {e}")
            return {
                "history": [],
                "total": 0,
                "page": page,
                "page_size": page_size,
                "has_more": False,
                "error": str(e)
            }

    async def save_sync_history(self, sync_data: Dict[str, Any]) -> bool:
        """‰øùÂ≠òÂêåÊ≠•ÂéÜÂè≤Âà∞Êï∞ÊçÆÂ∫ì"""
        try:
            async with get_async_db() as db:
                sync_record = SyncHistory(
                    sync_id=sync_data.get('sync_id'),
                    source_name=sync_data.get('source_name'),
                    target_name=sync_data.get('target_name'),
                    status=sync_data.get('status', 'running'),
                    start_time=sync_data.get('start_time'),
                    end_time=sync_data.get('end_time'),
                    duration_seconds=sync_data.get('duration_seconds'),
                    total_tables=sync_data.get('total_tables', 0),
                    success_tables=sync_data.get('success_tables', 0),
                    failed_tables=sync_data.get('failed_tables', 0),
                    total_records=sync_data.get('total_records', 0),
                    sync_mode=sync_data.get('sync_mode'),
                    parallel_jobs=sync_data.get('parallel_jobs', 1),
                    created_by=sync_data.get('created_by'),
                    error_message=sync_data.get('error_message'),
                    sync_result=sync_data.get('sync_result'),
                    current_step=sync_data.get('current_step'),
                    progress=sync_data.get('progress', 0)
                )

                db.add(sync_record)
                await db.commit()

                logger.info(f"ÂêåÊ≠•ÂéÜÂè≤‰øùÂ≠òÊàêÂäü: {sync_data.get('sync_id')}")
                return True

        except Exception as e:
            logger.error(f"‰øùÂ≠òÂêåÊ≠•ÂéÜÂè≤Â§±Ë¥•: {e}")
            await db.rollback()
            return False

    async def update_sync_status(self, sync_id: str, status_data: Dict[str, Any]) -> bool:
        """Êõ¥Êñ∞Êï∞ÊçÆÂ∫ì‰∏≠ÁöÑÂêåÊ≠•Áä∂ÊÄÅ"""
        try:
            async with get_async_db() as db:
                # Êü•ÊâæÁé∞ÊúâËÆ∞ÂΩï
                result = await db.execute(
                    select(SyncHistory).where(SyncHistory.sync_id == sync_id)
                )
                record = result.scalar_one_or_none()

                if record:
                    # Êõ¥Êñ∞Áé∞ÊúâËÆ∞ÂΩï
                    for key, value in status_data.items():
                        if hasattr(record, key):
                            setattr(record, key, value)
                    record.updated_at = datetime.now()
                else:
                    # ÂàõÂª∫Êñ∞ËÆ∞ÂΩï
                    record = SyncHistory(sync_id=sync_id, **status_data)
                    db.add(record)

                await db.commit()
                return True

        except Exception as e:
            logger.error(f"Êõ¥Êñ∞ÂêåÊ≠•Áä∂ÊÄÅÂ§±Ë¥•: {e}")
            await db.rollback()
            return False
    async def analyze_sync_plan(self, sync_request: Dict[str, Any]) -> Dict[str, Any]:
        """ÂàÜÊûêÂêåÊ≠•ËÆ°ÂàíÔºåËá™Âä®ÁîüÊàêÂêåÊ≠•Á≠ñÁï•"""
        try:
            source_name = sync_request['source_name']
            target_name = sync_request['target_name']
            tables = sync_request['tables']
            sync_mode = sync_request.get('sync_mode', 'single')

            # Ëé∑ÂèñÊ∫êÂíåÁõÆÊ†áÊï∞ÊçÆÊ∫êÈÖçÁΩÆ
            source_config = await self._get_data_source_config(source_name)
            target_config = await self._get_data_source_config(target_name)

            if not source_config or not target_config:
                return {
                    "success": False,
                    "error": "Êï∞ÊçÆÊ∫êÈÖçÁΩÆ‰∏çÂ≠òÂú®"
                }

            # ‰øùÂ≠òÊ∫êÊï∞ÊçÆÂ∫ìÁ±ªÂûãÔºå‰æõÂ≠óÊÆµÈïøÂ∫¶ÂàÜÊûê‰ΩøÁî®
            self._current_source_type = source_config.get('type', 'mysql')

            # ÂàÜÊûêÊØè‰∏™Ë°®ÁöÑÂêåÊ≠•ËÆ°Âàí
            sync_plans = []
            total_estimated_time = 0
            total_estimated_rows = 0

            for table_info in tables:
                source_table = table_info['source_table']
                target_table = table_info['target_table']

                logger.info(f"ÂàÜÊûêË°®: {source_table} -> {target_table}")

                # ‰øùÂ≠òÂΩìÂâçË°®‰ø°ÊÅØ
                self._current_source_name = source_name
                self._current_table_name = source_table
                self._current_target_type = target_config['type']

                # Ëé∑ÂèñÊ∫êË°®ÂÖÉÊï∞ÊçÆ
                source_metadata = await self.integration_service.get_table_metadata(
                    source_name, source_table
                )

                if not source_metadata.get('success'):
                    logger.error(f"Ëé∑ÂèñÊ∫êË°® {source_table} ÂÖÉÊï∞ÊçÆÂ§±Ë¥•")
                    continue

                table_meta = source_metadata['metadata']

                # Ê£ÄÊü•ÁõÆÊ†áË°®ÊòØÂê¶Â≠òÂú®
                target_exists = await self._check_target_table_exists(
                    target_name, target_table
                )

                # ÁîüÊàêÂêåÊ≠•Á≠ñÁï•
                strategy = await self._generate_sync_strategy(
                    source_config, target_config, table_meta, target_exists
                )

                # ‰º∞ÁÆóÂêåÊ≠•Êó∂Èó¥ÂíåÊï∞ÊçÆÈáè
                estimated_rows = table_meta.get('statistics', {}).get('row_count', 0)
                estimated_time = self._estimate_sync_time(estimated_rows, source_config['type'], target_config['type'])

                sync_plan = {
                    "source_table": source_table,
                    "target_table": target_table,
                    "estimated_rows": estimated_rows,
                    "estimated_time_minutes": estimated_time,
                    "target_exists": target_exists,
                    "strategy": strategy,
                    "schema_mapping": await self._generate_schema_mapping(table_meta, target_config['type']),
                    "warnings": self._check_compatibility_warnings(table_meta, source_config, target_config)
                }

                sync_plans.append(sync_plan)
                total_estimated_time += estimated_time
                total_estimated_rows += estimated_rows

            return {
                "success": True,
                "sync_mode": sync_mode,
                "source_name": source_name,
                "target_name": target_name,
                "total_tables": len(sync_plans),
                "total_estimated_rows": total_estimated_rows,
                "total_estimated_time_minutes": total_estimated_time,
                "sync_plans": sync_plans,
                "global_strategy": self._determine_global_strategy(sync_mode, sync_plans),
                "recommended_parallel_jobs": self._recommend_parallel_jobs(total_estimated_rows),
                "analysis_time": datetime.now()
            }

        except Exception as e:
            logger.error(f"ÂàÜÊûêÂêåÊ≠•ËÆ°ÂàíÂ§±Ë¥•: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def execute_smart_sync(self, sync_plan: Dict[str, Any]) -> Dict[str, Any]:
        """ÊâßË°åÊô∫ËÉΩÂêåÊ≠•"""
        try:
            sync_id = f"smart_sync_{int(datetime.now().timestamp())}"
            logger.info(f"ÂºÄÂßãÊâßË°åÊô∫ËÉΩÂêåÊ≠•: {sync_id}")

            # È¢ÑÊ£ÄÊü•
            precheck_result = await self._precheck_sync_conditions(sync_plan)
            if not precheck_result['success']:
                return precheck_result

            # Ê£ÄÊµãÁõÆÊ†áÁ±ªÂûã
            target_config = await self._get_data_source_config(sync_plan['target_name'])
            is_hive_target = (target_config and target_config.get('type', '').lower() == 'hive')

            logger.info(f"ÁõÆÊ†áÁ±ªÂûã: {target_config.get('type', 'unknown')}, HiveÁõÆÊ†á: {is_hive_target}")

            # üîß ‰øÆÂ§çÔºöÊ≠£Á°ÆÁöÑÊâßË°åÈ°∫Â∫è
            hdfs_result = None
            table_creation_results = []

            if is_hive_target:
                logger.info("=== HiveÂêåÊ≠•ÊµÅÁ®ãÂºÄÂßã ===")

                logger.info("Ê≠•È™§1: ÂàõÂª∫HiveÂ§ñÈÉ®Ë°®...")
                table_creation_results = await self._create_hive_external_tables(sync_plan)

                # Ê£ÄÊü•Âª∫Ë°®ÁªìÊûú
                table_creation_success = all(result.get('success', False) for result in table_creation_results)
                if not table_creation_success:
                    logger.error("HiveÂ§ñÈÉ®Ë°®ÂàõÂª∫Â§±Ë¥•ÔºåÁªàÊ≠¢ÂêåÊ≠•")
                    return {
                        "success": False,
                        "error": "HiveÂ§ñÈÉ®Ë°®ÂàõÂª∫Â§±Ë¥•",
                        "table_creation_results": table_creation_results
                    }

                logger.info("Ê≠•È™§2: ÂàõÂª∫HDFSÁõÆÂΩï...")
                hdfs_result = await self._create_hdfs_directories(sync_plan)

                # Ê£ÄÊü•HDFSÁõÆÂΩïÂàõÂª∫ÁªìÊûú
                if not hdfs_result.get('success', False):
                    logger.error("HDFSÁõÆÂΩïÂàõÂª∫Â§±Ë¥•ÔºåÁªàÊ≠¢ÂêåÊ≠•")
                    return {
                        "success": False,
                        "error": "HDFSÁõÆÂΩïÂàõÂª∫Â§±Ë¥•",
                        "hdfs_result": hdfs_result,
                        "table_creation_results": table_creation_results
                    }

                logger.info("HiveË°®ÂíåHDFSÁõÆÂΩïÂáÜÂ§áÂÆåÊàêÔºåÂºÄÂßãÊï∞ÊçÆÂêåÊ≠•...")

            else:
                logger.info("ÊôÆÈÄöÊï∞ÊçÆÂ∫ìÁõÆÊ†áÔºåÂàõÂª∫Â∏∏ËßÑË°®...")
                table_creation_results = await self._create_target_tables(sync_plan)

            # ÊâßË°åÊï∞ÊçÆÂêåÊ≠•
            sync_results = []
            successful_syncs = 0
            failed_syncs = 0

            logger.info("Ê≠•È™§3: ÂºÄÂßãÊâßË°åÊï∞ÊçÆÂêåÊ≠•...")

            for plan in sync_plan['sync_plans']:
                try:
                    logger.info(f"ÂêåÊ≠•Ë°®: {plan['source_table']} -> {plan['target_table']}")

                    # üîß ÂΩªÂ∫ïÊ∏ÖÁêÜÈÖçÁΩÆÔºåÈò≤Ê≠¢Âæ™ÁéØÂºïÁî®
                    clean_sync_plan = {
                        'source_name': sync_plan['source_name'],
                        'target_name': sync_plan['target_name'],
                        'sync_mode': sync_plan.get('sync_mode', 'full'),
                        'recommended_parallel_jobs': sync_plan.get('recommended_parallel_jobs', 4)
                    }

                    clean_plan = {
                        'source_table': plan['source_table'],
                        'target_table': plan['target_table'],
                        'target_exists': plan.get('target_exists', False),
                        'strategy': plan.get('strategy', 'full_copy')
                        # üîß ÊïÖÊÑè‰∏çÂåÖÂê´ schema_mappingÔºåÈÅøÂÖç55‰∏™Â≠óÊÆµÁöÑÂ§çÊùÇÂØπË±°
                    }

                    logger.info(
                        f"Ê∏ÖÁêÜÂêéÁöÑÈÖçÁΩÆ - Ê∫êË°®: {clean_plan['source_table']}, ÁõÆÊ†áË°®: {clean_plan['target_table']}")

                    # ÁîüÊàêDataXÈÖçÁΩÆ
                    datax_config = await self._generate_datax_config(clean_sync_plan, clean_plan)
                    logger.info(f"DataXÈÖçÁΩÆÁîüÊàêÊàêÂäü")

                    # ÊâßË°åÂêåÊ≠•
                    logger.info(f"‚ö° ÂºÄÂßãÊâßË°åDataXÂêåÊ≠•‰ªªÂä°...")
                    sync_result = await self.datax_service.execute_sync_task(datax_config)
                    logger.info(f"DataXÊâßË°åÁªìÊûú: success={sync_result.get('success')}")

                    if sync_result.get('success'):
                        successful_syncs += 1
                        logger.info(f"Ë°® {plan['source_table']} ÂêåÊ≠•ÊàêÂäü")

                        # üîß ‰øÆÂ§çÔºöÂè™ÊúâDataXÂêåÊ≠•ÊàêÂäüÂêéÊâçËøõË°åHiveÂêéÁª≠Êìç‰Ωú
                        if is_hive_target:
                            logger.info("Ê≠•È™§4: Âà∑Êñ∞HiveÂàÜÂå∫...")
                            partition_result = await self._refresh_hive_partition(sync_plan, plan)
                            sync_result['partition_refresh'] = partition_result
                            logger.info(f"HiveÂàÜÂå∫Âà∑Êñ∞ÁªìÊûú: {partition_result.get('success')}")

                        # È™åËØÅÊï∞ÊçÆÂÆåÊï¥ÊÄß
                        # verification = await self._verify_sync_integrity(sync_plan, plan)
                        # sync_result['verification'] = verification
                        # logger.info(f"üîç Êï∞ÊçÆÈ™åËØÅÁªìÊûú: {verification}")

                    else:
                        failed_syncs += 1
                        error_msg = sync_result.get('error', 'Êú™Áü•ÈîôËØØ')
                        logger.error(f"Ë°® {plan['source_table']} ÂêåÊ≠•Â§±Ë¥•: {error_msg}")

                    sync_results.append({
                        "table": plan['source_table'],
                        "target_table": plan['target_table'],
                        "result": sync_result
                    })

                except Exception as e:
                    failed_syncs += 1
                    error_msg = f"ÂêåÊ≠•ÂºÇÂ∏∏: {str(e)}"
                    logger.error(f"Ë°® {plan['source_table']} ÂêåÊ≠•ÂºÇÂ∏∏: {error_msg}")
                    logger.error(f"ÂºÇÂ∏∏ËØ¶ÊÉÖ: {traceback.format_exc()}")

                    sync_results.append({
                        "table": plan['source_table'],
                        "target_table": plan['target_table'],
                        "result": {
                            "success": False,
                            "error": error_msg,
                            "error_type": "exception",
                            "traceback": traceback.format_exc()
                        }
                    })

            # ÁîüÊàêÂêåÊ≠•Êä•Âëä
            sync_report = {
                "sync_id": sync_id,
                "success": failed_syncs == 0,
                "total_tables": len(sync_plan['sync_plans']),
                "successful_syncs": successful_syncs,
                "failed_syncs": failed_syncs,
                "sync_results": sync_results,
                "table_creation_results": table_creation_results,
                "hdfs_directories": hdfs_result if is_hive_target else None,
                "execution_time": datetime.now(),
                "summary": self._generate_sync_summary(sync_results),
                "workflow": "hive" if is_hive_target else "standard"
            }

            if is_hive_target:
                logger.info("=== HiveÂêåÊ≠•ÊµÅÁ®ãÂÆåÊàê ===")

            return sync_report

        except Exception as e:
            logger.error(f"ÊâßË°åÊô∫ËÉΩÂêåÊ≠•Â§±Ë¥•: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _is_hive_target(self, sync_plan: Dict[str, Any]) -> bool:
        """ÂºÇÊ≠•Âà§Êñ≠ÊòØÂê¶‰∏∫HiveÁõÆÊ†á"""
        try:
            target_config = await self._get_data_source_config(sync_plan['target_name'])
            return target_config and target_config.get('type', '').lower() == 'hive'
        except:
            return False

    async def _create_hive_external_tables(self, sync_plan: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ÂàõÂª∫HiveÂ§ñÈÉ®Ë°®"""
        creation_results = []
        target_name = sync_plan['target_name']

        for plan in sync_plan['sync_plans']:
            try:
                # ÁîüÊàêHiveÂ§ñÈÉ®Ë°®SQL
                create_sql = await self._generate_hive_external_table_sql(
                    plan['schema_mapping'],
                    plan['target_table'],
                    sync_plan['target_name']
                )

                # ÊâßË°åÂª∫Ë°®
                result = await self._execute_hive_table_creation(target_name, create_sql, plan['target_table'])

                creation_results.append({
                    "table": plan['target_table'],
                    "success": result['success'],
                    "sql": create_sql,
                    "message": result.get('message', '')
                })

            except Exception as e:
                logger.error(f"ÂàõÂª∫HiveÂ§ñÈÉ®Ë°®Â§±Ë¥• {plan['target_table']}: {e}")
                creation_results.append({
                    "table": plan['target_table'],
                    "success": False,
                    "error": str(e)
                })

        return creation_results

    async def _generate_hive_external_table_sql(self, schema_mapping: Dict[str, Any],
                                                table_name: str, target_source: str) -> str:
        """ÁîüÊàêHiveÂ§ñÈÉ®Ë°®SQL - ‰øÆÂ§çÁâàÊú¨"""
        target_config = await self._get_data_source_config(target_source)
        columns = schema_mapping.get('columns', [])

        if not columns:
            raise ValueError("Áº∫Â∞ëÂ≠óÊÆµÊò†Â∞Ñ‰ø°ÊÅØ")

        # ÊûÑÂª∫Â≠óÊÆµÂÆö‰πâ
        column_definitions = []
        for col in columns:
            col_name = col['name']
            # ÂéªÊéâÂèåÂºïÂè∑ÔºåHiveÂ≠óÊÆµÂêç‰∏çÈúÄË¶ÅÂºïÂè∑
            clean_col_name = col_name.strip('"')
            # Â∞ÜÂ≠óÊÆµÁ±ªÂûãËΩ¨Êç¢‰∏∫HiveÁ±ªÂûã
            hive_type = self._convert_to_hive_type(col['target_type'])
            column_definitions.append(f"    {clean_col_name} {hive_type}")

        str1 = ',\n'.join(column_definitions)  # Â≠óÊÆµÂÆö‰πâÈÉ®ÂàÜ
        database = target_config.get('database', 'default')

        # üîß ÈáçË¶Å‰øÆÂ§çÔºöË°®ÂêçÂ§ÑÁêÜÈÄªËæë
        if table_name.startswith('ODS_'):
            final_table_name = table_name  # Â¶ÇÊûúÂ∑≤ÁªèÊúâODS_ÂâçÁºÄÔºåÁõ¥Êé•‰ΩøÁî®
        else:
            final_table_name = f"ODS_{table_name}"  # Ê∑ªÂä†ODS_ÂâçÁºÄ

        # üîß ÂÖ≥ÈîÆ‰øÆÂ§çÔºöÁîüÊàêÊ≠£Á°ÆÁöÑLOCATIONË∑ØÂæÑÔºå‰∏éDataXË∑ØÂæÑ‰∏ÄËá¥
        base_path = target_config.get('base_path', '/user/hive/warehouse')
        if database != 'default':
            table_location = f"{base_path}/{database}.db/{final_table_name}"
        else:
            table_location = f"{base_path}/{final_table_name}"

        # üîß ÊåâÁÖß‰Ω†ÁöÑÂª∫Ë°®Ê†ºÂºè + ‰øÆÂ§çÂàÜÂå∫ÂíåORC
        sql = f"""CREATE EXTERNAL TABLE IF NOT EXISTS {database.upper()}.{final_table_name.upper()} (
    {str1}
    ) PARTITIONED BY (dt string) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '\\t' 
    STORED AS ORC  
    LOCATION '{table_location}'
    TBLPROPERTIES ('orc.compress' = 'snappy')"""

        logger.info(f"ÁîüÊàêHiveÂ§ñÈÉ®Ë°®SQL: {sql}")
        logger.info(f"Ë°®‰ΩçÁΩÆ: {table_location}")
        return sql

    def _convert_to_hive_type(self, datax_type: str) -> str:
        """Â∞ÜDataXÂ≠óÊÆµÁ±ªÂûãËΩ¨Êç¢‰∏∫HiveÁ±ªÂûã"""
        type_mapping = {
            'VARCHAR': 'STRING',
            'TEXT': 'STRING',
            'INT': 'INT',
            'INTEGER': 'INT',
            'BIGINT': 'BIGINT',
            'DECIMAL': 'DECIMAL(10,2)',
            'DOUBLE': 'DOUBLE',
            'FLOAT': 'FLOAT',
            'DATE': 'DATE',
            'DATETIME': 'TIMESTAMP',
            'TIMESTAMP': 'TIMESTAMP',
            'BOOLEAN': 'BOOLEAN'
        }

        # ÊèêÂèñÂü∫Á°ÄÁ±ªÂûãÔºàÂéªÊéâÈïøÂ∫¶Á≠âÔºâ
        base_type = datax_type.split('(')[0].upper()
        return type_mapping.get(base_type, 'STRING')

    async def _execute_hive_table_creation(self, target_name: str, create_sql: str, table_name: str) -> Dict[str, Any]:
        """ÊâßË°åHiveÂª∫Ë°®SQL"""
        try:
            logger.info(f"ÂºÄÂßãÂàõÂª∫HiveÂ§ñÈÉ®Ë°®: {table_name}")
            logger.info(f"Âª∫Ë°®SQL: {create_sql}")

            # ‰ΩøÁî®ÈõÜÊàêÊúçÂä°ÊâßË°åSQL
            result = await self.integration_service.execute_query(
                source_name=target_name,
                query=create_sql,
                database=None,
                schema=None,
                limit=1
            )

            logger.info(f"HiveÂª∫Ë°®ÊâßË°åÁªìÊûú: {result}")

            return {
                "success": result.get('success', False),
                "message": f"HiveÂ§ñÈÉ®Ë°® {table_name} ÂàõÂª∫ÊàêÂäü" if result.get('success') else result.get('error',
                                                                                                        'ÂàõÂª∫Â§±Ë¥•'),
                "sql": create_sql
            }

        except Exception as e:
            logger.error(f"HiveÂª∫Ë°®Â§±Ë¥•: {e}")
            return {
                "success": False,
                "message": f"HiveÂª∫Ë°®Â§±Ë¥•: {str(e)}",
                "sql": create_sql
            }

    async def _refresh_hive_partition(self, sync_plan: Dict[str, Any], table_plan: Dict[str, Any]) -> Dict[str, Any]:
        """Âà∑Êñ∞HiveÂàÜÂå∫"""
        try:
            target_config = await self._get_data_source_config(sync_plan['target_name'])
            database = target_config.get('database', 'default')

            # üîß ‰øÆÂ§çÔºö‰ΩøÁî®‰∏éDataXÈÖçÁΩÆ‰∏ÄËá¥ÁöÑË°®ÂêçÂ§ÑÁêÜÈÄªËæë
            original_table_name = table_plan['target_table']
            if original_table_name.startswith('ODS_'):
                final_table_name = original_table_name
            else:
                final_table_name = f"ODS_{original_table_name}"

            # Ëé∑ÂèñÂàÜÂå∫ÂÄº
            from datetime import datetime
            partition_value = datetime.now().strftime('%Y-%m-%d')

            # üîß ‰øÆÂ§çÔºöÊûÑÂª∫Ê≠£Á°ÆÁöÑÂàÜÂå∫Ë∑ØÂæÑ
            base_path = target_config.get('base_path', '/user/hive/warehouse')
            if database != 'default':
                partition_location = f"{base_path}/{database}.db/{final_table_name}/dt={partition_value}"
            else:
                partition_location = f"{base_path}/{final_table_name}/dt={partition_value}"

            # üîß ‰øÆÂ§çÔºöÊ∑ªÂä†ÂàÜÂå∫Êó∂ÊåáÂÆöLOCATION
            add_partition_sql = f"""ALTER TABLE {database.upper()}.{final_table_name.upper()} 
    ADD IF NOT EXISTS PARTITION (dt='{partition_value}') 
    LOCATION '{partition_location}'"""

            logger.info(f"Ê∑ªÂä†ÂàÜÂå∫SQL: {add_partition_sql}")

            # ÊâßË°åÊ∑ªÂä†ÂàÜÂå∫
            result = await self.integration_service.execute_query(
                source_name=sync_plan['target_name'],
                query=add_partition_sql,
                limit=1
            )

            if result.get('success'):
                # Âà∑Êñ∞ÂÖÉÊï∞ÊçÆ
                repair_sql = f"MSCK REPAIR TABLE {database.upper()}.{final_table_name.upper()}"
                repair_result = await self.integration_service.execute_query(
                    source_name=sync_plan['target_name'],
                    query=repair_sql,
                    limit=1
                )

                return {
                    "success": True,
                    "partition_added": True,
                    "partition_value": partition_value,
                    "partition_location": partition_location,  # üÜï Ê∑ªÂä†‰ΩçÁΩÆ‰ø°ÊÅØ
                    "table_name": final_table_name,  # üÜï Ê∑ªÂä†Ë°®Âêç‰ø°ÊÅØ
                    "metadata_refreshed": repair_result.get('success', False),
                    "message": f"ÂàÜÂå∫ dt={partition_value} Ê∑ªÂä†ÊàêÂäü"
                }
            else:
                return {
                    "success": False,
                    "error": f"Ê∑ªÂä†ÂàÜÂå∫Â§±Ë¥•: {result.get('error')}"
                }

        except Exception as e:
            logger.error(f"Âà∑Êñ∞HiveÂàÜÂå∫Â§±Ë¥•: {e}")
            return {
                "success": False,
                "error": f"Âà∑Êñ∞ÂàÜÂå∫Â§±Ë¥•: {str(e)}"
            }

    async def _create_target_tables(self, sync_plan: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Ëá™Âä®ÂàõÂª∫ÁõÆÊ†áË°®"""
        creation_results = []
        target_name = sync_plan['target_name']

        for plan in sync_plan['sync_plans']:
            if not plan['target_exists']:
                try:
                    # ÁîüÊàêÂª∫Ë°®SQL
                    create_sql = await self._generate_create_table_sql(
                        plan['schema_mapping'],
                        plan['target_table'],
                        sync_plan['target_name']
                    )

                    # ÊâßË°åÂª∫Ë°®
                    result = await self._execute_create_table(target_name, create_sql)

                    creation_results.append({
                        "table": plan['target_table'],
                        "success": result['success'],
                        "sql": create_sql,
                        "message": result.get('message', '')
                    })

                except Exception as e:
                    creation_results.append({
                        "table": plan['target_table'],
                        "success": False,
                        "error": str(e)
                    })

        return creation_results

    async def _generate_create_table_sql(self, schema_mapping: Dict[str, Any],
                                         table_name: str, target_source: str) -> str:
        """ÁîüÊàêÂª∫Ë°®SQL"""
        target_config = await self._get_data_source_config(target_source)
        target_type = target_config['type'].lower()

        columns = []
        for col in schema_mapping['columns']:
            col_name = col['name']
            col_type = col['target_type']
            nullable = "NULL" if col.get('nullable', True) else "NOT NULL"

            # Ê†πÊçÆÊï∞ÊçÆÂ∫ìÁ±ªÂûã‰ΩøÁî®Ê≠£Á°ÆÁöÑÂºïÂè∑Ê†ºÂºè
            if target_type == 'kingbase':
                columns.append(f'    "{col_name}" {col_type} {nullable}')
            elif target_type == 'mysql':
                columns.append(f'    `{col_name}` {col_type} {nullable}')
            elif target_type == 'doris':
                columns.append(f'    `{col_name}` {col_type} {nullable}')
            else:
                # PostgreSQL, OracleÁ≠â‰ΩøÁî®ÂèåÂºïÂè∑Êàñ‰∏çÁî®ÂºïÂè∑
                columns.append(f'    "{col_name}" {col_type} {nullable}')
        newline = '\n'
        column_definitions = f',{newline}'.join(columns)

        if target_type == 'mysql':
            sql = f"""CREATE TABLE IF NOT EXISTS `{table_name}` (
        {column_definitions}
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;""".strip()

        elif target_type == 'doris':
            # DorisÂª∫Ë°®ËØ≠Âè•
            first_column = schema_mapping['columns'][0]['name']
            sql = f"""CREATE TABLE IF NOT EXISTS `{table_name}` (
        {column_definitions}
        ) ENGINE=OLAP
        DUPLICATE KEY(`{first_column}`)
        DISTRIBUTED BY HASH(`{first_column}`) BUCKETS 1
        PROPERTIES (
            "replication_allocation" = "tag.location.default: 1"
        );""".strip()

        elif target_type == 'postgresql':
            sql = f"""CREATE TABLE IF NOT EXISTS "{table_name}" (
        {column_definitions}
        );""".strip()

        elif target_type == 'kingbase':
            sql = f'''CREATE TABLE IF NOT EXISTS "{table_name}" (
            {column_definitions}
            );'''.strip()

        elif target_type == 'hive':
            sql = f"""CREATE TABLE IF NOT EXISTS `{table_name}` (
        {column_definitions}
        ) 
        STORED AS TEXTFILE
        LOCATION '/user/hive/warehouse/{table_name}';""".strip()

        elif target_type == 'oracle':
            sql = f"""CREATE TABLE "{table_name}" (
        {column_definitions}
        );""".strip()

        else:
            # ÈÄöÁî®SQL
            sql = f"""CREATE TABLE IF NOT EXISTS `{table_name}` (
        {column_definitions}
        );""".strip()

        logger.info(f"ÁîüÊàê{target_type}Âª∫Ë°®SQL: {sql}")
        return sql

    async def _generate_schema_mapping(self, table_metadata: Dict[str, Any], target_type: str) -> Dict[str, Any]:
        """ÁîüÊàêschemaÊò†Â∞Ñ"""
        source_columns = table_metadata.get('schema', {}).get('columns', [])

        if not source_columns:
            logger.error("Ê∫êË°®Ê≤°ÊúâÂ≠óÊÆµ‰ø°ÊÅØ")
            raise ValueError("Ê∫êË°®Â≠óÊÆµ‰ø°ÊÅØÁº∫Â§±")

        mapped_columns = []
        total_columns = len(source_columns)

        # Â¶ÇÊûúÊòØMySQLÁõÆÊ†á‰∏îÂ≠óÊÆµÂæàÂ§öÔºåÂàÜÊûêÂÆûÈôÖÂ≠óÊÆµÈïøÂ∫¶
        field_lengths = {}
        if target_type.lower() == 'mysql' and total_columns > 30:
            logger.info(f"MySQLÁõÆÊ†áË°®Êúâ{total_columns}‰∏™Â≠óÊÆµÔºåÂºÄÂßãÊô∫ËÉΩÈïøÂ∫¶ÂàÜÊûê...")
            try:
                # Ëé∑ÂèñÂΩìÂâçÂ§ÑÁêÜÁöÑÊ∫êË°®‰ø°ÊÅØ
                source_name = getattr(self, '_current_source_name', None)
                table_name = getattr(self, '_current_table_name', None)

                if source_name and table_name:
                    # ‰º†ÈÄíÂ≠óÊÆµ‰ø°ÊÅØËøõË°åÈïøÂ∫¶ÂàÜÊûê
                    field_lengths = await self._analyze_field_lengths(source_name, table_name, source_columns)
                else:
                    logger.warning("Êó†Ê≥ïËé∑ÂèñÊ∫êË°®‰ø°ÊÅØÔºåË∑≥ËøáÊô∫ËÉΩÈïøÂ∫¶ÂàÜÊûê")
            except Exception as e:
                logger.warning(f"Êô∫ËÉΩÈïøÂ∫¶ÂàÜÊûêÂ§±Ë¥•: {e}")

        for i, col in enumerate(source_columns):
            source_type = col.get('data_type', 'VARCHAR').upper()
            col_name = col.get('name', col.get('column_name', f'column_{i}'))

            if not col_name:
                col_name = f'column_{i}'

            # ‰ΩøÁî®Êô∫ËÉΩÈïøÂ∫¶ÊàñÈªòËÆ§Êò†Â∞Ñ
            if col_name in field_lengths and target_type.lower() == 'mysql':
                recommended_length = field_lengths[col_name]
                if recommended_length == 'TEXT':
                    target_col_type = 'TEXT'
                else:
                    target_col_type = f'VARCHAR({recommended_length})'
                logger.info(f"Â≠óÊÆµ {col_name} ‰ΩøÁî®Êô∫ËÉΩÈïøÂ∫¶: {target_col_type}")
            else:
                target_col_type = self._map_data_type(source_type, target_type)

                # Â¶ÇÊûúÊ≤°ÊúâÊô∫ËÉΩÂàÜÊûêÔºå‰ΩÜÂ≠óÊÆµÂæàÂ§öÔºåÈÄÇÂΩìÂáèÂ∞ëÈïøÂ∫¶
                if (target_type.lower() == 'mysql' and 'VARCHAR(255)' in target_col_type and
                        total_columns > 50 and col_name not in field_lengths):
                    target_col_type = 'VARCHAR(120)'  # ÈÄÇÂ∫¶ÂáèÂ∞ë
                    logger.info(f"Â≠óÊÆµ {col_name} ‰ΩøÁî®ÈªòËÆ§‰ºòÂåñÈïøÂ∫¶: {target_col_type}")

            mapped_columns.append({
                "name": col_name,
                "source_type": source_type,
                "target_type": target_col_type,
                "nullable": col.get('is_nullable', True),
                "length": col.get('character_maximum_length'),
                "precision": col.get('numeric_precision'),
                "scale": col.get('numeric_scale')
            })

        logger.info(f"Â≠óÊÆµÊò†Â∞ÑÁîüÊàêÂÆåÊàêÔºåÂÖ± {len(mapped_columns)} ‰∏™Â≠óÊÆµ")

        return {
            "columns": mapped_columns,
            "mapping_strategy": "intelligent" if field_lengths else "auto",
            "total_columns": len(mapped_columns),
            "analyzed_fields": len(field_lengths)
        }

    def _map_data_type(self, source_type: str, target_type: str) -> str:
        """Êï∞ÊçÆÁ±ªÂûãÊò†Â∞Ñ - Êâ©Â±ïÁâàÊîØÊåÅÂ§öÁßçÊï∞ÊçÆÂ∫ì"""

        # ÂÆåÊï¥ÁöÑÊï∞ÊçÆÁ±ªÂûãÊò†Â∞ÑË°®
        type_mappings = {
            # MySQL ‰Ωú‰∏∫Ê∫êÊï∞ÊçÆÂ∫ìÁöÑÊò†Â∞Ñ
            'mysql': {
                'mysql': {
                    'INT': 'INT', 'BIGINT': 'BIGINT', 'VARCHAR': 'VARCHAR',
                    'TEXT': 'TEXT', 'DATETIME': 'DATETIME', 'TIMESTAMP': 'TIMESTAMP',
                    'DECIMAL': 'DECIMAL', 'FLOAT': 'FLOAT', 'DOUBLE': 'DOUBLE',
                    'TINYINT': 'TINYINT', 'SMALLINT': 'SMALLINT', 'MEDIUMINT': 'MEDIUMINT',
                    'CHAR': 'CHAR', 'LONGTEXT': 'LONGTEXT', 'MEDIUMTEXT': 'MEDIUMTEXT',
                    'DATE': 'DATE', 'TIME': 'TIME', 'YEAR': 'YEAR',
                    'BINARY': 'BINARY', 'VARBINARY': 'VARBINARY', 'BLOB': 'BLOB'
                },
                'postgresql': {
                    'INT': 'INTEGER', 'BIGINT': 'BIGINT', 'VARCHAR': 'VARCHAR',
                    'TEXT': 'TEXT', 'DATETIME': 'TIMESTAMP', 'TIMESTAMP': 'TIMESTAMP',
                    'DECIMAL': 'NUMERIC', 'FLOAT': 'REAL', 'DOUBLE': 'DOUBLE PRECISION',
                    'TINYINT': 'SMALLINT', 'SMALLINT': 'SMALLINT', 'MEDIUMINT': 'INTEGER',
                    'CHAR': 'CHAR', 'LONGTEXT': 'TEXT', 'MEDIUMTEXT': 'TEXT',
                    'DATE': 'DATE', 'TIME': 'TIME', 'YEAR': 'INTEGER',
                    'BINARY': 'BYTEA', 'VARBINARY': 'BYTEA', 'BLOB': 'BYTEA'
                },
                'hive': {
                    'INT': 'INT', 'BIGINT': 'BIGINT', 'VARCHAR': 'STRING',
                    'TEXT': 'STRING', 'DATETIME': 'TIMESTAMP', 'TIMESTAMP': 'TIMESTAMP',
                    'DECIMAL': 'DECIMAL', 'FLOAT': 'FLOAT', 'DOUBLE': 'DOUBLE',
                    'TINYINT': 'TINYINT', 'SMALLINT': 'SMALLINT', 'MEDIUMINT': 'INT',
                    'CHAR': 'CHAR', 'LONGTEXT': 'STRING', 'MEDIUMTEXT': 'STRING',
                    'DATE': 'DATE', 'TIME': 'STRING', 'YEAR': 'INT',
                    'BINARY': 'BINARY', 'VARBINARY': 'BINARY', 'BLOB': 'BINARY'
                },
                'doris': {
                    'INT': 'INT', 'BIGINT': 'BIGINT', 'VARCHAR': 'VARCHAR',
                    'TEXT': 'STRING', 'DATETIME': 'DATETIME', 'TIMESTAMP': 'DATETIME',
                    'DECIMAL': 'DECIMAL', 'FLOAT': 'FLOAT', 'DOUBLE': 'DOUBLE',
                    'TINYINT': 'TINYINT', 'SMALLINT': 'SMALLINT', 'MEDIUMINT': 'INT',
                    'CHAR': 'CHAR', 'LONGTEXT': 'STRING', 'MEDIUMTEXT': 'STRING',
                    'DATE': 'DATE', 'TIME': 'STRING', 'YEAR': 'INT',
                    'BINARY': 'STRING', 'VARBINARY': 'STRING', 'BLOB': 'STRING'
                },
                'kingbase': {
                    'INT': 'INTEGER', 'BIGINT': 'BIGINT', 'VARCHAR': 'VARCHAR',
                    'TEXT': 'TEXT', 'DATETIME': 'TIMESTAMP', 'TIMESTAMP': 'TIMESTAMP',
                    'DECIMAL': 'NUMERIC', 'FLOAT': 'REAL', 'DOUBLE': 'DOUBLE PRECISION',
                    'TINYINT': 'SMALLINT', 'SMALLINT': 'SMALLINT', 'MEDIUMINT': 'INTEGER',
                    'CHAR': 'CHAR', 'LONGTEXT': 'TEXT', 'MEDIUMTEXT': 'TEXT',
                    'DATE': 'DATE', 'TIME': 'TIME', 'YEAR': 'INTEGER',
                    'BINARY': 'BYTEA', 'VARBINARY': 'BYTEA', 'BLOB': 'BYTEA'
                }
            },

            # KingBase ‰Ωú‰∏∫Ê∫êÊï∞ÊçÆÂ∫ìÁöÑÊò†Â∞Ñ
            'kingbase': {
                'mysql': {
                    'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'VARCHAR': 'VARCHAR',
                    'TEXT': 'TEXT', 'TIMESTAMP': 'DATETIME', 'TIMESTAMPTZ': 'TIMESTAMP',
                    'NUMERIC': 'DECIMAL', 'REAL': 'FLOAT', 'DOUBLE PRECISION': 'DOUBLE',
                    'SMALLINT': 'SMALLINT', 'CHAR': 'CHAR', 'BOOLEAN': 'TINYINT',
                    'DATE': 'DATE', 'TIME': 'TIME', 'INTERVAL': 'VARCHAR(50)',
                    'BYTEA': 'BLOB', 'UUID': 'VARCHAR(36)', 'JSON': 'JSON',
                    'JSONB': 'JSON', 'ARRAY': 'TEXT', 'SERIAL': 'INT AUTO_INCREMENT'
                },
                'hive': {
                    'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'VARCHAR': 'STRING',
                    'TEXT': 'STRING', 'TIMESTAMP': 'TIMESTAMP', 'TIMESTAMPTZ': 'TIMESTAMP',
                    'NUMERIC': 'DECIMAL', 'REAL': 'FLOAT', 'DOUBLE PRECISION': 'DOUBLE',
                    'SMALLINT': 'SMALLINT', 'CHAR': 'CHAR', 'BOOLEAN': 'BOOLEAN',
                    'DATE': 'DATE', 'TIME': 'STRING', 'INTERVAL': 'STRING',
                    'BYTEA': 'BINARY', 'UUID': 'STRING', 'JSON': 'STRING',
                    'JSONB': 'STRING', 'ARRAY': 'ARRAY<STRING>', 'SERIAL': 'INT'
                },
                'doris': {
                    'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'VARCHAR': 'VARCHAR',
                    'TEXT': 'STRING', 'TIMESTAMP': 'DATETIME', 'TIMESTAMPTZ': 'DATETIME',
                    'NUMERIC': 'DECIMAL', 'REAL': 'FLOAT', 'DOUBLE PRECISION': 'DOUBLE',
                    'SMALLINT': 'SMALLINT', 'CHAR': 'CHAR', 'BOOLEAN': 'BOOLEAN',
                    'DATE': 'DATE', 'TIME': 'STRING', 'INTERVAL': 'STRING',
                    'BYTEA': 'STRING', 'UUID': 'VARCHAR(36)', 'JSON': 'JSON',
                    'JSONB': 'JSON', 'ARRAY': 'STRING', 'SERIAL': 'INT'
                },
                'kingbase': {
                    'INTEGER': 'INTEGER', 'BIGINT': 'BIGINT', 'VARCHAR': 'VARCHAR',
                    'TEXT': 'TEXT', 'TIMESTAMP': 'TIMESTAMP', 'TIMESTAMPTZ': 'TIMESTAMPTZ',
                    'NUMERIC': 'NUMERIC', 'REAL': 'REAL', 'DOUBLE PRECISION': 'DOUBLE PRECISION',
                    'SMALLINT': 'SMALLINT', 'CHAR': 'CHAR', 'BOOLEAN': 'BOOLEAN',
                    'DATE': 'DATE', 'TIME': 'TIME', 'INTERVAL': 'INTERVAL',
                    'BYTEA': 'BYTEA', 'UUID': 'UUID', 'JSON': 'JSON',
                    'JSONB': 'JSONB', 'ARRAY': 'ARRAY', 'SERIAL': 'SERIAL'
                }
            },

            # Oracle ‰Ωú‰∏∫Ê∫êÊï∞ÊçÆÂ∫ìÁöÑÊò†Â∞Ñ
            'oracle': {
                'mysql': {
                    'NUMBER': 'DECIMAL', 'VARCHAR2': 'VARCHAR', 'CHAR': 'CHAR',
                    'CLOB': 'LONGTEXT', 'BLOB': 'LONGBLOB', 'DATE': 'DATETIME',
                    'TIMESTAMP': 'TIMESTAMP', 'RAW': 'VARBINARY', 'LONG': 'LONGTEXT',
                    'NVARCHAR2': 'VARCHAR', 'NCHAR': 'CHAR', 'NCLOB': 'LONGTEXT'
                },
                'hive': {
                    'NUMBER': 'DECIMAL', 'VARCHAR2': 'STRING', 'CHAR': 'CHAR',
                    'CLOB': 'STRING', 'BLOB': 'BINARY', 'DATE': 'TIMESTAMP',
                    'TIMESTAMP': 'TIMESTAMP', 'RAW': 'BINARY', 'LONG': 'STRING',
                    'NVARCHAR2': 'STRING', 'NCHAR': 'CHAR', 'NCLOB': 'STRING'
                },
                'doris': {
                    'NUMBER': 'DECIMAL', 'VARCHAR2': 'VARCHAR', 'CHAR': 'CHAR',
                    'CLOB': 'STRING', 'BLOB': 'STRING', 'DATE': 'DATETIME',
                    'TIMESTAMP': 'DATETIME', 'RAW': 'STRING', 'LONG': 'STRING',
                    'NVARCHAR2': 'VARCHAR', 'NCHAR': 'CHAR', 'NCLOB': 'STRING'
                },
                'kingbase': {
                    'NUMBER': 'NUMERIC', 'VARCHAR2': 'VARCHAR', 'CHAR': 'CHAR',
                    'CLOB': 'TEXT', 'BLOB': 'BYTEA', 'DATE': 'TIMESTAMP',
                    'TIMESTAMP': 'TIMESTAMP', 'RAW': 'BYTEA', 'LONG': 'TEXT',
                    'NVARCHAR2': 'VARCHAR', 'NCHAR': 'CHAR', 'NCLOB': 'TEXT'
                }
            },

            # PostgreSQL ‰Ωú‰∏∫Ê∫êÊï∞ÊçÆÂ∫ìÁöÑÊò†Â∞Ñ
            'postgresql': {
                'mysql': {
                    'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'VARCHAR': 'VARCHAR',
                    'TEXT': 'TEXT', 'TIMESTAMP': 'DATETIME', 'NUMERIC': 'DECIMAL',
                    'REAL': 'FLOAT', 'DOUBLE PRECISION': 'DOUBLE', 'SMALLINT': 'SMALLINT',
                    'CHAR': 'CHAR', 'BOOLEAN': 'TINYINT', 'DATE': 'DATE',
                    'TIME': 'TIME', 'BYTEA': 'BLOB', 'UUID': 'VARCHAR(36)',
                    'JSON': 'JSON', 'JSONB': 'JSON'
                },
                'hive': {
                    'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'VARCHAR': 'STRING',
                    'TEXT': 'STRING', 'TIMESTAMP': 'TIMESTAMP', 'NUMERIC': 'DECIMAL',
                    'REAL': 'FLOAT', 'DOUBLE PRECISION': 'DOUBLE', 'SMALLINT': 'SMALLINT',
                    'CHAR': 'CHAR', 'BOOLEAN': 'BOOLEAN', 'DATE': 'DATE',
                    'TIME': 'STRING', 'BYTEA': 'BINARY', 'UUID': 'STRING',
                    'JSON': 'STRING', 'JSONB': 'STRING'
                },
                'doris': {
                    'INTEGER': 'INT', 'BIGINT': 'BIGINT', 'VARCHAR': 'VARCHAR',
                    'TEXT': 'STRING', 'TIMESTAMP': 'DATETIME', 'NUMERIC': 'DECIMAL',
                    'REAL': 'FLOAT', 'DOUBLE PRECISION': 'DOUBLE', 'SMALLINT': 'SMALLINT',
                    'CHAR': 'CHAR', 'BOOLEAN': 'BOOLEAN', 'DATE': 'DATE',
                    'TIME': 'STRING', 'BYTEA': 'STRING', 'UUID': 'VARCHAR(36)',
                    'JSON': 'JSON', 'JSONB': 'JSON'
                },
                'kingbase': {
                    'INTEGER': 'INTEGER', 'BIGINT': 'BIGINT', 'VARCHAR': 'VARCHAR',
                    'TEXT': 'TEXT', 'TIMESTAMP': 'TIMESTAMP', 'NUMERIC': 'NUMERIC',
                    'REAL': 'REAL', 'DOUBLE PRECISION': 'DOUBLE PRECISION', 'SMALLINT': 'SMALLINT',
                    'CHAR': 'CHAR', 'BOOLEAN': 'BOOLEAN', 'DATE': 'DATE',
                    'TIME': 'TIME', 'BYTEA': 'BYTEA', 'UUID': 'UUID',
                    'JSON': 'JSON', 'JSONB': 'JSONB'
                }
            },

            # Hive ‰Ωú‰∏∫Ê∫êÊï∞ÊçÆÂ∫ìÁöÑÊò†Â∞Ñ
            'hive': {
                'mysql': {
                    'INT': 'INT', 'BIGINT': 'BIGINT', 'STRING': 'TEXT',
                    'DOUBLE': 'DOUBLE', 'FLOAT': 'FLOAT', 'DECIMAL': 'DECIMAL',
                    'BOOLEAN': 'TINYINT', 'TINYINT': 'TINYINT', 'SMALLINT': 'SMALLINT',
                    'TIMESTAMP': 'TIMESTAMP', 'DATE': 'DATE', 'CHAR': 'CHAR',
                    'VARCHAR': 'VARCHAR', 'BINARY': 'BLOB', 'ARRAY': 'JSON',
                    'MAP': 'JSON', 'STRUCT': 'JSON'
                },
                'kingbase': {
                    'INT': 'INTEGER', 'BIGINT': 'BIGINT', 'STRING': 'TEXT',
                    'DOUBLE': 'DOUBLE PRECISION', 'FLOAT': 'REAL', 'DECIMAL': 'NUMERIC',
                    'BOOLEAN': 'BOOLEAN', 'TINYINT': 'SMALLINT', 'SMALLINT': 'SMALLINT',
                    'TIMESTAMP': 'TIMESTAMP', 'DATE': 'DATE', 'CHAR': 'CHAR',
                    'VARCHAR': 'VARCHAR', 'BINARY': 'BYTEA', 'ARRAY': 'JSONB',
                    'MAP': 'JSONB', 'STRUCT': 'JSONB'
                },
                'doris': {
                    'INT': 'INT', 'BIGINT': 'BIGINT', 'STRING': 'STRING',
                    'DOUBLE': 'DOUBLE', 'FLOAT': 'FLOAT', 'DECIMAL': 'DECIMAL',
                    'BOOLEAN': 'BOOLEAN', 'TINYINT': 'TINYINT', 'SMALLINT': 'SMALLINT',
                    'TIMESTAMP': 'DATETIME', 'DATE': 'DATE', 'CHAR': 'CHAR',
                    'VARCHAR': 'VARCHAR', 'BINARY': 'STRING', 'ARRAY': 'JSON',
                    'MAP': 'JSON', 'STRUCT': 'JSON'
                }
            },

            # Doris ‰Ωú‰∏∫Ê∫êÊï∞ÊçÆÂ∫ìÁöÑÊò†Â∞Ñ
            'doris': {
                'mysql': {
                    'INT': 'INT', 'BIGINT': 'BIGINT', 'STRING': 'TEXT',
                    'DOUBLE': 'DOUBLE', 'FLOAT': 'FLOAT', 'DECIMAL': 'DECIMAL',
                    'BOOLEAN': 'TINYINT', 'TINYINT': 'TINYINT', 'SMALLINT': 'SMALLINT',
                    'DATETIME': 'DATETIME', 'DATE': 'DATE', 'CHAR': 'CHAR',
                    'VARCHAR': 'VARCHAR', 'JSON': 'JSON'
                },
                'hive': {
                    'INT': 'INT', 'BIGINT': 'BIGINT', 'STRING': 'STRING',
                    'DOUBLE': 'DOUBLE', 'FLOAT': 'FLOAT', 'DECIMAL': 'DECIMAL',
                    'BOOLEAN': 'BOOLEAN', 'TINYINT': 'TINYINT', 'SMALLINT': 'SMALLINT',
                    'DATETIME': 'TIMESTAMP', 'DATE': 'DATE', 'CHAR': 'CHAR',
                    'VARCHAR': 'STRING', 'JSON': 'STRING'
                },
                'kingbase': {
                    'INT': 'INTEGER', 'BIGINT': 'BIGINT', 'STRING': 'TEXT',
                    'DOUBLE': 'DOUBLE PRECISION', 'FLOAT': 'REAL', 'DECIMAL': 'NUMERIC',
                    'BOOLEAN': 'BOOLEAN', 'TINYINT': 'SMALLINT', 'SMALLINT': 'SMALLINT',
                    'DATETIME': 'TIMESTAMP', 'DATE': 'DATE', 'CHAR': 'CHAR',
                    'VARCHAR': 'VARCHAR', 'JSON': 'JSONB'
                }
            }
        }

        # Ëé∑ÂèñÊ∫êÊï∞ÊçÆÂ∫ìÁ±ªÂûãÁöÑÊò†Â∞Ñ
        source_mappings = type_mappings.get(source_type.lower(), {})
        target_mappings = source_mappings.get(target_type.lower(), {})

        # ËøîÂõûÊò†Â∞ÑÂêéÁöÑÁ±ªÂûãÔºåÂ¶ÇÊûúÊâæ‰∏çÂà∞ÂàôËøîÂõûÈªòËÆ§Á±ªÂûã
        mapped_type = target_mappings.get(source_type.upper())

        if mapped_type:
            return mapped_type
        else:
            # ÈªòËÆ§Êò†Â∞ÑÁ≠ñÁï•
            default_mappings = {
                'mysql': 'VARCHAR(255)',
                'postgresql': 'VARCHAR(255)',
                'kingbase': 'VARCHAR(255)',
                'hive': 'STRING',
                'doris': 'VARCHAR(255)',
                'oracle': 'VARCHAR2(255)'
            }
            return default_mappings.get(target_type.lower(), 'VARCHAR(255)')

    async def _generate_datax_config(self, sync_plan: Dict[str, Any], table_plan: Dict[str, Any]) -> Dict[str, Any]:
        """ÁîüÊàêDataXÈÖçÁΩÆ"""
        try:
            source_config = await self._get_data_source_config(sync_plan['source_name'])
            target_config = await self._get_data_source_config(sync_plan['target_name'])

            if not source_config:
                raise ValueError(f"Êó†Ê≥ïËé∑ÂèñÊ∫êÊï∞ÊçÆÊ∫êÈÖçÁΩÆ: {sync_plan['source_name']}")
            if not target_config:
                raise ValueError(f"Êó†Ê≥ïËé∑ÂèñÁõÆÊ†áÊï∞ÊçÆÊ∫êÈÖçÁΩÆ: {sync_plan['target_name']}")

            # ‰øùÂ≠òÂΩìÂâçÂ§ÑÁêÜÁöÑË°®‰ø°ÊÅØÔºå‰æõÂÖ∂‰ªñÊñπÊ≥ï‰ΩøÁî®
            self._current_source_name = sync_plan['source_name']
            self._current_table_name = table_plan['source_table']
            self._current_target_type = target_config['type']

            # üîß ÂÖ≥ÈîÆ‰øÆÂ§çÔºöÊâãÂä®ÊûÑÂª∫ÈÖçÁΩÆÔºåÈÅøÂÖçÂæ™ÁéØÂºïÁî®
            final_source_config = {
                'type': source_config.get('type'),
                'host': source_config.get('host'),
                'port': source_config.get('port'),
                'database': source_config.get('database'),
                'username': source_config.get('username'),
                'password': source_config.get('password'),
                'table': table_plan['source_table']
            }

            final_target_config = {
                'type': target_config.get('type'),
                'host': target_config.get('host'),
                'port': target_config.get('port'),
                'database': target_config.get('database'),
                'username': target_config.get('username'),
                'password': target_config.get('password')
            }

            # HiveÁâπÊÆäÂ§ÑÁêÜ
            target_table_name = table_plan['target_table']
            if target_config['type'].lower() == 'hive':
                # Â§ÑÁêÜË°®ÂêçÔºöÁ°Æ‰øùODS_ÂâçÁºÄ
                original_table_name = table_plan['target_table']
                if original_table_name.startswith('ODS_'):
                    final_table_name = original_table_name
                else:
                    final_table_name = f"ODS_{original_table_name}"

                # ÁîüÊàêHDFSË∑ØÂæÑ
                database = target_config.get('database', 'default')
                base_path = target_config.get('base_path', '/user/hive/warehouse')

                # ÁîüÊàêÂΩìÂâçÊó•ÊúüÂàÜÂå∫
                from datetime import datetime
                current_date = datetime.now().strftime('%Y-%m-%d')
                partition_value = current_date

                # ÊûÑÂª∫ÂÆåÊï¥ÁöÑHDFSË∑ØÂæÑ
                if database != 'default':
                    base_table_path = f"{base_path}/{database}.db/{final_table_name}"
                else:
                    base_table_path = f"{base_path}/{final_table_name}"

                hdfs_path = f"{base_table_path}/dt={partition_value}"

                # ÊâãÂä®Ê∑ªÂä†HiveÂ≠óÊÆµ
                final_target_config['hdfs_path'] = hdfs_path
                final_target_config['table'] = final_table_name
                final_target_config['partition_column'] = 'dt'
                final_target_config['partition_value'] = partition_value
                final_target_config['storage_format'] = 'ORC'
                final_target_config['compression'] = 'snappy'
                final_target_config['namenode_host'] = target_config.get('namenode_host', '192.142.76.242')
                final_target_config['namenode_port'] = target_config.get('namenode_port', '8020')

                target_table_name = final_table_name
                logger.info(f"HiveÁõÆÊ†áÊ£ÄÊµãÂà∞ÔºåÁîüÊàêË∑ØÂæÑ: {hdfs_path}")
                logger.info(f"ÊúÄÁªàË°®Âêç: {final_table_name}")
                logger.info(f"ÂàÜÂå∫‰ø°ÊÅØ: dt={partition_value}")
            else:
                final_target_config['table'] = target_table_name

            # Â§ÑÁêÜHiveÊ∫êÈÖçÁΩÆ
            if source_config.get('type', '').lower() == 'hive':
                final_source_config['namenode_host'] = source_config.get('namenode_host')
                final_source_config['namenode_port'] = source_config.get('namenode_port')
                final_source_config['base_path'] = source_config.get('base_path')
                final_source_config['file_type'] = source_config.get('file_type', 'orc')
                final_source_config['field_delimiter'] = source_config.get('field_delimiter', '\t')

            # üîß ÁÆÄÂåñÂ≠óÊÆµÂ§ÑÁêÜÔºöÁõ¥Êé•‰ªétable_planËé∑ÂèñÔºå‰∏ç‰ΩøÁî®schema_mapping
            schema_mapping = table_plan.get('schema_mapping', {})
            columns_mapping = schema_mapping.get('columns', [])

            if not columns_mapping:
                # üîß Â¶ÇÊûúÊ≤°Êúâschema_mappingÔºå‰ΩøÁî®ÁÆÄÂçïÁöÑÂ≠óÊÆµÂàóË°®
                logger.warning(f"Ë°® {table_plan['source_table']} Ê≤°Êúâschema_mappingÔºå‰ΩøÁî®ÁÆÄÂåñÂ≠óÊÆµÈÖçÁΩÆ")
                # ÁîüÊàêÁÆÄÂçïÁöÑÂ≠óÊÆµÈÖçÁΩÆ
                simple_columns = [f"col_{i}" for i in range(10)]  # ÂÅáËÆæ10‰∏™Â≠óÊÆµ
                source_columns = simple_columns
                target_columns = simple_columns
            else:
                # ËøáÊª§ÊéâÂàÜÂå∫Â≠óÊÆµÔºåÂè™‰øùÁïôÂ≠óÊÆµÂêç
                data_columns = [col for col in columns_mapping if col['name'].lower() != 'dt']
                source_columns = [col['name'] for col in data_columns]
                target_columns = [col['name'] for col in data_columns]

            logger.info(f"Â≠óÊÆµÈÖçÁΩÆ: Ê∫êÂ≠óÊÆµ({len(source_columns)})={source_columns[:5]}...")
            logger.info(f"Â≠óÊÆµÈÖçÁΩÆ: ÁõÆÊ†áÂ≠óÊÆµ({len(target_columns)})={target_columns[:5]}...")

            if not source_columns or not target_columns:
                raise ValueError("Â≠óÊÆµÂàóË°®‰∏∫Á©∫")

            # Á°ÆÂÆöÂÜôÂÖ•Ê®°Âºè
            write_mode = self._determine_write_mode(table_plan, sync_plan.get('sync_mode', 'full'))

            # ÊâãÂä®Ê∑ªÂä†Â≠óÊÆµÈÖçÁΩÆ
            final_source_config['columns'] = source_columns
            final_target_config['columns'] = target_columns
            final_target_config['write_mode'] = write_mode

            # üîß ÊûÑÂª∫ÊúÄÁªàÁöÑDataXÈÖçÁΩÆÔºåÂÆåÂÖ®ÂéªÊéâschema_mappingÂºïÁî®
            datax_config = {
                "id": f"sync_{sync_plan['source_name']}_{table_plan['source_table']}",
                "name": f"{table_plan['source_table']} -> {target_table_name}",
                "source": final_source_config,
                "target": final_target_config,
                "sync_type": "full",
                "parallel_jobs": sync_plan.get('recommended_parallel_jobs', 4)
                # üîß ÂÆåÂÖ®Âà†Èô§ schema_mapping ÂºïÁî®
            }

            logger.info(f"DataXÈÖçÁΩÆÁîüÊàêÂÆåÊàêÔºåÊ∫êÂ≠óÊÆµÊï∞: {len(source_columns)}, ÁõÆÊ†áÂ≠óÊÆµÊï∞: {len(target_columns)}")
            try:
                import json
                test_json = json.dumps(datax_config, ensure_ascii=False)
                logger.info("JSONÂ∫èÂàóÂåñÊµãËØïÈÄöËøá")
            except Exception as e:
                logger.error(f"JSONÂ∫èÂàóÂåñÂ§±Ë¥•: {e}")
                raise
            return datax_config

        except Exception as e:
            logger.error(f"DataXÈÖçÁΩÆÁîüÊàêÂ§±Ë¥•: {str(e)}")
            raise e

    async def _analyze_field_lengths(self, source_name: str, table_name: str, columns: List[Dict]) -> Dict[str, int]:
        """ÂàÜÊûêÂ≠óÊÆµÂÆûÈôÖ‰ΩøÁî®ÁöÑÊúÄÂ§ßÈïøÂ∫¶"""
        field_lengths = {}

        try:
            logger.info(f"ÂºÄÂßãÂàÜÊûêË°® {table_name} ÁöÑÂ≠óÊÆµÈïøÂ∫¶...")

            # Á≠õÈÄâÂá∫ÂèØËÉΩÈúÄË¶ÅÈïøÂ∫¶ÂàÜÊûêÁöÑÂ≠óÊÆµ
            text_columns = []
            for col in columns:
                col_name = col.get('name', '')
                source_type = col.get('source_type', '').upper()
                if source_type in ['VARCHAR', 'CHAR', 'TEXT', 'CHARACTER VARYING']:
                    text_columns.append(col_name)

            if not text_columns:
                logger.info("Ê≤°ÊúâÈúÄË¶ÅÂàÜÊûêÈïøÂ∫¶ÁöÑÊñáÊú¨Â≠óÊÆµ")
                return field_lengths

            logger.info(f"ÈúÄË¶ÅÂàÜÊûêÈïøÂ∫¶ÁöÑÂ≠óÊÆµ: {text_columns}")

            # ÂàÜÊâπÂàÜÊûêÂ≠óÊÆµÔºàÈÅøÂÖçSQLÂ§™ÈïøÔºâ
            batch_size = 10
            for i in range(0, len(text_columns), batch_size):
                batch_columns = text_columns[i:i + batch_size]

                # ÊûÑÂª∫ÈïøÂ∫¶Êü•ËØ¢SQL
                length_queries = []
                for col in batch_columns:
                    # Ê†πÊçÆ‰∏çÂêåÊï∞ÊçÆÂ∫ì‰ΩøÁî®‰∏çÂêåÁöÑÈïøÂ∫¶ÂáΩÊï∞
                    if hasattr(self, '_current_source_type'):
                        source_type = getattr(self, '_current_source_type', 'mysql').lower()
                    else:
                        source_type = 'mysql'  # ÈªòËÆ§

                    if source_type in ['kingbase', 'postgresql']:
                        length_queries.append(f'MAX(LENGTH("{col}")) as "{col}_len"')
                    else:
                        length_queries.append(f'MAX(LENGTH(`{col}`)) as {col}_len')

                if length_queries:
                    sql = f"SELECT {', '.join(length_queries)} FROM {table_name} LIMIT 1"
                    logger.info(f"ÊâßË°åÈïøÂ∫¶ÂàÜÊûêSQL: {sql}")

                    result = await self.integration_service.execute_query(
                        source_name=source_name,
                        query=sql,
                        limit=1
                    )

                    if result.get('success') and result.get('results'):
                        row = result['results'][0]
                        for col in batch_columns:
                            length_key = f"{col}_len"
                            actual_length = row.get(length_key, 0) or 0

                            # Êô∫ËÉΩÊé®ËçêÈïøÂ∫¶
                            if actual_length == 0:
                                recommended_length = 100  # Â¶ÇÊûúÊ≤°ÊúâÊï∞ÊçÆÔºå‰ΩøÁî®ÈªòËÆ§ÂÄº
                            elif actual_length <= 50:
                                recommended_length = 100  # Áü≠Â≠óÊÆµÔºåÁªô‰∏Ä‰∫õ‰ΩôÈáè
                            elif actual_length <= 200:
                                recommended_length = min(actual_length + 100, 300)  # ‰∏≠Á≠âÂ≠óÊÆµ
                            elif actual_length <= 1000:
                                recommended_length = min(actual_length + 200, 1200)  # ËæÉÈïøÂ≠óÊÆµ
                            else:
                                recommended_length = 'TEXT'  # ÂæàÈïøÁöÑÂ≠óÊÆµÁî®TEXT

                            field_lengths[col] = recommended_length
                            logger.info(f"Â≠óÊÆµ {col} - ÂÆûÈôÖÊúÄÂ§ßÈïøÂ∫¶: {actual_length}, Êé®Ëçê: {recommended_length}")
                    else:
                        logger.warning(f"Â≠óÊÆµÈïøÂ∫¶ÂàÜÊûêÂ§±Ë¥•: {result.get('error', 'Êú™Áü•ÈîôËØØ')}")
                        # Â¶ÇÊûúÂàÜÊûêÂ§±Ë¥•ÔºåÁªôÊâÄÊúâÂ≠óÊÆµËÆæÁΩÆÈªòËÆ§ÂÄº
                        for col in batch_columns:
                            field_lengths[col] = 150

        except Exception as e:
            logger.error(f"ÂàÜÊûêÂ≠óÊÆµÈïøÂ∫¶ÂºÇÂ∏∏: {e}")
            # ÂºÇÂ∏∏Êó∂ÁªôÊâÄÊúâÊñáÊú¨Â≠óÊÆµËÆæÁΩÆ‰øùÂÆàÁöÑÈªòËÆ§ÂÄº
            for col in columns:
                if col.get('source_type', '').upper() in ['VARCHAR', 'CHAR', 'TEXT']:
                    field_lengths[col.get('name', '')] = 150

        logger.info(f"Â≠óÊÆµÈïøÂ∫¶ÂàÜÊûêÂÆåÊàêÔºåÁªìÊûú: {field_lengths}")
        return field_lengths
    def _determine_write_mode(self, table_plan: Dict[str, Any], sync_mode: str) -> str:
        """Ê†πÊçÆÊÉÖÂÜµÂÜ≥ÂÆöÂÜôÂÖ•Ê®°Âºè"""
        target_exists = table_plan.get('target_exists', False)
        strategy = table_plan.get('strategy', 'full_copy')

        if not target_exists:
            return "insert"
        elif strategy == 'incremental_update':
            return "insert"
        elif sync_mode == 'full' or strategy in ['full_copy', 'batch_insert']:
            return "replace"
        else:
            return "insert"

    async def _verify_sync_integrity(self, sync_plan: Dict[str, Any],
                                     table_plan: Dict[str, Any]) -> Dict[str, Any]:
        """È™åËØÅÂêåÊ≠•ÂÆåÊï¥ÊÄß"""
        try:
            source_name = sync_plan['source_name']
            target_name = sync_plan['target_name']
            source_table = table_plan['source_table']
            target_table = table_plan['target_table']

            # Ëé∑ÂèñÊ∫êË°®Ë°åÊï∞
            source_count_result = await self.integration_service.execute_query(
                source_name, f"SELECT COUNT(*) as cnt FROM {source_table}"
            )

            # Ëé∑ÂèñÁõÆÊ†áË°®Ë°åÊï∞
            target_count_result = await self.integration_service.execute_query(
                target_name, f"SELECT COUNT(*) as cnt FROM {target_table}"
            )

            if (source_count_result.get('success') and target_count_result.get('success')):
                source_count = source_count_result['results'][0]['cnt']
                target_count = target_count_result['results'][0]['cnt']

                return {
                    "success": True,
                    "source_rows": source_count,
                    "target_rows": target_count,
                    "integrity_check": source_count == target_count,
                    "data_loss": max(0, source_count - target_count),
                    "verification_time": datetime.now()
                }
            else:
                return {
                    "success": False,
                    "error": "Êó†Ê≥ïËé∑ÂèñË°åÊï∞ËøõË°åÈ™åËØÅ"
                }

        except Exception as e:
            return {
                "success": False,
                "error": f"È™åËØÅÂ§±Ë¥•: {str(e)}"
            }

    async def _create_hdfs_directories(self, sync_plan: Dict[str, Any]) -> Dict[str, Any]:
        """ÂàõÂª∫HDFSÁõÆÂΩï"""
        try:
            target_config = await self._get_data_source_config(sync_plan['target_name'])

            if target_config.get('type', '').lower() != 'hive':
                return {"success": True, "message": "ÈùûHiveÁõÆÊ†áÔºåË∑≥ËøáHDFSÁõÆÂΩïÂàõÂª∫"}

            # Ëé∑Âèñnamenode‰ø°ÊÅØ
            namenode_host = target_config.get('namenode_host', '192.142.76.242')
            namenode_port = target_config.get('namenode_port', '8020')

            logger.info(f"HDFSËøûÊé•‰ø°ÊÅØ: {namenode_host}:{namenode_port}")

            created_paths = []
            failed_paths = []

            for plan in sync_plan['sync_plans']:
                try:
                    # ÁîüÊàê‰∏éDataXÈÖçÁΩÆ‰∏ÄËá¥ÁöÑË∑ØÂæÑ
                    original_table_name = plan['target_table']
                    if original_table_name.startswith('ODS_'):
                        final_table_name = original_table_name
                    else:
                        final_table_name = f"ODS_{original_table_name}"

                    database = target_config.get('database', 'default')
                    base_path = target_config.get('base_path', '/user/hive/warehouse')

                    # ÁîüÊàêÂΩìÂâçÊó•ÊúüÂàÜÂå∫
                    from datetime import datetime
                    current_date = datetime.now().strftime('%Y-%m-%d')

                    # ÊûÑÂª∫ÂÆåÊï¥Ë∑ØÂæÑÔºà‰∏éDataXÈÖçÁΩÆ‰∏≠ÁöÑË∑ØÂæÑÁîüÊàêÈÄªËæë‰∏ÄËá¥Ôºâ
                    if database != 'default':
                        base_table_path = f"{base_path}/{database}.db/{final_table_name}"
                    else:
                        base_table_path = f"{base_path}/{final_table_name}"

                    partition_path = f"{base_table_path}/dt={current_date}"

                    logger.info(f"ÂáÜÂ§áÂàõÂª∫HDFSË∑ØÂæÑ: {partition_path}")

                    # üîß ‰ΩøÁî®‰Ω†Êèê‰æõÁöÑÊñπÊ≥ïÂàõÂª∫HDFSÁõÆÂΩï
                    success = await self._create_hdfs_path(namenode_host, namenode_port, partition_path)

                    if success:
                        created_paths.append(partition_path)
                        logger.info(f"HDFSÁõÆÂΩïÂàõÂª∫ÊàêÂäü: {partition_path}")
                    else:
                        failed_paths.append(partition_path)
                        logger.error(f"HDFSÁõÆÂΩïÂàõÂª∫Â§±Ë¥•: {partition_path}")

                except Exception as e:
                    error_msg = f"{plan['target_table']}: {str(e)}"
                    logger.error(f"ÂàõÂª∫HDFSÁõÆÂΩïÂºÇÂ∏∏: {error_msg}")
                    failed_paths.append(error_msg)

            return {
                "success": len(failed_paths) == 0,
                "created_paths": created_paths,
                "failed_paths": failed_paths,
                "total_paths": len(created_paths) + len(failed_paths),
                "namenode": f"{namenode_host}:{namenode_port}",
                "message": f"HDFSÁõÆÂΩïÂàõÂª∫ÂÆåÊàêÔºåÊàêÂäü{len(created_paths)}‰∏™ÔºåÂ§±Ë¥•{len(failed_paths)}‰∏™"
            }

        except Exception as e:
            logger.error(f"HDFSÁõÆÂΩïÂàõÂª∫ÂºÇÂ∏∏: {e}")
            return {
                "success": False,
                "error": f"HDFSÁõÆÂΩïÂàõÂª∫ÂºÇÂ∏∏: {str(e)}"
            }

    async def _create_hdfs_path(self, namenode_host: str, namenode_port: str, hdfs_path: str) -> bool:
        """ÂàõÂª∫Âçï‰∏™HDFSË∑ØÂæÑ - ‰ΩøÁî®WebHDFS API"""
        try:
            import aiohttp
            import asyncio

            # WebHDFS API Á´ØÂè£ÈÄöÂ∏∏ÊòØ 9870 (Hadoop 3.x) Êàñ 50070 (Hadoop 2.x)
            webhdfs_port = 9870  # ‰Ω†ÂèØ‰ª•Ê†πÊçÆÂÆûÈôÖÊÉÖÂÜµË∞ÉÊï¥

            # ÊûÑÂª∫WebHDFS URL
            webhdfs_url = f"http://{namenode_host}:{webhdfs_port}/webhdfs/v1{hdfs_path}?op=MKDIRS&user.name=bigdata"

            logger.info(f"‰ΩøÁî®WebHDFSÂàõÂª∫ÁõÆÂΩï: {webhdfs_url}")

            timeout = aiohttp.ClientTimeout(total=30)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.put(webhdfs_url) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result.get('boolean'):
                            logger.info(f"HDFSÁõÆÂΩïÂàõÂª∫ÊàêÂäü: {hdfs_path}")
                            return True
                        else:
                            logger.error(f"HDFSÁõÆÂΩïÂàõÂª∫Â§±Ë¥•: {hdfs_path}, ÂìçÂ∫î: {result}")
                            return False
                    else:
                        error_text = await response.text()
                        logger.error(f"WebHDFSËØ∑Ê±ÇÂ§±Ë¥•: {response.status}, ÈîôËØØ: {error_text}")
                        return False

        except Exception as e:
            logger.error(f"WebHDFS APIË∞ÉÁî®ÂºÇÂ∏∏: {e}")
            return False

    async def _verify_hdfs_path(self, namenode_host: str, namenode_port: str, hdfs_path: str) -> bool:
        """È™åËØÅHDFSË∑ØÂæÑÊòØÂê¶Â≠òÂú® - ‰ΩøÁî®WebHDFS API"""
        try:
            import aiohttp

            webhdfs_port = 9870
            webhdfs_url = f"http://{namenode_host}:{webhdfs_port}/webhdfs/v1{hdfs_path}?op=GETFILESTATUS&user.name=bigdata"

            logger.info(f"È™åËØÅHDFSË∑ØÂæÑ: {webhdfs_url}")

            timeout = aiohttp.ClientTimeout(total=10)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(webhdfs_url) as response:
                    if response.status == 200:
                        logger.info(f"HDFSË∑ØÂæÑÈ™åËØÅÊàêÂäü: {hdfs_path}")
                        return True
                    else:
                        logger.warning(f"HDFSË∑ØÂæÑ‰∏çÂ≠òÂú®ÊàñÈ™åËØÅÂ§±Ë¥•: {hdfs_path}")
                        return False

        except Exception as e:
            logger.error(f"HDFSË∑ØÂæÑÈ™åËØÅÂºÇÂ∏∏: {e}")
            return False

    def _estimate_sync_time(self, rows: int, source_type: str, target_type: str) -> int:
        """‰º∞ÁÆóÂêåÊ≠•Êó∂Èó¥ÔºàÂàÜÈíüÔºâ"""
        # Âü∫‰∫éÁªèÈ™åÁöÑÂêåÊ≠•ÈÄüÂ∫¶‰º∞ÁÆóÔºàË°å/ÂàÜÈíüÔºâ
        base_speed = {
            ('mysql', 'mysql'): 50000,
            ('mysql', 'hive'): 30000,
            ('oracle', 'mysql'): 40000,
            ('postgresql', 'hive'): 35000
        }

        speed = base_speed.get((source_type.lower(), target_type.lower()), 25000)
        estimated_minutes = max(1, rows // speed)

        return estimated_minutes

    def _recommend_parallel_jobs(self, total_rows: int) -> int:
        """Êé®ËçêÂπ∂Ë°å‰Ωú‰∏öÊï∞"""
        if total_rows < 100000:
            return 2
        elif total_rows < 1000000:
            return 4
        elif total_rows < 10000000:
            return 6
        else:
            return 8

    def _check_compatibility_warnings(self, table_meta: Dict[str, Any],
                                      source_config: Dict[str, Any],
                                      target_config: Dict[str, Any]) -> List[str]:
        """Ê£ÄÊü•ÂÖºÂÆπÊÄßË≠¶Âëä"""
        warnings = []

        # Ê£ÄÊü•Â≠óÁ¨¶ÈõÜÂÖºÂÆπÊÄß
        if source_config['type'] == 'mysql' and target_config['type'] == 'kingbase':
            warnings.append("MySQLÂà∞kingbaseÂèØËÉΩÂ≠òÂú®Â≠óÁ¨¶ÈõÜËΩ¨Êç¢ÈóÆÈ¢ò")

        # Ê£ÄÊü•Â§ßË°®Ë≠¶Âëä
        row_count = table_meta.get('statistics', {}).get('row_count', 0)
        if row_count > 10000000:
            warnings.append(f"Â§ßË°®ÂêåÊ≠•Ôºà{row_count:,}Ë°åÔºâÔºåÂª∫ËÆÆÂú®‰ΩéÂ≥∞ÊúüÊâßË°å")

        # Ê£ÄÊü•Êï∞ÊçÆÁ±ªÂûãÂÖºÂÆπÊÄß
        columns = table_meta.get('schema', {}).get('columns', [])
        for col in columns:
            if col.get('data_type', '').upper() in ['JSON', 'JSONB']:
                col_name = col.get('name', col.get('column_name', 'Êú™Áü•Â≠óÊÆµ'))
                warnings.append(f"Âàó {col_name} ‰ΩøÁî®JSONÁ±ªÂûãÔºåËØ∑Á°Æ‰øùÁõÆÊ†áÊï∞ÊçÆÂ∫ìÊîØÊåÅ")

        return warnings

    async def _get_data_source_config(self, source_name: str) -> Optional[Dict[str, Any]]:
        """Ëé∑ÂèñÊï∞ÊçÆÊ∫êÈÖçÁΩÆ"""
        try:
            # ‰ªéÂÆûÈôÖÁöÑÊï∞ÊçÆÈõÜÊàêÊúçÂä°Ëé∑ÂèñÊï∞ÊçÆÊ∫êÈÖçÁΩÆ
            sources_list = await self.integration_service.get_data_sources_list_basic()

            # Êü•ÊâæÊåáÂÆöÂêçÁß∞ÁöÑÊï∞ÊçÆÊ∫ê
            target_source = None
            for source in sources_list:
                if source.get('name') == source_name:
                    target_source = source
                    break

            if not target_source:
                logger.error(f"Êú™ÊâæÂà∞Êï∞ÊçÆÊ∫ê: {source_name}")
                return None

            # üîß ‰øÆÂ§çÔºöÁ°Æ‰øùÁ±ªÂûãÊò†Â∞ÑÊ≠£Á°Æ
            source_type = target_source.get('type', '').lower()
            if not source_type:
                logger.error(f"Êï∞ÊçÆÊ∫ê {source_name} Á±ªÂûã‰∏∫Á©∫")
                return None

            # Êò†Â∞ÑÊï∞ÊçÆÊ∫êÁ±ªÂûã
            type_mapping = {
                'mysql': 'mysql',
                'kingbase': 'kingbase',
                'hive': 'hive',
                'postgresql': 'postgresql',
                'oracle': 'oracle',
                'doris': 'doris'
            }

            mapped_type = type_mapping.get(source_type, source_type)

            # üîß ÈáçË¶Å‰øÆÂ§çÔºöÁ°Æ‰øùÂØÜÁ†Å‰∏ç‰∏∫Á©∫
            password = target_source.get('password', '')
            if not password:
                # Â¶ÇÊûúÂØÜÁ†Å‰∏∫Á©∫ÔºåÂ∞ùËØï‰ªéÂÖ∂‰ªñÂú∞ÊñπËé∑ÂèñÊàñ‰ΩøÁî®ÈªòËÆ§ÂÄº
                logger.warning(f"Êï∞ÊçÆÊ∫ê {source_name} ÂØÜÁ†Å‰∏∫Á©∫ÔºåËØ∑Ê£ÄÊü•ÈÖçÁΩÆ")

            username = target_source.get('username', '')
            if not username:
                logger.warning(f"Êï∞ÊçÆÊ∫ê {source_name} Áî®Êà∑Âêç‰∏∫Á©∫ÔºåËØ∑Ê£ÄÊü•ÈÖçÁΩÆ")

            config = {
                "type": mapped_type,
                "host": target_source.get('host', ''),
                "port": target_source.get('port', 3306),
                "database": target_source.get('database', ''),
                "username": username,
                "password": password,
            }

            # üÜï HiveÁâπÊÆäÈÖçÁΩÆ
            if mapped_type == 'hive':
                config.update({
                    'namenode_host': target_source.get('namenode_host', '192.142.76.242'),
                    'namenode_port': target_source.get('namenode_port', '8020'),
                    'base_path': target_source.get('base_path', '/user/hive/warehouse'),
                    'storage_format': 'ORC',
                    'compression': 'snappy',
                    'field_delimiter': target_source.get('field_delimiter', '\t'),
                    'add_ods_prefix': target_source.get('add_ods_prefix', True),
                    'partition_column': 'dt'
                })

            # üÜï DorisÁâπÊÆäÈÖçÁΩÆ
            if mapped_type == 'doris':
                # DorisÈúÄË¶ÅÈ¢ùÂ§ñÁöÑHTTPÁ´ØÂè£ÈÖçÁΩÆ
                config['http_port'] = 8060  # FE HTTPÁ´ØÂè£
                # DorisÁöÑÊü•ËØ¢Á´ØÂè£ÈÄöÂ∏∏ÊòØ9030
                if config['port'] == 3306:  # Â¶ÇÊûúÊòØÈªòËÆ§MySQLÁ´ØÂè£ÔºåÊîπ‰∏∫DorisÁ´ØÂè£
                    config['port'] = 9030
            # üîß È™åËØÅÂøÖË¶ÅÂ≠óÊÆµ
            required_fields = ['host', 'username', 'password']
            missing_fields = [field for field in required_fields if not config.get(field)]

            if missing_fields:
                logger.error(f"Êï∞ÊçÆÊ∫ê {source_name} Áº∫Â∞ëÂøÖË¶ÅÂ≠óÊÆµ: {missing_fields}")
                logger.error(f"ÂΩìÂâçÈÖçÁΩÆ: {config}")
                return None

            logger.info(f"Ëé∑ÂèñÊï∞ÊçÆÊ∫êÈÖçÁΩÆÊàêÂäü: {source_name} -> {mapped_type}")
            logger.info(
                f"ÈÖçÁΩÆËØ¶ÊÉÖ: host={config['host']}, username={config['username']}, password={'***' if config['password'] else 'EMPTY'}")

            return config

        except Exception as e:
            logger.error(f"Ëé∑ÂèñÊï∞ÊçÆÊ∫êÈÖçÁΩÆÂ§±Ë¥• {source_name}: {e}")
            return None

    async def _check_target_table_exists(self, target_name: str, table_name: str) -> bool:
        """Ê£ÄÊü•ÁõÆÊ†áË°®ÊòØÂê¶Â≠òÂú®"""
        try:
            result = await self.integration_service.get_tables(target_name)
            if result.get('success'):
                tables = result.get('tables', [])
                return any(t.get('table_name') == table_name for t in tables)
            return False
        except:
            return False

    async def _execute_create_table(self, target_name: str, create_sql: str) -> Dict[str, Any]:
        """ÊâßË°åÂª∫Ë°®SQL"""
        try:
            logger.info(f"ÂºÄÂßãÊâßË°åÂª∫Ë°®SQLÔºåÁõÆÊ†áÊï∞ÊçÆÊ∫ê: {target_name}")
            logger.info(f"Âª∫Ë°®SQL: {create_sql}")

            # ÊâßË°åÂª∫Ë°®SQL
            result = await self.integration_service.execute_query(
                source_name=target_name,
                query=create_sql,
                database=None,
                schema=None,
                limit=None
            )

            logger.info(f"Âª∫Ë°®SQLÊâßË°åÁªìÊûú: {result}")

            if result.get('success'):
                logger.info("Âª∫Ë°®SQLÊâßË°åÊàêÂäüÔºåÁ≠âÂæÖÈ™åËØÅË°®ÊòØÂê¶Â≠òÂú®...")
                import asyncio
                await asyncio.sleep(2)

                # ÊèêÂèñË°®Âêç
                if 'CREATE TABLE IF NOT EXISTS `' in create_sql:
                    table_name = create_sql.split('`')[1]
                elif 'CREATE TABLE IF NOT EXISTS "' in create_sql:
                    table_name = create_sql.split('"')[1]
                else:
                    # ÂÖ∂‰ªñÊÉÖÂÜµÁöÑË°®ÂêçÊèêÂèñ
                    parts = create_sql.split()
                    table_name = parts[5] if len(parts) > 5 else "unknown"

                logger.info(f"ÊèêÂèñÁöÑË°®Âêç: {table_name}")

                # È™åËØÅË°®ÊòØÂê¶ÁúüÁöÑÂ≠òÂú®
                verify_sql = f"SELECT 1 FROM `{table_name}` WHERE 1=2"
                verify_result = await self.integration_service.execute_query(
                    source_name=target_name,
                    query=verify_sql,
                    database=None,
                    schema=None,
                    limit=1
                )

                logger.info(f"Ë°®Â≠òÂú®È™åËØÅÁªìÊûú: {verify_result}")

                if verify_result.get('success'):
                    logger.info(f"Ë°® {table_name} ÂàõÂª∫Âπ∂È™åËØÅÊàêÂäü")
                    return {
                        "success": True,
                        "message": f"Ë°® {table_name} ÂàõÂª∫ÊàêÂäüÂπ∂È™åËØÅÈÄöËøá",
                        "sql": create_sql,
                        "table_name": table_name
                    }
                else:
                    logger.error(f"Ë°® {table_name} ÂàõÂª∫ÂêéÈ™åËØÅÂ§±Ë¥•: {verify_result.get('error')}")
                    return {
                        "success": False,
                        "message": f"Ë°®ÂàõÂª∫ÂêéÈ™åËØÅÂ§±Ë¥•: {verify_result.get('error')}",
                        "sql": create_sql,
                        "table_name": table_name
                    }
            else:
                error_msg = result.get('error', 'ÂàõÂª∫Â§±Ë¥•')
                logger.error(f"Âª∫Ë°®SQLÊâßË°åÂ§±Ë¥•: {error_msg}")
                return {
                    "success": False,
                    "message": f"Âª∫Ë°®Â§±Ë¥•: {error_msg}",
                    "sql": create_sql
                }

        except Exception as e:
            logger.error(f"Âª∫Ë°®ËøáÁ®ãÂºÇÂ∏∏: {e}")
            import traceback
            logger.error(f"ÂºÇÂ∏∏ËØ¶ÊÉÖ: {traceback.format_exc()}")
            return {
                "success": False,
                "message": f"Âª∫Ë°®ÂºÇÂ∏∏: {str(e)}",
                "sql": create_sql
            }

    async def _generate_sync_strategy(self, source_config: Dict[str, Any],
                                      target_config: Dict[str, Any],
                                      table_meta: Dict[str, Any],
                                      target_exists: bool) -> str:
        """ÁîüÊàêÂêåÊ≠•Á≠ñÁï•"""
        source_type = source_config.get('type', '').lower()
        target_type = target_config.get('type', '').lower()
        row_count = table_meta.get('statistics', {}).get('row_count', 0)

        if target_exists:
            return "incremental_update"  # ÁõÆÊ†áË°®Â≠òÂú®ÔºåÂ¢ûÈáèÊõ¥Êñ∞
        elif row_count > 10000000:
            return "batch_insert"  # Â§ßË°®ÔºåÊâπÈáèÊèíÂÖ•
        elif source_type == target_type:
            return "direct_copy"  # ÂêåÁ±ªÂûãÊï∞ÊçÆÂ∫ìÔºåÁõ¥Êé•Â§çÂà∂
        else:
            return "full_copy"  # ‰∏çÂêåÁ±ªÂûãÔºåÂÖ®ÈáèÂ§çÂà∂

    def _determine_global_strategy(self, sync_mode: str, sync_plans: List[Dict]) -> str:
        """Á°ÆÂÆöÂÖ®Â±ÄÂêåÊ≠•Á≠ñÁï•"""
        if sync_mode == "single":
            return "ÂçïË°®ÂêåÊ≠•"
        elif sync_mode == "multiple":
            return f"Â§öË°®ÂêåÊ≠•({len(sync_plans)}Âº†Ë°®)"
        elif sync_mode == "database":
            return "Êï¥Â∫ìÂêåÊ≠•"
        else:
            return "Ëá™ÂÆö‰πâÂêåÊ≠•"

    async def _precheck_sync_conditions(self, sync_plan: Dict[str, Any]) -> Dict[str, Any]:
        """È¢ÑÊ£ÄÊü•ÂêåÊ≠•Êù°‰ª∂"""
        try:
            # Ê£ÄÊü•Ê∫êÊï∞ÊçÆÊ∫êËøûÊé•
            source_name = sync_plan.get('source_name')
            target_name = sync_plan.get('target_name')

            if not source_name or not target_name:
                return {
                    "success": False,
                    "error": "Áº∫Â∞ëÊ∫êÊàñÁõÆÊ†áÊï∞ÊçÆÊ∫êÂêçÁß∞"
                }

            # ÁÆÄÂçïÁöÑËøûÊé•Ê£ÄÊü•
            source_config = await self._get_data_source_config(source_name)
            target_config = await self._get_data_source_config(target_name)

            if not source_config or not target_config:
                return {
                    "success": False,
                    "error": "Êï∞ÊçÆÊ∫êÈÖçÁΩÆÈ™åËØÅÂ§±Ë¥•"
                }

            return {
                "success": True,
                "message": "È¢ÑÊ£ÄÊü•ÈÄöËøá"
            }

        except Exception as e:
            return {
                "success": False,
                "error": f"È¢ÑÊ£ÄÊü•Â§±Ë¥•: {str(e)}"
            }

    def _generate_sync_summary(self, sync_results: List[Dict]) -> Dict[str, Any]:
        """ÁîüÊàêÂêåÊ≠•ÊëòË¶Å"""
        total_tables = len(sync_results)
        successful_tables = len([r for r in sync_results if r.get('result', {}).get('success')])
        failed_tables = total_tables - successful_tables

        return {
            "total_tables": total_tables,
            "successful_tables": successful_tables,
            "failed_tables": failed_tables,
            "success_rate": f"{(successful_tables / total_tables * 100):.1f}%" if total_tables > 0 else "0%",
            "summary_message": f"ÂÖ±{total_tables}Âº†Ë°®ÔºåÊàêÂäü{successful_tables}Âº†ÔºåÂ§±Ë¥•{failed_tables}Âº†"
        }

    def _deep_clean_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Ê∑±Â∫¶Ê∏ÖÁêÜÈÖçÁΩÆÔºåÁßªÈô§Âæ™ÁéØÂºïÁî®Âíå‰∏çÂèØÂ∫èÂàóÂåñÂØπË±°"""
        import copy
        import json

        if not isinstance(config, dict):
            return config

        clean_config = {}
        exclude_keys = {'__class__', '__dict__', 'class', 'classLoader', 'module'}

        for key, value in config.items():
            if key in exclude_keys or callable(value):
                continue

            try:
                if isinstance(value, dict):
                    clean_config[key] = self._deep_clean_config(value)
                elif isinstance(value, list):
                    clean_config[key] = [
                        self._deep_clean_config(item) if isinstance(item, dict) else item
                        for item in value
                    ]
                else:
                    # ÊµãËØïÊòØÂê¶ÂèØÂ∫èÂàóÂåñ
                    json.dumps(value)
                    clean_config[key] = value
            except (TypeError, ValueError, UnicodeDecodeError):
                # Â¶ÇÊûú‰∏çËÉΩÂ∫èÂàóÂåñÔºåËΩ¨‰∏∫Â≠óÁ¨¶‰∏≤
                clean_config[key] = str(value) if value is not None else None

        return clean_config

# ÂÖ®Â±ÄÊô∫ËÉΩÂêåÊ≠•ÊúçÂä°ÂÆû‰æã
smart_sync_service = SmartSyncService()
