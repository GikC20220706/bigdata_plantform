# app/services/optimized_data_integration_service.py
"""
ä¼˜åŒ–åçš„æ•°æ®é›†æˆæœåŠ¡ - æ”¯æŒé«˜å¹¶å‘å’Œå¤šå±‚ç¼“å­˜
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from loguru import logger
import json
from pathlib import Path
import os
from app.utils.data_integration_clients import (
    DatabaseClientFactory,
    connection_manager,
)
from app.utils.integration_cache import (
    integration_cache,
    cache_connection_status,
    cache_table_schema,
    cache_data_preview
)
from app.models.data_source import DataSource, DataSourceConnection
from app.utils.database import get_async_db
from config.settings import settings
from sqlalchemy import text


class OptimizedDataIntegrationService:
    """ä¼˜åŒ–åçš„æ•°æ®é›†æˆæœåŠ¡"""

    def __init__(self):
        self.connection_manager = connection_manager
        self.cache_manager = integration_cache
        self._initialize_default_connections()

    def _initialize_default_connections(self):
        """åˆå§‹åŒ–é»˜è®¤è¿æ¥é…ç½®"""
        if not settings.use_real_clusters:
            logger.info("Mockæ¨¡å¼ï¼šè·³è¿‡çœŸå®æ•°æ®åº“è¿æ¥åˆå§‹åŒ–")
            return

        # ä»æ•°æ®åº“åŠ è½½å·²ä¿å­˜çš„è¿æ¥é…ç½®
        self._load_saved_connections()

    def _load_saved_connections(self):
        """ä»æ•°æ®åº“åŠ è½½å·²ä¿å­˜çš„è¿æ¥é…ç½® - ä½¿ç”¨åŒæ­¥æ•°æ®åº“ä¼šè¯"""
        try:
            from app.utils.database import get_sync_db_session
            from app.models.data_source import DataSource
            import json

            logger.info("ğŸ”„ å¼€å§‹ä»æ•°æ®åº“åŠ è½½æ•°æ®æºè¿æ¥é…ç½®...")

            # ä½¿ç”¨åŒæ­¥æ•°æ®åº“ä¼šè¯
            db = get_sync_db_session()
            try:
                # æŸ¥è¯¢æ‰€æœ‰æ¿€æ´»çš„æ•°æ®æº
                saved_sources = db.query(DataSource).filter(DataSource.is_active == True).all()
                logger.info(f"ğŸ“Š ä»æ•°æ®åº“æŸ¥è¯¢åˆ° {len(saved_sources)} ä¸ªæ•°æ®æº")

                loaded_count = 0
                for source in saved_sources:
                    try:
                        logger.info(f"   å¤„ç†æ•°æ®æº: {source.name} (ç±»å‹: {source.source_type})")

                        if source.connection_config:
                            # è§£æè¿æ¥é…ç½®
                            config = json.loads(source.connection_config) if isinstance(
                                source.connection_config, str) else source.connection_config

                            logger.info(f"   é…ç½®è§£ææˆåŠŸ: {source.name}")

                            # æ·»åŠ åˆ°è¿æ¥ç®¡ç†å™¨
                            self.connection_manager.add_client(
                                source.name,
                                source.source_type,
                                config
                            )
                            logger.info(f"   âœ… è¿æ¥ç®¡ç†å™¨æ·»åŠ æˆåŠŸ: {source.name}")
                            loaded_count += 1
                        else:
                            logger.warning(f"   âš ï¸  æ•°æ®æº {source.name} ç¼ºå°‘è¿æ¥é…ç½®")

                    except Exception as e:
                        logger.error(f"   âŒ åŠ è½½æ•°æ®æº {source.name} å¤±è´¥: {e}")
                        import traceback
                        logger.error(f"   é”™è¯¯å †æ ˆ: {traceback.format_exc()}")

                logger.info(f"âœ… æˆåŠŸåŠ è½½ {loaded_count}/{len(saved_sources)} ä¸ªæ•°æ®æºè¿æ¥é…ç½®")

                # éªŒè¯è¿æ¥ç®¡ç†å™¨ä¸­çš„å®¢æˆ·ç«¯
                clients = self.connection_manager.list_clients()
                logger.info(f"ğŸ“‹ è¿æ¥ç®¡ç†å™¨ä¸­çš„å®¢æˆ·ç«¯åˆ—è¡¨: {clients}")

            finally:
                db.close()
                logger.info("ğŸ”’ æ•°æ®åº“ä¼šè¯å·²å…³é—­")

        except Exception as e:
            logger.error(f"âŒ ä»æ•°æ®åº“åŠ è½½è¿æ¥é…ç½®å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            logger.info("å°†ç»§ç»­å¯åŠ¨ï¼Œä½†éœ€è¦æ‰‹åŠ¨é…ç½®æ•°æ®æºè¿æ¥")

    @cache_table_schema(ttl=1800)
    async def get_table_schema(self, source_name: str, table_name: str, database: str = None, schema: str = None) -> \
    Dict[str, Any]:
        """è·å–è¡¨ç»“æ„ - é•¿æœŸç¼“å­˜"""
        import traceback

        try:
            logger.info(f"å¼€å§‹è·å–è¡¨ç»“æ„: source={source_name}, table={table_name}, db={database}, schema={schema}")

            client = self.connection_manager.get_client(source_name)
            if not client:
                logger.error(f"æ•°æ®æºä¸å­˜åœ¨: {source_name}")
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
                }

            logger.info(f"å®¢æˆ·ç«¯ç±»å‹: {client.__class__.__name__}")

            # å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆ
            table_schema = None

            # å°è¯•1: åªä¼  table_name
            try:
                logger.info("å°è¯•è°ƒç”¨: get_table_schema(table_name)")
                table_schema = await client.get_table_schema(table_name)
                logger.info("è°ƒç”¨æˆåŠŸ(1ä¸ªå‚æ•°)")
            except TypeError as te:
                # å‚æ•°ä¸åŒ¹é…,å°è¯•ä¸‹ä¸€ç§
                logger.debug(f"1ä¸ªå‚æ•°è°ƒç”¨å¤±è´¥: {te}")

                # å°è¯•2: ä¼  table_name, database
                try:
                    logger.info("å°è¯•è°ƒç”¨: get_table_schema(table_name, database)")
                    table_schema = await client.get_table_schema(table_name, database)
                    logger.info("è°ƒç”¨æˆåŠŸ(2ä¸ªå‚æ•°)")
                except TypeError as te2:
                    # å‚æ•°ä¸åŒ¹é…,å°è¯•ä¸‹ä¸€ç§
                    logger.debug(f"2ä¸ªå‚æ•°è°ƒç”¨å¤±è´¥: {te2}")

                    # å°è¯•3: ä¼  table_name, database, schema
                    try:
                        logger.info("å°è¯•è°ƒç”¨: get_table_schema(table_name, database, schema)")
                        table_schema = await client.get_table_schema(table_name, database, schema)
                        logger.info("è°ƒç”¨æˆåŠŸ(3ä¸ªå‚æ•°)")
                    except Exception as te3:
                        logger.error(f"3ä¸ªå‚æ•°è°ƒç”¨å¤±è´¥: {te3}")
                        raise

            if not table_schema:
                raise ValueError("æ— æ³•è·å–è¡¨ç»“æ„")

            logger.info(f"æˆåŠŸè·å–è¡¨ç»“æ„,åˆ—æ•°: {len(table_schema.get('columns', []))}")

            return {
                "success": True,
                "source_name": source_name,
                "database": database,
                "schema": schema,
                "table_name": table_name,
                "columns": table_schema.get('columns', []),
                "fields": table_schema.get('columns', []),
                "schema": table_schema,
                "retrieved_at": datetime.now(),
                "cached": True
            }
        except Exception as e:
            error_trace = traceback.format_exc()
            logger.error(f"è·å–è¡¨ç»“æ„å¤±è´¥ {source_name}.{database}.{table_name}: {e}")
            logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ:\n{error_trace}")
            return {
                "success": False,
                "error": str(e),
                "error_trace": error_trace
            }

    @cache_data_preview(ttl=300)  # 5åˆ†é’Ÿç¼“å­˜
    async def execute_query(self, source_name: str, query: str, database: str = None, schema: str = None,
                            limit: int = 100) -> Dict[str, Any]:
        """æ‰§è¡ŒæŸ¥è¯¢ - çŸ­æœŸç¼“å­˜"""
        try:
            client = self.connection_manager.get_client(source_name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
                }

            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æœ‰LIMITé™åˆ¶
            if limit and "limit" not in query.lower() and "select" in query.lower():
                if query.strip().endswith(';'):
                    query = query.strip()[:-1]
                query = f"{query} LIMIT {limit}"

            start_time = datetime.now()
            import inspect

            # æ£€æŸ¥å®¢æˆ·ç«¯çš„execute_queryæ–¹æ³•æ”¯æŒå“ªäº›å‚æ•°
            sig = inspect.signature(client.execute_query)
            params = list(sig.parameters.keys())

            if 'schema' in params:
                # æ”¯æŒschemaå‚æ•°çš„å®¢æˆ·ç«¯ï¼ˆå¦‚Hive, PostgreSQLç­‰ï¼‰
                results = await client.execute_query(query, database, schema)
            else:
                # ä¸æ”¯æŒschemaå‚æ•°çš„å®¢æˆ·ç«¯ï¼ˆå¦‚MySQLç­‰ï¼‰
                results = await client.execute_query(query, database)
            end_time = datetime.now()

            return {
                "success": True,
                "source_name": source_name,
                "database": database,
                "schema": schema,
                "query": query,
                "results": results,
                "row_count": len(results),
                "execution_time_ms": int((end_time - start_time).total_seconds() * 1000),
                "executed_at": start_time,
                "cached": True
            }
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥ {source_name}: {e}")
            return {
                "success": False,
                "error": str(e),
                "query": query
            }

    async def get_data_sources_list(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ•°æ®æºåˆ—è¡¨ - æ··åˆç¼“å­˜ç­–ç•¥"""
        try:
            # ä»ç¼“å­˜è·å–åŸºç¡€åˆ—è¡¨
            cache_key = "sources_list"

            async def fetch_sources_data():
                return await self._fetch_sources_from_clients()

            sources = await self.cache_manager.get_cached_data(
                cache_key,
                fetch_sources_data,
                {'redis': 120}  # 2åˆ†é’Ÿç¼“å­˜
            )

            # å¼‚æ­¥æ›´æ–°æ•°æ®åº“ä¸­çš„ç»Ÿè®¡ä¿¡æ¯
            asyncio.create_task(self._update_sources_statistics(sources))

            return sources

        except Exception as e:
            logger.error(f"è·å–æ•°æ®æºåˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def _fetch_sources_from_clients(self) -> List[Dict[str, Any]]:
        """ä»å®¢æˆ·ç«¯è·å–æ•°æ®æºä¿¡æ¯"""
        clients = self.connection_manager.list_clients()

        if not clients and not settings.use_real_clusters:
            # è¿”å›Mockæ•°æ®
            return await self._get_mock_sources_list()

        # å¹¶è¡Œè·å–è¿æ¥çŠ¶æ€
        connection_results = await self._parallel_test_connections(clients)

        sources_list = []
        for name in clients:
            client = self.connection_manager.get_client(name)
            test_result = connection_results.get(name, {})

            source_info = {
                "name": name,
                "type": client.__class__.__name__.replace('Client', '').lower(),
                "status": "connected" if test_result.get('success') else "disconnected",
                "host": client.config.get('host', 'æœªçŸ¥'),
                "port": client.config.get('port', 0),
                "database": client.config.get('database', ''),
                "description": f"{test_result.get('database_type', 'æœªçŸ¥')} æ•°æ®åº“",
                "last_test": test_result.get('test_time', datetime.now()),
                "version": test_result.get('version', 'æœªçŸ¥'),
                "tables_count": 0  # å»¶è¿ŸåŠ è½½
            }

            if not test_result.get('success'):
                source_info["error"] = test_result.get('error', 'è¿æ¥å¤±è´¥')

            sources_list.append(source_info)

        return sources_list

    async def _get_mock_sources_list(self) -> List[Dict[str, Any]]:
        """è·å–Mockæ•°æ®æºåˆ—è¡¨"""
        return [
            {
                "name": "MySQL-Demo",
                "type": "mysql",
                "status": "connected",
                "host": "localhost",
                "port": 3306,
                "database": "demo",
                "description": "MySQLæ¼”ç¤ºæ•°æ®åº“",
                "tables_count": 15,
                "last_test": datetime.now() - timedelta(minutes=5),
                "created_at": datetime.now() - timedelta(days=7)
            },
            {
                "name": "Hive-Warehouse",
                "type": "hive",
                "status": "connected",
                "host": "hadoop101",
                "port": 10000,
                "database": "default",
                "description": "Hiveæ•°æ®ä»“åº“",
                "tables_count": 128,
                "last_test": datetime.now() - timedelta(minutes=3),
                "created_at": datetime.now() - timedelta(days=30)
            },
            # ... å…¶ä»–Mockæ•°æ®
        ]

    async def _update_sources_statistics(self, sources: List[Dict[str, Any]]):
        """å¼‚æ­¥æ›´æ–°æ•°æ®æºç»Ÿè®¡ä¿¡æ¯"""
        try:
            for source in sources:
                if source.get('status') == 'connected':
                    # å¼‚æ­¥è·å–è¡¨æ•°é‡ç­‰ç»Ÿè®¡ä¿¡æ¯
                    asyncio.create_task(self._update_single_source_stats(source['name']))
        except Exception as e:
            logger.error(f"æ›´æ–°æ•°æ®æºç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

    async def _update_single_source_stats(self, source_name: str):
        """æ›´æ–°å•ä¸ªæ•°æ®æºçš„ç»Ÿè®¡ä¿¡æ¯"""
        try:
            tables_result = await self.get_tables(source_name)
            if tables_result.get('success'):
                table_count = tables_result.get('count', 0)

                # æ›´æ–°æ•°æ®åº“è®°å½•
                db = next(get_async_db())
                data_source = db.query(DataSource).filter(DataSource.name == source_name).first()
                if data_source:
                    # è¿™é‡Œå¯ä»¥æ·»åŠ è¡¨æ•°é‡ç­‰ç»Ÿè®¡å­—æ®µåˆ°æ•°æ®æºæ¨¡å‹
                    # data_source.tables_count = table_count
                    db.commit()
                db.close()

        except Exception as e:
            logger.debug(f"æ›´æ–°æ•°æ®æºç»Ÿè®¡ä¿¡æ¯å¤±è´¥ {source_name}: {e}")

    async def batch_test_connections(self, source_names: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡æµ‹è¯•è¿æ¥ - é«˜æ•ˆå¹¶å‘ç‰ˆæœ¬"""
        try:
            # ä½¿ç”¨ç¼“å­˜å’Œå¹¶å‘å¤„ç†
            results = {}

            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…è¿‡è½½
            batch_size = 10
            for i in range(0, len(source_names), batch_size):
                batch = source_names[i:i + batch_size]
                batch_results = await self._parallel_test_connections(batch)
                results.update(batch_results)

            # ç»Ÿè®¡ç»“æœ
            total_tested = len(results)
            successful = sum(1 for result in results.values() if result.get('success'))
            failed = total_tested - successful

            # å¼‚æ­¥è®°å½•æµ‹è¯•ç»“æœ
            for name, result in results.items():
                asyncio.create_task(self._record_connection_test(name, result))

            summary = {
                "total_tested": total_tested,
                "successful": successful,
                "failed": failed,
                "success_rate": round((successful / total_tested) * 100, 1) if total_tested > 0 else 0,
                "test_results": results,
                "tested_at": datetime.now()
            }

            return summary
        except Exception as e:
            logger.error(f"æ‰¹é‡æµ‹è¯•è¿æ¥å¤±è´¥: {e}")
            raise

    async def get_health_status(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†æˆæ¨¡å—å¥åº·çŠ¶æ€"""
        try:
            # è·å–æ¦‚è§ˆä¿¡æ¯ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
            overview = await self.get_data_sources_overview()

            total_sources = overview.get('total_sources', 0)
            active_connections = overview.get('active_connections', 0)
            failed_connections = overview.get('failed_connections', 0)

            # è®¡ç®—å¥åº·åˆ†æ•°
            if total_sources == 0:
                health_score = 100
                status = "healthy"
            else:
                health_score = (active_connections / total_sources) * 100
                if health_score >= 90:
                    status = "healthy"
                elif health_score >= 70:
                    status = "warning"
                else:
                    status = "critical"

            # è·å–ç¼“å­˜ç»Ÿè®¡
            cache_stats = self.cache_manager.get_cache_stats()

            health_data = {
                "status": status,
                "health_score": round(health_score, 1),
                "total_sources": total_sources,
                "active_connections": active_connections,
                "failed_connections": failed_connections,
                "uptime_percentage": health_score,
                "last_check": datetime.now(),
                "issues": [],
                "cache_performance": cache_stats,
                "performance_metrics": {
                    "average_response_time": "< 100ms",
                    "cache_hit_rate": f"{cache_stats.get('memory_hit_rate', 0):.1f}%",
                    "concurrent_connections": len(self.connection_manager.list_clients())
                }
            }

            # æ·»åŠ é—®é¢˜æè¿°
            if failed_connections > 0:
                health_data["issues"].append(f"{failed_connections} ä¸ªæ•°æ®æºè¿æ¥å¤±è´¥")

            if total_sources == 0:
                health_data["issues"].append("æœªé…ç½®ä»»ä½•æ•°æ®æº")

            # æ£€æŸ¥ç¼“å­˜æ€§èƒ½
            if cache_stats.get('cache_miss_rate', 0) > 50:
                health_data["issues"].append("ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œå¯èƒ½å½±å“æ€§èƒ½")

            return health_data
        except Exception as e:
            logger.error(f"è·å–å¥åº·çŠ¶æ€å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e),
                "last_check": datetime.now()
            }

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜ï¼Œä½†å¯ä»¥æ·»åŠ é€‚å½“çš„ç¼“å­˜è£…é¥°å™¨
    async def get_databases(self, source_name: str) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“åˆ—è¡¨ - æ·»åŠ ç¼“å­˜"""
        cache_key = f"databases_{source_name}"

        async def fetch_databases():
            client = self.connection_manager.get_client(source_name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
                }

            databases = await client.get_databases()
            return {
                "success": True,
                "source_name": source_name,
                "databases": databases,
                "count": len(databases),
                "retrieved_at": datetime.now()
            }

        return await self.cache_manager.get_cached_data(
            cache_key,
            fetch_databases,
            {'redis': 600}  # 10åˆ†é’Ÿç¼“å­˜
        )

    async def get_tables(self, source_name: str, database: str = None, schema: str = None,
                         limit: int = 100, offset: int = 0) -> Dict[str, Any]:
        """è·å–è¡¨åˆ—è¡¨ - æ·»åŠ ç¼“å­˜å’Œåˆ†é¡µæ”¯æŒ"""
        # ç¼“å­˜keyåŒ…å«åˆ†é¡µä¿¡æ¯
        cache_key = f"tables_{source_name}_{database or 'default'}_{schema or 'default'}_{limit}_{offset}"

        async def fetch_tables():
            client = self.connection_manager.get_client(source_name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
                }

            # è°ƒç”¨å®¢æˆ·ç«¯çš„åˆ†é¡µæ–¹æ³•
            tables = await client.get_tables(database, schema, limit, offset)

            # è·å–æ€»æ•°ï¼ˆå¦‚æœå®¢æˆ·ç«¯æ”¯æŒï¼‰
            total_count = 0
            if hasattr(client, 'get_tables_count'):
                total_count = await client.get_tables_count(database, schema)
            else:
                # å¦‚æœå®¢æˆ·ç«¯ä¸æ”¯æŒget_tables_countï¼Œä¼°ç®—æ€»æ•°
                if len(tables) == limit:
                    # å¦‚æœè¿”å›çš„æ•°é‡ç­‰äºlimitï¼Œè¯´æ˜å¯èƒ½è¿˜æœ‰æ›´å¤šæ•°æ®
                    total_count = offset + len(tables) + 1  # ä¼°ç®—å€¼
                else:
                    total_count = offset + len(tables)  # å®é™…æ€»æ•°

            return {
                "success": True,
                "source_name": source_name,
                "database": database,
                "schema": schema,
                "tables": tables,
                "count": len(tables),
                "total_count": total_count,
                "limit": limit,
                "offset": offset,
                "has_more": offset + len(tables) < total_count,
                "retrieved_at": datetime.now()
            }

        return await self.cache_manager.get_cached_data(
            cache_key,
            fetch_tables,
            {'redis': 300}  # 5åˆ†é’Ÿç¼“å­˜
        )

    # å¯é€‰ï¼šæ·»åŠ è·å–æ€»æ•°çš„ç‹¬ç«‹æ–¹æ³•ï¼ˆç”¨äºå‰ç«¯åˆ†é¡µè®¡ç®—ï¼‰
    async def get_tables_count(self, source_name: str, database: str = None, schema: str = None) -> Dict[str, Any]:
        """è·å–è¡¨æ€»æ•°"""
        cache_key = f"tables_count_{source_name}_{database or 'default'}_{schema or 'default'}"

        async def fetch_count():
            client = self.connection_manager.get_client(source_name)
            if not client:
                return {"success": False, "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"}

            if hasattr(client, 'get_tables_count'):
                count = await client.get_tables_count(database, schema)
            else:
                # é™çº§ï¼šè·å–æ‰€æœ‰è¡¨ç„¶åè®¡æ•°ï¼ˆé€‚ç”¨äºè¡¨ä¸å¤šçš„æƒ…å†µï¼‰
                tables = await client.get_tables(database, schema, 999999, 0)
                count = len(tables)

            return {
                "success": True,
                "source_name": source_name,
                "database": database,
                "schema": schema,
                "total_count": count
            }

        return await self.cache_manager.get_cached_data(
            cache_key,
            fetch_count,
            {'redis': 600}  # 10åˆ†é’Ÿç¼“å­˜ï¼ˆæ€»æ•°å˜åŒ–è¾ƒå°‘ï¼‰
        )

    async def get_data_sources_overview(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æºæ¦‚è§ˆ"""
        try:
            clients = self.connection_manager.list_clients()

            # å¦‚æœæ²¡æœ‰å®¢æˆ·ç«¯ï¼Œè¿”å›åŸºç¡€ä¿¡æ¯
            if not clients:
                return {
                    "total_sources": 0,
                    "active_connections": 0,
                    "failed_connections": 0,
                    "supported_types": ["mysql", "postgresql", "hive", "doris", "kingbase"],
                    "sources_by_type": {},
                    "data_volume_estimate": "0GB",
                    "last_sync": datetime.now(),
                    "health_status": "æ­£å¸¸"
                }

            # å¹¶è¡Œæµ‹è¯•è¿æ¥
            connection_results = await self._parallel_test_connections(clients)

            # ç»Ÿè®¡ç»“æœ
            total_sources = len(clients)
            active_connections = sum(1 for result in connection_results.values() if result.get('success'))
            failed_connections = total_sources - active_connections

            # æŒ‰ç±»å‹ç»Ÿè®¡
            sources_by_type = {}
            for client_name in clients:
                client = self.connection_manager.get_client(client_name)
                client_type = client.__class__.__name__.replace('Client', '').lower()
                sources_by_type[client_type] = sources_by_type.get(client_type, 0) + 1

            return {
                "total_sources": total_sources,
                "active_connections": active_connections,
                "failed_connections": failed_connections,
                "supported_types": ["mysql", "postgresql", "hive", "doris", "kingbase"],
                "sources_by_type": sources_by_type,
                "data_volume_estimate": f"{total_sources * 10}GB",
                "last_sync": datetime.now(),
                "health_status": "è‰¯å¥½" if failed_connections == 0 else "éƒ¨åˆ†å¼‚å¸¸" if active_connections > 0 else "å¼‚å¸¸"
            }

        except Exception as e:
            logger.error(f"è·å–æ•°æ®æºæ¦‚è§ˆå¤±è´¥: {e}")
            return {
                "total_sources": 0,
                "active_connections": 0,
                "failed_connections": 0,
                "supported_types": ["mysql", "postgresql", "hive", "doris", "kingbase"],
                "sources_by_type": {},
                "data_volume_estimate": "0GB",
                "last_sync": datetime.now(),
                "health_status": "å¼‚å¸¸",
                "error": str(e)
            }

    async def _parallel_test_connections(self, client_names: List[str]) -> Dict[str, Dict[str, Any]]:
        """å¹¶è¡Œæµ‹è¯•è¿æ¥"""
        results = {}
        try:
            # é™åˆ¶å¹¶å‘æ•°é‡
            semaphore = asyncio.Semaphore(5)

            async def test_single_connection(name):
                async with semaphore:
                    try:
                        client = self.connection_manager.get_client(name)
                        if client:
                            result = await client.test_connection()
                            return name, result
                        else:
                            return name, {"success": False, "error": "å®¢æˆ·ç«¯ä¸å­˜åœ¨"}
                    except Exception as e:
                        return name, {"success": False, "error": str(e)}

            # å¹¶è¡Œæ‰§è¡Œæµ‹è¯•
            tasks = [test_single_connection(name) for name in client_names[:10]]  # é™åˆ¶æœ€å¤š10ä¸ª
            test_results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in test_results:
                if isinstance(result, tuple) and len(result) == 2:
                    name, test_result = result
                    results[name] = test_result

        except Exception as e:
            logger.error(f"å¹¶è¡Œæµ‹è¯•è¿æ¥å¤±è´¥: {e}")

        return results

    async def _estimate_total_data_volume(self, client_names: List[str]) -> str:
        """ä¼°ç®—æ€»æ•°æ®é‡"""
        try:
            # å°è¯•ä»çœŸå®é›†ç¾¤è·å–æ•°æ®é‡
            from app.utils.hadoop_client import HDFSClient

            try:
                hdfs_client = HDFSClient()
                storage_info = hdfs_client.get_storage_info()
                if storage_info and storage_info.get('total_size', 0) > 0:
                    total_gb = storage_info['total_size'] / (1024 ** 3)  # è½¬æ¢ä¸ºGB
                    return f"{total_gb:.1f}GB"
            except:
                pass

            # å¤‡ç”¨ä¼°ç®—
            total_gb = len(client_names) * 50  # ç”Ÿäº§ç¯å¢ƒä¼°ç®—æ›´å¤§
            return f"{total_gb}GB"
        except:
            return "æœªçŸ¥"

    async def _get_mock_overview(self) -> Dict[str, Any]:
        """è·å–Mockæ¦‚è§ˆæ•°æ®"""
        return {
            "total_sources": 6,
            "active_connections": 5,
            "failed_connections": 1,
            "supported_types": DatabaseClientFactory.get_supported_types(),
            "sources_by_type": {
                "mysql": 2,
                "hive": 1,
                "doris": 1,
                "kingbase": 1,
                "tidb": 1
            },
            "data_volume_estimate": "125.6GB",
            "last_sync": datetime.now(),
            "health_status": "è‰¯å¥½",
            "cache_info": self.cache_manager.get_cache_stats()
        }

    async def add_data_source(self, name: str, db_type: str, config: Dict[str, Any], description: str = "") -> Dict[
        str, Any]:
        try:
            logger.info(f"å¼€å§‹æ·»åŠ æ•°æ®æº: name={name}, type={db_type}")

            # 1. å‚æ•°éªŒè¯
            if not name or not db_type or not config:
                return {
                    "success": False,
                    "error": "å‚æ•°ä¸å®Œæ•´: éœ€è¦name, db_type, config"
                }

            # 2. æ£€æŸ¥æ•°æ®æºæ˜¯å¦å·²å­˜åœ¨
            if self.connection_manager.get_client(name):
                return {
                    "success": False,
                    "error": f"æ•°æ®æº '{name}' å·²å­˜åœ¨"
                }

            # 3. ä¸´æ—¶æ·»åŠ åˆ°è¿æ¥ç®¡ç†å™¨è¿›è¡Œæµ‹è¯•
            logger.info(f"ä¸´æ—¶æ·»åŠ åˆ°è¿æ¥ç®¡ç†å™¨: {name}")
            success = self.connection_manager.add_client(name, db_type, config)
            if not success:
                return {
                    "success": False,
                    "error": f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {db_type}"
                }

            # 4. æµ‹è¯•è¿æ¥ - å…³é”®æ­¥éª¤
            logger.info(f"å¼€å§‹è¿æ¥æµ‹è¯•: {name}")
            client = self.connection_manager.get_client(name)
            test_result = await client.test_connection()

            logger.info(f"è¿æ¥æµ‹è¯•ç»“æœ: {name} -> {test_result.get('success', False)}")

            # 5. å…³é”®ä¿®å¤ï¼šè¿æ¥æµ‹è¯•å¤±è´¥æ—¶çš„å¤„ç†
            if not test_result.get('success'):
                error_msg = test_result.get('error', 'æœªçŸ¥è¿æ¥é”™è¯¯')
                logger.warning(f"è¿æ¥æµ‹è¯•å¤±è´¥: {name}, åŸå› : {error_msg}")

                # ä»è¿æ¥ç®¡ç†å™¨ä¸­ç§»é™¤å¤±è´¥çš„è¿æ¥
                try:
                    self.connection_manager.remove_client(name)
                    logger.info(f"å·²æ¸…ç†å¤±è´¥çš„è¿æ¥: {name}")
                except Exception as remove_error:
                    logger.error(f"æ¸…ç†å¤±è´¥è¿æ¥æ—¶å‡ºé”™: {remove_error}")

                # è¿”å›å¤±è´¥ç»“æœ
                return {
                    "success": False,
                    "error": f"æ•°æ®æºè¿æ¥æµ‹è¯•å¤±è´¥: {error_msg}",
                    "test_result": test_result,
                    "connection_details": {
                        "host": config.get('host'),
                        "port": config.get('port'),
                        "database": config.get('database')
                    }
                }

            # 6. è¿æ¥æµ‹è¯•æˆåŠŸ - ä¿å­˜åˆ°æ•°æ®åº“
            logger.info(f"è¿æ¥æµ‹è¯•æˆåŠŸï¼Œå¼€å§‹ä¿å­˜åˆ°æ•°æ®åº“: {name}")
            try:
                await self._save_data_source_to_db(name, db_type, config, test_result, description)
                logger.info(f"æ•°æ®æºä¿å­˜åˆ°æ•°æ®åº“æˆåŠŸ: {name}")
            except Exception as db_error:
                logger.error(f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {name}, é”™è¯¯: {db_error}")

                # æ•°æ®åº“ä¿å­˜å¤±è´¥ï¼Œæ¸…ç†è¿æ¥ç®¡ç†å™¨
                try:
                    self.connection_manager.remove_client(name)
                    logger.info(f"å·²æ¸…ç†ä¿å­˜å¤±è´¥çš„è¿æ¥: {name}")
                except Exception as cleanup_error:
                    logger.error(f"æ¸…ç†è¿æ¥æ—¶å‡ºé”™: {cleanup_error}")

                return {
                    "success": False,
                    "error": f"ä¿å­˜æ•°æ®æºé…ç½®å¤±è´¥: {str(db_error)}",
                    "test_result": test_result
                }

            # 7. æ¸…é™¤ç›¸å…³ç¼“å­˜
            try:
                await self.cache_manager.invalidate_cache(pattern=name)
                await self.cache_manager.invalidate_cache(pattern="overview")
                logger.info(f"ç¼“å­˜æ¸…ç†æˆåŠŸ: {name}")
            except Exception as cache_error:
                logger.warning(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {cache_error}")

            # 8. æˆåŠŸå®Œæˆ
            logger.info(f"æ•°æ®æºæ·»åŠ å®Œå…¨æˆåŠŸ: {name}")
            return {
                "success": True,
                "name": name,
                "type": db_type,
                "status": "connected",
                "test_result": test_result,
                "created_at": datetime.now(),
                "message": "æ•°æ®æºè¿æ¥æµ‹è¯•æˆåŠŸå¹¶å·²ä¿å­˜"
            }

        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ•°æ®æºå¼‚å¸¸ {name}: {e}")
            import traceback
            logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")

            # å¼‚å¸¸æƒ…å†µä¸‹æ¸…ç†è¿æ¥ç®¡ç†å™¨
            try:
                self.connection_manager.remove_client(name)
                logger.info(f"âœ… å¼‚å¸¸æ¸…ç†è¿æ¥æˆåŠŸ: {name}")
            except Exception as cleanup_error:
                logger.error(f"âš ï¸ å¼‚å¸¸æ¸…ç†è¿æ¥å¤±è´¥: {cleanup_error}")

            return {
                "success": False,
                "error": f"æ·»åŠ æ•°æ®æºæ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
            }

    async def _save_data_source_to_db(self, name: str, db_type: str, config: Dict[str, Any],
                                      test_result: Dict[str, Any],
                                      description: str = ""):
        """ä¿å­˜æ•°æ®æºé…ç½®åˆ°æ•°æ®åº“"""
        try:
            from app.utils.database import get_sync_db_session
            from sqlalchemy import text
            import json

            # ä½¿ç”¨åŒæ­¥æ•°æ®åº“ä¼šè¯
            db = get_sync_db_session()

            try:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                result = db.execute(text("SHOW TABLES LIKE 'data_sources'"))
                if not result.fetchone():
                    logger.warning("data_sourcesè¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡æ•°æ®åº“ä¿å­˜")
                    return

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                result = db.execute(text("SELECT id FROM data_sources WHERE name = :name"),
                                    {"name": name})
                existing = result.fetchone()

                if existing:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    db.execute(text("""
                        UPDATE data_sources
                        SET source_type = :db_type, connection_config = :config, status = :status, 
                            last_connection_test = NOW(), description = :description, is_active = TRUE
                        WHERE name = :name
                    """), {
                        "db_type": db_type,
                        "config": json.dumps(config),
                        "status": "connected" if test_result.get('success') else "disconnected",
                        "description": description,
                        "name": name
                    })
                else:
                    # åˆ›å»ºæ–°è®°å½•
                    db.execute(text("""
                        INSERT INTO data_sources 
                        (name, display_name, source_type, connection_config, status, description, is_active, last_connection_test)
                        VALUES (:name, :display_name, :db_type, :config, :status, :description, TRUE, NOW())
                    """), {
                        "name": name,
                        "display_name": name,
                        "db_type": db_type,
                        "config": json.dumps(config),
                        "status": "connected" if test_result.get('success') else "disconnected",
                        "description": description
                    })

                db.commit()
                logger.info(f"æ•°æ®æº {name} ä¿å­˜åˆ°æ•°æ®åº“æˆåŠŸ")

            finally:
                db.close()

        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®æºåˆ°æ•°æ®åº“å¤±è´¥: {e}")
            if 'db' in locals():
                try:
                    db.rollback()
                    db.close()
                except:
                    pass

    @cache_connection_status(ttl=30)
    async def test_data_source(self, name: str) -> Dict[str, Any]:
        """æµ‹è¯•æ•°æ®æºè¿æ¥ - ç¼“å­˜ä¼˜åŒ–"""
        try:
            client = self.connection_manager.get_client(name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {name} ä¸å­˜åœ¨"
                }

            result = await client.test_connection()

            # å¼‚æ­¥è®°å½•æµ‹è¯•ç»“æœåˆ°æ•°æ®åº“
            asyncio.create_task(self._record_connection_test(name, result))

            return result
        except Exception as e:
            logger.error(f"æµ‹è¯•æ•°æ®æºè¿æ¥å¤±è´¥ {name}: {e}")
            return {
                "success": False,
                "error": str(e),
                "test_time": datetime.now()
            }

    async def _record_connection_test(self, name: str, test_result: Dict[str, Any]):
        """å¼‚æ­¥è®°å½•è¿æ¥æµ‹è¯•ç»“æœ"""
        try:
            db = next(get_async_db())
            data_source = db.query(DataSource).filter(DataSource.name == name).first()

            if data_source:
                # æ›´æ–°æ•°æ®æºçŠ¶æ€
                data_source.status = "online" if test_result.get('success') else "offline"
                data_source.last_connection_test = datetime.now()

                # è®°å½•è¿æ¥å†å²
                connection_record = DataSourceConnection(
                    data_source_id=data_source.id,
                    connection_timestamp=datetime.now(),
                    connection_type="test",
                    success=test_result.get('success', False),
                    response_time_ms=test_result.get('response_time_ms', 0),
                    error_message=test_result.get('error') if not test_result.get('success') else None
                )
                db.add(connection_record)
                db.commit()

            db.close()
        except Exception as e:
            logger.error(f"è®°å½•è¿æ¥æµ‹è¯•ç»“æœå¤±è´¥: {e}")

    async def remove_data_source(self, name: str) -> Dict[str, Any]:
        """åˆ é™¤æ•°æ®æº"""
        try:
            logger.info(f"å¼€å§‹åˆ é™¤æ•°æ®æº: {name}")

            # æ£€æŸ¥æ•°æ®æºæ˜¯å¦å­˜åœ¨
            client_exists = self.connection_manager.get_client(name) is not None
            logger.info(f"æ•°æ®æºæ˜¯å¦å­˜åœ¨: {client_exists}")

            if not client_exists:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {name} ä¸å­˜åœ¨"
                }

            # ä»è¿æ¥ç®¡ç†å™¨ä¸­ç§»é™¤
            logger.info(f"æ­¥éª¤1: ä»è¿æ¥ç®¡ç†å™¨ç§»é™¤ {name}")
            self.connection_manager.remove_client(name)
            logger.info(f"è¿æ¥ç®¡ç†å™¨ç§»é™¤å®Œæˆ: {name}")

            # ä»æ•°æ®åº“ä¸­è½¯åˆ é™¤
            try:
                logger.info(f"æ­¥éª¤2: å¼€å§‹æ•°æ®åº“è½¯åˆ é™¤ {name}")
                await self._remove_from_db(name)
                logger.info(f"æ•°æ®åº“è½¯åˆ é™¤æˆåŠŸ: {name}")
            except Exception as db_error:
                logger.error(f"æ•°æ®åº“åˆ é™¤å¤±è´¥ {name}: {db_error}")
                # å³ä½¿æ•°æ®åº“åˆ é™¤å¤±è´¥ï¼Œè¿æ¥å·²ç§»é™¤ï¼Œä»ç„¶è®¤ä¸ºåˆ é™¤æˆåŠŸ

            logger.info(f"æ­¥éª¤3: åˆ é™¤æ“ä½œå®Œæˆ {name}")
            return {
                "success": True,
                "message": f"æ•°æ®æº {name} åˆ é™¤æˆåŠŸ"
            }

        except Exception as e:
            logger.error(f"åˆ é™¤æ•°æ®æºå¼‚å¸¸ {name}: {e}")
            import traceback
            logger.error(f"åˆ é™¤å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e) if str(e) else "åˆ é™¤è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯"
            }
    async def _remove_data_source_from_db(self, name: str) -> bool:
        """ä»æ•°æ®åº“åˆ é™¤æ•°æ®æºï¼ˆè½¯åˆ é™¤ï¼‰"""
        try:
            db = next(get_async_db())
            data_source = db.query(DataSource).filter(DataSource.name == name).first()

            if data_source:
                # è½¯åˆ é™¤ï¼šè®¾ç½®ä¸ºéæ´»è·ƒçŠ¶æ€
                data_source.is_active = False
                data_source.status = "deleted"
                db.commit()
                logger.info(f"æ•°æ®æº {name} å·²ä»æ•°æ®åº“è½¯åˆ é™¤")
                return True
            else:
                logger.warning(f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æ•°æ®æº {name}")
                return False

            db.close()

        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“åˆ é™¤æ•°æ®æºå¤±è´¥: {e}")
            if 'db' in locals():
                db.rollback()
                db.close()
            return False

    async def _remove_from_db(self, name: str):
        """ä»æ•°æ®åº“ä¸­ç§»é™¤æ•°æ®æº - è°ƒè¯•ç‰ˆæœ¬"""
        try:
            logger.info(f"_remove_from_db: å¼€å§‹å¤„ç† {name}")

            from app.utils.database import get_sync_db_session
            from sqlalchemy import text

            logger.info(f"_remove_from_db: è·å–æ•°æ®åº“ä¼šè¯")
            db = get_sync_db_session()

            try:
                logger.info(f"_remove_from_db: æ‰§è¡ŒSQLæ›´æ–° {name}")
                # è½¯åˆ é™¤ - ä½¿ç”¨æ­£ç¡®çš„SQLAlchemyå ä½ç¬¦æ ¼å¼
                result = db.execute(text("""
                    UPDATE data_sources 
                    SET is_active = FALSE, status = 'deleted'
                    WHERE name = :name
                """), {"name": name})

                logger.info(f"_remove_from_db: SQLæ‰§è¡Œç»“æœ rowcount={result.rowcount}")
                db.commit()
                logger.info(f"_remove_from_db: æ•°æ®åº“æäº¤æˆåŠŸ {name}")

            except Exception as db_error:
                logger.error(f"_remove_from_db: æ•°æ®åº“æ“ä½œå¤±è´¥ {name}: {db_error}")
                import traceback
                logger.error(f"_remove_from_db: æ•°æ®åº“å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                db.rollback()
                raise
            finally:
                logger.info(f"_remove_from_db: å…³é—­æ•°æ®åº“è¿æ¥")
                db.close()

        except Exception as e:
            logger.error(f"_remove_from_db: æ•´ä½“å¼‚å¸¸ {name}: {e}")
            import traceback
            logger.error(f"_remove_from_db: æ•´ä½“å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
    async def get_table_metadata(self, source_name: str, table_name: str, database: str = None, schema: str = None) -> Dict[str, Any]:
        """è·å–è¡¨çš„å®Œæ•´å…ƒæ•°æ® - ç¼“å­˜ä¼˜åŒ–"""
        cache_key = f"metadata_{source_name}_{database or 'default'}_{table_name}"

        async def fetch_metadata():
            client = self.connection_manager.get_client(source_name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
                }

            metadata = await client.get_table_metadata(table_name, database,schema)
            return {
                "success": True,
                "source_name": source_name,
                "metadata": metadata,
                "retrieved_at": datetime.now()
            }

        return await self.cache_manager.get_cached_data(
            cache_key,
            fetch_metadata,
            {'redis': 1800}  # 30åˆ†é’Ÿç¼“å­˜
        )

    async def search_tables(self, keyword: str = None, source_name: str = None, table_type: str = None) -> Dict[
        str, Any]:
        """æœç´¢è¡¨ - æ™ºèƒ½ç¼“å­˜"""
        cache_key = f"search_{keyword or 'all'}_{source_name or 'all'}_{table_type or 'all'}"

        async def fetch_search_results():
            all_tables = []
            clients_to_search = [source_name] if source_name else self.connection_manager.list_clients()

            # å¹¶è¡Œæœç´¢å¤šä¸ªæ•°æ®æº
            search_tasks = []
            for client_name in clients_to_search[:10]:  # é™åˆ¶å¹¶å‘æ•°
                search_tasks.append(self._search_single_source(client_name, keyword, table_type))

            search_results = await asyncio.gather(*search_tasks, return_exceptions=True)

            for result in search_results:
                if isinstance(result, list):
                    all_tables.extend(result)

            return {
                "success": True,
                "tables": all_tables,
                "total_count": len(all_tables),
                "search_criteria": {
                    "keyword": keyword,
                    "source_name": source_name,
                    "table_type": table_type
                },
                "searched_at": datetime.now()
            }

        return await self.cache_manager.get_cached_data(
            cache_key,
            fetch_search_results,
            {'redis': 600}  # 10åˆ†é’Ÿç¼“å­˜
        )

    async def _search_single_source(self, client_name: str, keyword: str, table_type: str) -> List[Dict[str, Any]]:
        """æœç´¢å•ä¸ªæ•°æ®æº"""
        try:
            tables_result = await self.get_tables(client_name)
            if not tables_result.get('success'):
                return []

            tables = []
            for table in tables_result['tables']:
                table['source_name'] = client_name

                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if keyword and keyword.lower() not in table['table_name'].lower():
                    continue
                if table_type and table.get('table_type', '').lower() != table_type.lower():
                    continue

                tables.append(table)

            return tables
        except Exception as e:
            logger.warning(f"æœç´¢æ•°æ®æº {client_name} æ—¶å‡ºé”™: {e}")
            return []

    async def get_supported_database_types(self) -> List[Dict[str, Any]]:
        """è·å–æ”¯æŒçš„æ•°æ®åº“ç±»å‹ - é™æ€ç¼“å­˜"""
        cache_key = "supported_types"

        async def fetch_types():
            from app.utils.data_integration_clients import DatabaseClientFactory
            return DatabaseClientFactory.get_supported_types()

        types = await self.cache_manager.get_cached_data(
            cache_key,
            fetch_types,
            {'redis': 3600}  # 1å°æ—¶ç¼“å­˜
        )

        return types

    async def preview_data_source(self, source_name: str, table_name: str = None, database: str = None,
                                  limit: int = 10) -> \
            Dict[str, Any]:
        """é¢„è§ˆæ•°æ®æºæ•°æ®"""
        try:
            client = self.connection_manager.get_client(source_name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
                }

            # å¦‚æœæ²¡æœ‰æŒ‡å®šè¡¨åï¼Œè·å–ç¬¬ä¸€ä¸ªè¡¨
            if not table_name:
                tables_result = await self.get_tables(source_name, database)
                if not tables_result.get('success') or not tables_result.get('tables'):
                    return {
                        "success": False,
                        "error": "æ²¡æœ‰å¯ç”¨çš„è¡¨è¿›è¡Œé¢„è§ˆ"
                    }
                # è·å–ç¬¬ä¸€ä¸ªè¡¨çš„åç§°
                tables = tables_result['tables']
                if tables:
                    table_name = tables[0]['table_name']  # ä¿®å¤ï¼šä»tablesç»“æœä¸­è·å–table_name
                else:
                    return {
                        "success": False,
                        "error": "æ•°æ®æºä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨"
                    }

            # æ‰§è¡ŒæŸ¥è¯¢é¢„è§ˆ
            query = f"SELECT * FROM {table_name} LIMIT {limit}"
            result = await self.execute_query(source_name, query, database, limit=limit)

            if result.get('success'):
                return {
                    "success": True,
                    "source_name": source_name,
                    "table_name": table_name,
                    "database": database,
                    "preview_data": result.get('results', []),
                    "row_count": len(result.get('results', [])),
                    "limit": limit
                }
            else:
                return result

        except Exception as e:
            logger.error(f"é¢„è§ˆæ•°æ®æºå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def get_data_source_statistics(self, source_name: str) -> Dict[str, Any]:
        """è·å–æ•°æ®æºç»Ÿè®¡ä¿¡æ¯ - ç¼“å­˜ä¼˜åŒ–"""
        cache_key = f"statistics_{source_name}"

        async def fetch_statistics():
            # è·å–æ•°æ®åº“åˆ—è¡¨
            databases_result = await self.get_databases(source_name)
            if not databases_result.get('success'):
                return {
                    "success": False,
                    "error": databases_result.get('error', 'æ•°æ®æºä¸å­˜åœ¨')
                }

            databases = databases_result.get('databases', [])
            total_tables = 0
            tables_by_database = {}

            # å¹¶è¡Œç»Ÿè®¡æ¯ä¸ªæ•°æ®åº“çš„è¡¨æ•°é‡
            table_tasks = []
            for db in databases[:5]:  # é™åˆ¶å¹¶å‘æ•°ï¼Œåªç»Ÿè®¡å‰5ä¸ªæ•°æ®åº“
                table_tasks.append(self._count_tables_in_database(source_name, db))

            table_results = await asyncio.gather(*table_tasks, return_exceptions=True)

            for i, (db, result) in enumerate(zip(databases[:5], table_results)):
                if isinstance(result, dict) and result.get('success'):
                    db_table_count = result.get('count', 0)
                    total_tables += db_table_count
                    tables_by_database[db] = db_table_count
                else:
                    tables_by_database[db] = 0

            # æµ‹è¯•è¿æ¥çŠ¶æ€
            test_result = await self.test_data_source(source_name)

            statistics = {
                "source_name": source_name,
                "connection_status": "connected" if test_result.get('success') else "disconnected",
                "database_type": test_result.get('database_type', 'æœªçŸ¥'),
                "version": test_result.get('version', 'æœªçŸ¥'),
                "total_databases": len(databases),
                "total_tables": total_tables,
                "tables_by_database": tables_by_database,
                "last_test": test_result.get('test_time', datetime.now()),
                "response_time_ms": test_result.get('response_time_ms', 0),
                "collected_at": datetime.now()
            }

            if not test_result.get('success'):
                statistics["error"] = test_result.get('error', 'è¿æ¥å¤±è´¥')

            return {
                "success": True,
                **statistics
            }

        return await self.cache_manager.get_cached_data(
            cache_key,
            fetch_statistics,
            {'redis': 900}  # 15åˆ†é’Ÿç¼“å­˜
        )

    async def _count_tables_in_database(self, source_name: str, database: str) -> Dict[str, Any]:
        """ç»Ÿè®¡å•ä¸ªæ•°æ®åº“çš„è¡¨æ•°é‡"""
        try:
            tables_result = await self.get_tables(source_name, database)
            return tables_result
        except Exception as e:
            return {"success": False, "error": str(e), "count": 0}

    # Excelç›¸å…³æ–¹æ³•
    async def upload_excel_source(self, name: str, file, description: str = None) -> Dict[str, Any]:
        """ä¸Šä¼ Excelæ–‡ä»¶åˆ›å»ºæ•°æ®æº"""
        try:
            from app.utils.data_integration_clients import excel_service

            # éªŒè¯æ–‡ä»¶ç±»å‹
            if not file.filename.lower().endswith(('.xlsx', '.xls')):
                return {
                    "success": False,
                    "error": "ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œè¯·ä¸Šä¼ .xlsxæˆ–.xlsæ–‡ä»¶"
                }

            # éªŒè¯æ–‡ä»¶å¤§å° (é™åˆ¶ä¸º50MB)
            content = await file.read()
            file_size = len(content)
            await file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ

            if file_size > 50 * 1024 * 1024:  # 50MB
                return {
                    "success": False,
                    "error": "æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼ˆ50MBï¼‰"
                }

            # åˆ›å»ºExcelæ•°æ®æº
            result = await excel_service.create_excel_source(name, file, description)

            if result.get('success'):
                # æ·»åŠ åˆ°è¿æ¥ç®¡ç†å™¨
                config = result['config']
                add_result = await self.add_data_source(
                    name=name,
                    db_type='excel',
                    config=config
                )

                if add_result.get('success'):
                    # æ¸…é™¤ç›¸å…³ç¼“å­˜
                    await self.cache_manager.invalidate_cache(pattern="overview")

                    return {
                        "success": True,
                        "source_info": add_result,
                        "upload_info": result['upload_info']
                    }
                else:
                    # å¦‚æœæ·»åŠ æ•°æ®æºå¤±è´¥ï¼Œåˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶
                    await excel_service.delete_excel_source(config)
                    return {
                        "success": False,
                        "error": add_result.get('error', 'åˆ›å»ºæ•°æ®æºå¤±è´¥')
                    }
            else:
                return result

        except Exception as e:
            logger.error(f"ä¸Šä¼ Excelæ–‡ä»¶å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def list_excel_files(self) -> List[Dict[str, Any]]:
        """è·å–Excelæ–‡ä»¶åˆ—è¡¨"""
        try:
            # è¿”å›ç©ºåˆ—è¡¨ï¼Œå®é™…å®ç°å¯ä»¥æ‰«æä¸Šä¼ ç›®å½•
            return []
        except Exception as e:
            logger.error(f"è·å–Excelæ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def get_excel_sheets(self, source_name: str) -> Dict[str, Any]:
        """è·å–Excelå·¥ä½œè¡¨åˆ—è¡¨"""
        try:
            result = await self.get_tables(source_name)
            return result
        except Exception as e:
            logger.error(f"è·å–Excelå·¥ä½œè¡¨åˆ—è¡¨å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def preview_excel_sheet(self, source_name: str, sheet_name: str, limit: int = 10) -> Dict[str, Any]:
        """é¢„è§ˆExcelå·¥ä½œè¡¨æ•°æ®"""
        try:
            client = self.connection_manager.get_client(source_name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
                }

            # æ£€æŸ¥æ˜¯å¦ä¸ºExcelå®¢æˆ·ç«¯
            if not hasattr(client, 'get_data_preview'):
                return {
                    "success": False,
                    "error": "è¯¥æ•°æ®æºä¸æ”¯æŒé¢„è§ˆåŠŸèƒ½"
                }

            result = await client.get_data_preview(sheet_name, limit)
            return result
        except Exception as e:
            logger.error(f"é¢„è§ˆExcelå·¥ä½œè¡¨å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def delete_excel_source(self, source_name: str) -> Dict[str, Any]:
        """åˆ é™¤Excelæ•°æ®æºå’Œå¯¹åº”çš„æ–‡ä»¶"""
        try:
            client = self.connection_manager.get_client(source_name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
                }

            # è·å–æ–‡ä»¶é…ç½®
            config = client.config if hasattr(client, 'config') else {}

            # åˆ é™¤æ–‡ä»¶
            from app.utils.data_integration_clients import excel_service
            delete_result = await excel_service.delete_excel_source(config)

            # åˆ é™¤æ•°æ®æº
            source_result = await self.remove_data_source(source_name)

            return {
                "success": True,
                "file_deleted": delete_result.get('success', False),
                "source_deleted": source_result.get('success', False),
                "file_message": delete_result.get('message', ''),
                "source_message": source_result.get('message', '')
            }
        except Exception as e:
            logger.error(f"åˆ é™¤Excelæ•°æ®æºå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def export_excel_sheet(self, source_name: str, sheet_name: str, export_format: str = "json",
                                 limit: int = None) -> \
            Dict[str, Any]:
        """å¯¼å‡ºExcelå·¥ä½œè¡¨æ•°æ®"""
        try:
            # æ„å»ºæŸ¥è¯¢
            query = f"SELECT * FROM {sheet_name}"
            if limit:
                query += f" LIMIT {limit}"

            result = await self.execute_query(
                source_name=source_name,
                query=query,
                limit=limit or 1000
            )

            if result.get('success'):
                data = result['results']

                return {
                    "success": True,
                    "sheet_name": sheet_name,
                    "export_format": export_format,
                    "data": data,
                    "row_count": len(data),
                    "exported_at": datetime.now()
                }
            else:
                return result

        except Exception as e:
            logger.error(f"å¯¼å‡ºExcelå·¥ä½œè¡¨å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _load_real_cluster_connections(self):
        """åŠ è½½çœŸå®é›†ç¾¤è¿æ¥"""
        try:
            # åŠ è½½Hiveè¿æ¥
            if settings.HIVE_SERVER_HOST:
                hive_config = {
                    "host": settings.HIVE_SERVER_HOST,
                    "port": settings.HIVE_SERVER_PORT,
                    "username": settings.HIVE_USERNAME,
                    "password": settings.HIVE_PASSWORD,
                    "database": settings.HIVE_DATABASE or "default"
                }
                self.connection_manager.add_client("Production-Hive", "hive", hive_config)
                logger.info("âœ… åŠ è½½Hiveç”Ÿäº§è¿æ¥")

            # åŠ è½½HDFSè¿æ¥ï¼ˆå¦‚æœæœ‰å®¢æˆ·ç«¯æ”¯æŒï¼‰
            if settings.HDFS_NAMENODE:
                hdfs_config = {
                    "namenode": settings.HDFS_NAMENODE,
                    "user": settings.HDFS_USER
                }
                # å¦‚æœæœ‰HDFSå®¢æˆ·ç«¯å®ç°ï¼Œåœ¨è¿™é‡Œæ·»åŠ 
                logger.info("âœ… HDFSé…ç½®å·²è¯»å–")

        except Exception as e:
            logger.error(f"åŠ è½½çœŸå®é›†ç¾¤è¿æ¥å¤±è´¥: {e}")

    async def _get_production_fallback_overview(self) -> Dict[str, Any]:
        """ç”Ÿäº§ç¯å¢ƒå¤‡ç”¨æ¦‚è§ˆæ•°æ®"""
        return {
            "total_sources": 2,
            "active_connections": 1,
            "failed_connections": 1,
            "supported_types": DatabaseClientFactory.get_supported_types(),
            "sources_by_type": {"hive": 1, "hdfs": 1},
            "data_volume_estimate": "100.0GB",
            "last_sync": datetime.now(),
            "health_status": "éƒ¨åˆ†å¼‚å¸¸",
            "error": "éƒ¨åˆ†æ•°æ®æºè¿æ¥å¤±è´¥",
            "cluster_info": {
                "hive_host": settings.HIVE_SERVER_HOST,
                "hdfs_namenode": settings.HDFS_NAMENODE
            }
        }

    async def get_data_sources_list_basic(self) -> List[Dict[str, Any]]:
        """è·å–åŸºç¡€æ•°æ®æºåˆ—è¡¨ - ä¸æŸ¥è¯¢è¡¨æ•°é‡ï¼Œæ€§èƒ½ä¼˜åŒ–"""
        sources = []
        client_names = self.connection_manager.list_clients()

        # âœ… å¦‚æœè¿æ¥ç®¡ç†å™¨ä¸ºç©º,ç›´æ¥ä»æ•°æ®åº“è¯»å–
        if not client_names:
            logger.warning("è¿æ¥ç®¡ç†å™¨ä¸ºç©º,å°è¯•ç›´æ¥ä»æ•°æ®åº“è¯»å–æ•°æ®æº")
            db_sources = await self._get_sources_from_database()

            # âœ… å°è¯•é‡æ–°åŠ è½½è¿æ¥
            if db_sources:
                logger.info(f"ä»æ•°æ®åº“è¯»å–åˆ° {len(db_sources)} ä¸ªæ•°æ®æº,å°è¯•é‡æ–°åŠ è½½åˆ°è¿æ¥ç®¡ç†å™¨")
                self._load_saved_connections()

                # é‡æ–°è·å–å®¢æˆ·ç«¯åˆ—è¡¨
                client_names = self.connection_manager.list_clients()
                if not client_names:
                    logger.error("é‡æ–°åŠ è½½è¿æ¥å¤±è´¥,è¿”å›æ•°æ®åº“æ•°æ®")
                    return db_sources
                else:
                    logger.info(f"æˆåŠŸé‡æ–°åŠ è½½ {len(client_names)} ä¸ªè¿æ¥")
            else:
                return []

        for name in client_names:
            try:
                client = self.connection_manager.get_client(name)
                if not client:
                    continue

                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è·å–æ•°æ®æºç±»å‹
                db_type = 'unknown'

                # æ–¹æ³•1: ä»ç±»åæ¨æ–­ç±»å‹
                class_name = client.__class__.__name__
                if 'MySQL' in class_name:
                    db_type = 'mysql'
                elif 'KingBase' in class_name:
                    db_type = 'kingbase'
                elif 'Hive' in class_name:
                    db_type = 'hive'
                elif 'Doris' in class_name:
                    db_type = 'doris'
                elif 'PostgreSQL' in class_name:
                    db_type = 'postgresql'
                elif 'Oracle' in class_name:
                    db_type = 'oracle'
                else:
                    # æ–¹æ³•2: ä»é…ç½®æ¨æ–­ç±»å‹
                    if hasattr(client, 'config'):
                        host = client.config.get('host', '')
                        port = client.config.get('port', 0)

                        if port == 3306:
                            db_type = 'mysql'
                        elif port == 54321:
                            db_type = 'kingbase'
                        elif port == 10000:
                            db_type = 'hive'
                        elif port == 9030:
                            db_type = 'doris'
                        elif port == 5432:
                            db_type = 'postgresql'
                        elif port == 1521:
                            db_type = 'oracle'

                source_info = {
                    "name": name,
                    "type": db_type,
                    "status": "connected",
                    "last_test": datetime.now(),
                    "description": getattr(client, 'description', ''),
                    "table_count": "æœªç»Ÿè®¡"
                }

                # ğŸ”§ æ·»åŠ æ›´å¤šé…ç½®ä¿¡æ¯
                if hasattr(client, 'config'):
                    config = client.config
                    password = config.get('password', '')
                    masked_password = '******' if password else ''
                    source_info.update({
                        "host": config.get('host', ''),
                        "port": config.get('port', 0),
                        "database": config.get('database', ''),
                        "username": config.get('username', ''),
                        "password": masked_password
                    })
                    # ğŸ”§ è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥å¯†ç æ˜¯å¦å­˜åœ¨
                    password_status = "å­˜åœ¨" if config.get('password') else "ç¼ºå¤±"
                    logger.debug(f"æ•°æ®æº {name} å¯†ç çŠ¶æ€: {password_status}")

                    # ğŸ”§ å¦‚æœå¯†ç ç¼ºå¤±ï¼Œå°è¯•ä»åŸå§‹é…ç½®æˆ–å…¶ä»–æºè·å–
                    if not config.get('password'):
                        logger.warning(f"æ•°æ®æº {name} çš„å¯†ç åœ¨å®¢æˆ·ç«¯é…ç½®ä¸­ç¼ºå¤±")

                        # å¯ä»¥å°è¯•ä»æ•°æ®åº“é‡æ–°è·å–å®Œæ•´é…ç½®
                        try:
                            db_config = await self._get_source_config_from_db(name)
                            if db_config and db_config.get('password'):
                                source_info['password'] = db_config['password']
                                logger.info(f"ä»æ•°æ®åº“æ¢å¤äº†æ•°æ®æº {name} çš„å¯†ç ")
                        except Exception as e:
                            logger.error(f"ä»æ•°æ®åº“è·å–å¯†ç å¤±è´¥: {e}")
                else:
                    # å¦‚æœå®¢æˆ·ç«¯æ²¡æœ‰configå±æ€§ï¼Œå°è¯•ä»æ•°æ®åº“è·å–
                    try:
                        db_config = await self._get_source_config_from_db(name)
                        if db_config:
                            password = db_config.get('password', '')
                            masked_password = '******' if password else ''
                            source_info.update({
                                "host": db_config.get('host', ''),
                                "port": db_config.get('port', 0),
                                "database": db_config.get('database', ''),
                                "username": db_config.get('username', ''),
                                "password": masked_password
                            })
                            logger.info(f"ä»æ•°æ®åº“è·å–äº†æ•°æ®æº {name} çš„å®Œæ•´é…ç½®")
                    except Exception as e:
                        logger.error(f"ä»æ•°æ®åº“è·å–é…ç½®å¤±è´¥ {name}: {e}")
                        # è®¾ç½®é»˜è®¤å€¼
                        source_info.update({
                            "host": "",
                            "port": 0,
                            "database": "",
                            "username": "",
                            "password": ""
                        })

                source_id = await self._get_source_id_from_db(name)
                if source_id:
                    source_info["id"] = source_id  # ğŸ¯ æ·»åŠ  ID å­—æ®µ
                else:
                    logger.warning(f"æ•°æ®æº {name} æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ID")

                sources.append(source_info)

            except Exception as e:
                logger.error(f"è·å–æ•°æ®æºåŸºç¡€ä¿¡æ¯å¤±è´¥ {name}: {e}")
                sources.append({
                    "name": name,
                    "type": "unknown",
                    "status": "error",
                    "error": str(e),
                    "table_count": "æœªçŸ¥"
                })

        return sources

    async def _get_sources_from_database(self) -> List[Dict[str, Any]]:
        """ç›´æ¥ä»æ•°æ®åº“è¯»å–æ•°æ®æºåˆ—è¡¨(é™çº§æ–¹æ¡ˆ)"""
        try:
            from app.utils.database import get_async_db_context
            from app.models.data_source import DataSource
            from sqlalchemy import select
            import json

            async with get_async_db_context() as db:
                result = await db.execute(
                    select(DataSource).where(DataSource.is_active == True)
                )
                sources = result.scalars().all()

                sources_list = []
                for source in sources:
                    # âœ… è§£æè¿æ¥é…ç½®
                    config = {}
                    if source.connection_config:
                        try:
                            if isinstance(source.connection_config, str):
                                config = json.loads(source.connection_config)
                            else:
                                config = source.connection_config
                        except:
                            logger.error(f"è§£ææ•°æ®æº {source.name} çš„é…ç½®å¤±è´¥")

                    # âœ… æ„å»ºå®Œæ•´çš„æ•°æ®æºä¿¡æ¯
                    source_info = {
                        "id": source.id,
                        "name": source.name,
                        "type": source.source_type,
                        "status": "unknown",  # æ— æ³•æµ‹è¯•è¿æ¥
                        "host": config.get('host', ''),
                        "port": config.get('port', 0),
                        "database": config.get('database', ''),
                        "username": config.get('username', ''),
                        "password": '******' if config.get('password') else '',
                        "description": source.description or f"{source.source_type} æ•°æ®æº",
                        "last_test": source.last_connection_test or source.updated_at,
                        "table_count": "æœªç»Ÿè®¡"
                    }
                    sources_list.append(source_info)

                logger.info(f"âœ… ç›´æ¥ä»æ•°æ®åº“è¯»å–åˆ° {len(sources_list)} ä¸ªæ•°æ®æº")
                return sources_list

        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“è¯»å–æ•°æ®æºå¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return []

    def sync_sources_from_database(self):
        """æ‰‹åŠ¨åŒæ­¥æ•°æ®åº“ä¸­çš„æ•°æ®æºåˆ°è¿æ¥ç®¡ç†å™¨"""
        try:
            logger.info("ğŸ”„ æ‰‹åŠ¨åŒæ­¥æ•°æ®æº...")
            self._load_saved_connections()

            clients = self.connection_manager.list_clients()
            logger.info(f"âœ… åŒæ­¥å®Œæˆ,è¿æ¥ç®¡ç†å™¨ä¸­æœ‰ {len(clients)} ä¸ªå®¢æˆ·ç«¯")
            return {
                "success": True,
                "count": len(clients),
                "clients": clients
            }
        except Exception as e:
            logger.error(f"åŒæ­¥å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _get_source_config_from_db(self, source_name: str) -> Optional[Dict[str, Any]]:
        """ä»æ•°æ®åº“è·å–æ•°æ®æºçš„å®Œæ•´é…ç½®"""
        try:
            from app.utils.database import get_async_db
            from sqlalchemy import text
            import json

            async with get_async_db() as db:
                result = await db.execute(
                    text("SELECT connection_config FROM data_sources WHERE name = :name AND is_active = 1"),
                    {"name": source_name}
                )
                row = result.fetchone()

                if row and row.connection_config:
                    if isinstance(row.connection_config, str):
                        return json.loads(row.connection_config)
                    else:
                        return row.connection_config

            return None

        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“è·å–æ•°æ®æºé…ç½®å¤±è´¥ {source_name}: {e}")
            return None

    async def _get_source_id_from_db(self, source_name: str) -> Optional[int]:
        """ä»æ•°æ®åº“è·å–æ•°æ®æºçš„ID"""
        try:
            from app.utils.database import get_sync_db_session
            from sqlalchemy import text
            db = get_sync_db_session()

            try:
                result = db.execute(
                    text("SELECT id FROM data_sources WHERE name = :name AND is_active = 1"),
                    {"name": source_name}
                )
                row = result.fetchone()

                if row:
                    return row.id  # æˆ–è€… return row[0]

                return None

            finally:
                db.close()

        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“è·å–æ•°æ®æºIDå¤±è´¥ {source_name}: {e}")
            return None

    async def get_data_sources_list_with_limited_stats(
            self,
            table_limit: int = 1000,
            fast_mode: bool = True
    ) -> List[Dict[str, Any]]:
        """è·å–å¸¦é™åˆ¶ç»Ÿè®¡çš„æ•°æ®æºåˆ—è¡¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        sources = []
        client_names = self.connection_manager.list_clients()

        for name in client_names:
            try:
                client = self.connection_manager.get_client(name)
                if not client:
                    continue

                source_info = {
                    "name": name,
                    "type": client.db_type if hasattr(client, 'db_type') else 'unknown',
                    "status": "connected",
                    "last_test": datetime.now(),
                    "description": getattr(client, 'description', '')
                }

                # æ ¹æ®æ¨¡å¼è·å–è¡¨æ•°é‡
                if fast_mode:
                    # å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨ç¼“å­˜æˆ–é¢„ä¼°
                    table_count = await self._get_table_count_fast(name, limit=table_limit)
                else:
                    # å‡†ç¡®æ¨¡å¼ï¼šå®é™…æŸ¥è¯¢ä½†æœ‰é™åˆ¶
                    table_count = await self._get_table_count_accurate(name, limit=table_limit)

                source_info["table_count"] = table_count
                source_info["table_count_limited"] = table_count >= table_limit

                sources.append(source_info)

            except Exception as e:
                logger.error(f"è·å–æ•°æ®æºä¿¡æ¯å¤±è´¥ {name}: {e}")
                sources.append({
                    "name": name,
                    "type": "unknown",
                    "status": "error",
                    "error": str(e),
                    "table_count": 0
                })

        return sources

    async def _get_table_count_fast(self, source_name: str, limit: int = 1000) -> int:
        """å¿«é€Ÿè·å–è¡¨æ•°é‡ - ä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
        try:
            # å…ˆå°è¯•ä»ç¼“å­˜è·å–
            cache_key = f"table_count_fast_{source_name}"
            cached_count = await self.cache_manager.get_cached_data(
                cache_key,
                lambda: self._count_tables_with_limit(source_name, limit),
                {'redis': 1800}  # 30åˆ†é’Ÿç¼“å­˜
            )
            return cached_count

        except Exception as e:
            logger.warning(f"å¿«é€Ÿè·å–è¡¨æ•°é‡å¤±è´¥ {source_name}: {e}")
            return 0

    async def _count_tables_with_limit(self, source_name: str, limit: int) -> int:
        """ä½¿ç”¨é™åˆ¶çš„è¡¨æ•°é‡ç»Ÿè®¡"""
        try:
            client = self.connection_manager.get_client(source_name)
            if not client:
                return 0

            # åªæŸ¥è¯¢æŒ‡å®šæ•°é‡çš„è¡¨ï¼Œé¿å…å¤§é‡æŸ¥è¯¢
            result = await client.get_tables(limit=min(limit, 500), offset=0)  # æœ€å¤šæŸ¥500å¼ è¡¨
            if result.get('success'):
                tables = result.get('tables', [])
                return len(tables)
            return 0

        except Exception as e:
            logger.warning(f"ç»Ÿè®¡è¡¨æ•°é‡å¤±è´¥ {source_name}: {e}")
            return 0

    async def update_data_source(self, name: str, db_type: str = None, config: Dict[str, Any] = None,
                                 description: str = None) -> Dict[str, Any]:
        """
        æ›´æ–°æ•°æ®æºé…ç½®

        Args:
            name: æ•°æ®æºåç§°
            db_type: æ•°æ®åº“ç±»å‹ï¼ˆå¯é€‰ï¼‰
            config: è¿æ¥é…ç½®ï¼ˆå¯é€‰ï¼‰
            description: æè¿°ï¼ˆå¯é€‰ï¼‰

        Returns:
            Dict: æ“ä½œç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹æ›´æ–°æ•°æ®æº: {name}")

            # 1. æ£€æŸ¥æ•°æ®æºæ˜¯å¦å­˜åœ¨
            existing_client = self.connection_manager.get_client(name)
            if not existing_client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {name} ä¸å­˜åœ¨"
                }

            # 2. è·å–å½“å‰é…ç½®
            current_config = existing_client.config.copy()
            current_type = existing_client.__class__.__name__.replace('Client', '').lower()

            # 3. åˆå¹¶æ›´æ–°é…ç½®
            new_type = db_type if db_type else current_type
            new_config = {**current_config, **(config or {})}

            # 4. éªŒè¯æ–°é…ç½®
            if config:
                # éªŒè¯å¿…è¦å­—æ®µ
                if new_type in ['mysql', 'postgresql', 'kingbase']:
                    required_fields = ['host', 'username', 'password']
                elif new_type == 'excel':
                    required_fields = ['file_path']
                else:
                    required_fields = ['host', 'username']

                missing_fields = [field for field in required_fields if field not in new_config]
                if missing_fields:
                    return {
                        "success": False,
                        "error": f"ç¼ºå°‘å¿…è¦é…ç½®é¡¹: {', '.join(missing_fields)}"
                    }

            # 5. åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è¿›è¡Œæµ‹è¯•
            temp_name = f"{name}_temp_update"
            temp_success = self.connection_manager.add_client(temp_name, new_type, new_config)
            if not temp_success:
                return {
                    "success": False,
                    "error": f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {new_type}"
                }

            # 6. æµ‹è¯•æ–°é…ç½®è¿æ¥
            temp_client = self.connection_manager.get_client(temp_name)
            test_result = await temp_client.test_connection()

            # æ¸…ç†ä¸´æ—¶å®¢æˆ·ç«¯
            self.connection_manager.remove_client(temp_name)

            if not test_result.get('success'):
                return {
                    "success": False,
                    "error": f"è¿æ¥æµ‹è¯•å¤±è´¥: {test_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                    "test_result": test_result
                }

            # 7. æ›´æ–°è¿æ¥ç®¡ç†å™¨ä¸­çš„é…ç½®
            self.connection_manager.remove_client(name)
            update_success = self.connection_manager.add_client(name, new_type, new_config)
            if not update_success:
                # æ¢å¤åŸé…ç½®
                self.connection_manager.add_client(name, current_type, current_config)
                return {
                    "success": False,
                    "error": "æ›´æ–°è¿æ¥ç®¡ç†å™¨å¤±è´¥ï¼Œå·²æ¢å¤åŸé…ç½®"
                }

            # 8. æ›´æ–°æ•°æ®åº“è®°å½•
            try:
                await self._update_data_source_in_db(name, new_type, new_config, test_result, description)
                logger.info(f"æ•°æ®æº {name} æ•°æ®åº“è®°å½•æ›´æ–°æˆåŠŸ")
            except Exception as db_error:
                logger.error(f"æ›´æ–°æ•°æ®åº“è®°å½•å¤±è´¥: {db_error}")
                # æ•°æ®åº“æ›´æ–°å¤±è´¥ä¸å½±å“è¿æ¥ç®¡ç†å™¨çš„æ›´æ–°

            logger.info(f"æ•°æ®æº {name} æ›´æ–°æˆåŠŸ")
            return {
                "success": True,
                "name": name,
                "type": new_type,
                "status": "connected",
                "test_result": test_result,
                "updated_at": datetime.now(),
                "message": "æ•°æ®æºæ›´æ–°æˆåŠŸ"
            }

        except Exception as e:
            logger.error(f"æ›´æ–°æ•°æ®æºå¼‚å¸¸ {name}: {e}")
            import traceback
            logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return {
                "success": False,
                "error": f"æ›´æ–°æ•°æ®æºæ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
            }

    async def _update_data_source_in_db(self, name: str, db_type: str, config: Dict[str, Any],
                                        test_result: Dict[str, Any], description: str = None):
        """æ›´æ–°æ•°æ®åº“ä¸­çš„æ•°æ®æºè®°å½•"""
        try:
            from app.utils.database import get_sync_db_session
            from sqlalchemy import text
            import json

            db = get_sync_db_session()

            try:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                result = db.execute(text("SHOW TABLES LIKE 'data_sources'"))
                if not result.fetchone():
                    logger.warning("data_sourcesè¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡æ•°æ®åº“æ›´æ–°")
                    return

                # æ„å»ºæ›´æ–°SQL
                update_fields = [
                    "source_type = :db_type",
                    "connection_config = :config",
                    "status = :status",
                    "last_connection_test = NOW()"
                ]
                update_params = {
                    "db_type": db_type,
                    "config": json.dumps(config),
                    "status": "connected" if test_result.get('success') else "disconnected",
                    "name": name
                }

                # å¦‚æœæä¾›äº†æè¿°ï¼Œåˆ™æ›´æ–°æè¿°
                if description is not None:
                    update_fields.append("description = :description")
                    update_params["description"] = description

                update_sql = f"""
                    UPDATE data_sources
                    SET {', '.join(update_fields)}
                    WHERE name = :name
                """

                result = db.execute(text(update_sql), update_params)

                if result.rowcount == 0:
                    logger.warning(f"æ•°æ®æº {name} åœ¨æ•°æ®åº“ä¸­ä¸å­˜åœ¨ï¼Œæ— æ³•æ›´æ–°")
                else:
                    db.commit()
                    logger.info(f"æ•°æ®æº {name} æ•°æ®åº“è®°å½•æ›´æ–°æˆåŠŸ")

            finally:
                db.close()

        except Exception as e:
            logger.error(f"æ›´æ–°æ•°æ®åº“è®°å½•å¤±è´¥: {e}")
            if 'db' in locals():
                try:
                    db.rollback()
                    db.close()
                except:
                    pass
            raise

    async def get_data_source_info(self, name: str) -> Dict[str, Any]:
        """
        è·å–æ•°æ®æºè¯¦ç»†ä¿¡æ¯

        Args:
            name: æ•°æ®æºåç§°

        Returns:
            Dict: æ•°æ®æºä¿¡æ¯
        """
        try:
            logger.info(f"è·å–æ•°æ®æºä¿¡æ¯: {name}")

            # ä»è¿æ¥ç®¡ç†å™¨è·å–å®¢æˆ·ç«¯
            client = self.connection_manager.get_client(name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {name} ä¸å­˜åœ¨"
                }

            # è·å–åŸºæœ¬ä¿¡æ¯
            source_type = client.__class__.__name__.replace('Client', '').lower()
            config = client.config.copy()

            # éšè—æ•æ„Ÿä¿¡æ¯
            safe_config = config.copy()
            if 'password' in safe_config:
                safe_config['password'] = '***'

            # æµ‹è¯•è¿æ¥çŠ¶æ€
            test_result = await client.test_connection()

            # è·å–æ•°æ®åº“ä¸­çš„é¢å¤–ä¿¡æ¯
            db_info = await self._get_data_source_from_db(name)

            source_info = {
                "success": True,
                "name": name,
                "type": source_type,
                "config": safe_config,
                "status": "connected" if test_result.get('success') else "disconnected",
                "test_result": test_result,
                "created_at": db_info.get('created_at'),
                "updated_at": db_info.get('updated_at'),
                "description": db_info.get('description'),
                "last_connection_test": db_info.get('last_connection_test')
            }

            # å¦‚æœè¿æ¥æˆåŠŸï¼Œè·å–é¢å¤–ç»Ÿè®¡ä¿¡æ¯
            if test_result.get('success'):
                try:
                    # è·å–æ•°æ®åº“åˆ—è¡¨
                    databases = await client.get_databases()
                    source_info['databases_count'] = len(databases)
                    source_info['databases'] = databases[:5]  # åªè¿”å›å‰5ä¸ª

                    # è·å–è¡¨æ•°é‡ï¼ˆå¦‚æœæœ‰é»˜è®¤æ•°æ®åº“ï¼‰
                    if hasattr(client, 'config') and client.config.get('database'):
                        tables = await client.get_tables()
                        source_info['tables_count'] = len(tables)

                except Exception as stats_error:
                    logger.warning(f"è·å–æ•°æ®æºç»Ÿè®¡ä¿¡æ¯å¤±è´¥ {name}: {stats_error}")
                    source_info['databases_count'] = 0
                    source_info['tables_count'] = 0

            return source_info

        except Exception as e:
            logger.error(f"è·å–æ•°æ®æºä¿¡æ¯å¤±è´¥ {name}: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _get_data_source_from_db(self, name: str) -> Dict[str, Any]:
        """ä»æ•°æ®åº“è·å–æ•°æ®æºä¿¡æ¯"""
        try:
            from app.utils.database import get_sync_db_session
            from sqlalchemy import text

            db = get_sync_db_session()

            try:
                result = db.execute(text("""
                    SELECT created_at, updated_at, description, last_connection_test
                    FROM data_sources 
                    WHERE name = :name AND is_active = TRUE
                """), {"name": name})

                row = result.fetchone()
                if row:
                    return {
                        "created_at": row[0],
                        "updated_at": row[1],
                        "description": row[2],
                        "last_connection_test": row[3]
                    }
                else:
                    return {}

            finally:
                db.close()

        except Exception as e:
            logger.warning(f"ä»æ•°æ®åº“è·å–æ•°æ®æºä¿¡æ¯å¤±è´¥ {name}: {e}")
            return {}

    async def detect_data_source_health(self, name: str = None) -> Dict[str, Any]:
        """
        æ£€æµ‹æ•°æ®æºå¥åº·çŠ¶æ€

        Args:
            name: æ•°æ®æºåç§°ï¼ŒNoneè¡¨ç¤ºæ£€æµ‹æ‰€æœ‰æ•°æ®æº

        Returns:
            Dict: å¥åº·æ£€æµ‹ç»“æœ
        """
        try:
            start_time = datetime.now()
            logger.info(f"å¼€å§‹æ•°æ®æºå¥åº·æ£€æµ‹: {name or 'ALL'}")

            if name:
                # æ£€æµ‹å•ä¸ªæ•°æ®æº
                client = self.connection_manager.get_client(name)
                if not client:
                    return {
                        "success": False,
                        "error": f"æ•°æ®æº {name} ä¸å­˜åœ¨"
                    }

                health_result = await self._check_single_source_health(name, client)
                detection_time = (datetime.now() - start_time).total_seconds()

                return {
                    "success": True,
                    "source_name": name,
                    "health_status": health_result,
                    "detection_time_seconds": detection_time,
                    "detected_at": datetime.now()
                }
            else:
                # æ£€æµ‹æ‰€æœ‰æ•°æ®æº
                clients = self.connection_manager.list_clients()
                if not clients:
                    return {
                        "success": True,
                        "total_sources": 0,
                        "healthy_sources": 0,
                        "unhealthy_sources": 0,
                        "detection_results": [],
                        "detection_time_seconds": 0,
                        "detected_at": datetime.now()
                    }

                # å¹¶è¡Œæ£€æµ‹æ‰€æœ‰æ•°æ®æº
                detection_tasks = []
                for source_name in clients:
                    client = self.connection_manager.get_client(source_name)
                    if client:
                        task = self._check_single_source_health(source_name, client)
                        detection_tasks.append((source_name, task))

                # ç­‰å¾…æ‰€æœ‰æ£€æµ‹å®Œæˆ
                detection_results = []
                healthy_count = 0
                unhealthy_count = 0

                for source_name, task in detection_tasks:
                    try:
                        result = await task
                        detection_results.append({
                            "source_name": source_name,
                            **result
                        })
                        if result.get('is_healthy'):
                            healthy_count += 1
                        else:
                            unhealthy_count += 1
                    except Exception as e:
                        logger.error(f"æ£€æµ‹æ•°æ®æº {source_name} å¤±è´¥: {e}")
                        detection_results.append({
                            "source_name": source_name,
                            "is_healthy": False,
                            "error": str(e)
                        })
                        unhealthy_count += 1

                detection_time = (datetime.now() - start_time).total_seconds()

                return {
                    "success": True,
                    "total_sources": len(clients),
                    "healthy_sources": healthy_count,
                    "unhealthy_sources": unhealthy_count,
                    "health_rate": (healthy_count / len(clients) * 100) if clients else 0,
                    "detection_results": detection_results,
                    "detection_time_seconds": detection_time,
                    "detected_at": datetime.now()
                }

        except Exception as e:
            logger.error(f"æ•°æ®æºå¥åº·æ£€æµ‹å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _check_single_source_health(self, name: str, client) -> Dict[str, Any]:
        """æ£€æµ‹å•ä¸ªæ•°æ®æºçš„å¥åº·çŠ¶æ€"""
        try:
            check_start = datetime.now()

            # 1. è¿æ¥æµ‹è¯•
            connection_test = await client.test_connection()
            connection_time = (datetime.now() - check_start).total_seconds() * 1000

            if not connection_test.get('success'):
                health_info = {
                    "is_healthy": False,
                    "connection_status": "failed",
                    "connection_error": connection_test.get('error'),
                    "connection_time_ms": connection_time,
                    "checks_performed": ["connection"],
                    "health_score": 0
                }
                # è®°å½•å¥åº·æ£€æµ‹ç»“æœ
                asyncio.create_task(self._record_health_check(name, health_info))
                return health_info

            health_info = {
                "is_healthy": True,
                "connection_status": "success",
                "connection_time_ms": connection_time,
                "database_version": connection_test.get('version'),
                "checks_performed": ["connection"],
                "health_score": 100  # åˆå§‹åˆ†æ•°
            }

            # 2. æ•°æ®åº“åˆ—è¡¨æ£€æµ‹
            try:
                databases = await client.get_databases()
                health_info["databases_count"] = len(databases)
                health_info["databases_accessible"] = True
                health_info["checks_performed"].append("databases")
            except Exception as db_error:
                logger.warning(f"æ•°æ®æº {name} æ•°æ®åº“åˆ—è¡¨æ£€æµ‹å¤±è´¥: {db_error}")
                health_info["databases_accessible"] = False
                health_info["database_error"] = str(db_error)
                health_info["health_score"] -= 20  # å‡åˆ†

            # 3. è¡¨ç»“æ„æ£€æµ‹ï¼ˆå¦‚æœæœ‰é»˜è®¤æ•°æ®åº“ï¼‰
            try:
                if hasattr(client, 'config') and client.config.get('database'):
                    tables = await client.get_tables()
                    health_info["tables_count"] = len(tables)
                    health_info["tables_accessible"] = True
                    health_info["checks_performed"].append("tables")
            except Exception as table_error:
                logger.warning(f"æ•°æ®æº {name} è¡¨ç»“æ„æ£€æµ‹å¤±è´¥: {table_error}")
                health_info["tables_accessible"] = False
                health_info["table_error"] = str(table_error)
                health_info["health_score"] -= 10  # å‡åˆ†

            # 4. ç®€å•æŸ¥è¯¢æµ‹è¯•ï¼ˆå¦‚æœæ”¯æŒï¼‰
            try:
                if hasattr(client, 'execute_query'):
                    query_start = datetime.now()
                    await client.execute_query("SELECT 1 as test_query")
                    query_time = (datetime.now() - query_start).total_seconds() * 1000
                    health_info["query_test"] = "success"
                    health_info["query_time_ms"] = query_time
                    health_info["checks_performed"].append("query")
            except Exception as query_error:
                logger.warning(f"æ•°æ®æº {name} æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {query_error}")
                health_info["query_test"] = "failed"
                health_info["query_error"] = str(query_error)
                health_info["health_score"] -= 10  # å‡åˆ†

            # æœ€ç»ˆå¥åº·çŠ¶æ€åˆ¤å®š
            health_info["is_healthy"] = health_info["health_score"] >= 70

            # è®°å½•å¥åº·æ£€æµ‹ç»“æœåˆ°æ•°æ®åº“
            asyncio.create_task(self._record_health_check(name, health_info))

            return health_info

        except Exception as e:
            logger.error(f"æ£€æµ‹æ•°æ®æº {name} å¥åº·çŠ¶æ€å¤±è´¥: {e}")
            health_info = {
                "is_healthy": False,
                "error": str(e),
                "health_score": 0
            }
            asyncio.create_task(self._record_health_check(name, health_info))
            return health_info

    async def _record_health_check(self, name: str, health_info: Dict[str, Any]):
        """è®°å½•å¥åº·æ£€æµ‹ç»“æœåˆ°æ•°æ®åº“"""
        try:
            from app.utils.database import get_sync_db_session
            from sqlalchemy import text
            import json

            db = get_sync_db_session()

            try:
                # 1. æ›´æ–°æ•°æ®æºçŠ¶æ€
                db.execute(text("""
                    UPDATE data_sources 
                    SET status = :status, last_connection_test = NOW()
                    WHERE name = :name
                """), {
                    "name": name,
                    "status": "connected" if health_info.get('is_healthy') else "disconnected"
                })

                # 2. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ data_source_health_checks è¡¨
                result = db.execute(text("SHOW TABLES LIKE 'data_source_health_checks'"))
                if not result.fetchone():
                    # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºè¡¨

                    logger.info("åˆ›å»º data_source_health_checks è¡¨æˆåŠŸ")

                # 3. è®°å½•å¥åº·æ£€æµ‹å†å²
                db.execute(text("""
                    INSERT INTO data_source_health_checks 
                    (data_source_id, check_timestamp, check_type, is_healthy, health_score, 
                     connection_status, connection_time_ms, databases_accessible, databases_count,
                     tables_accessible, tables_count, query_test, query_time_ms,
                     connection_error, database_error, table_error, query_error, general_error, checks_performed)
                    SELECT ds.id, NOW(), 'full', :is_healthy, :health_score,
                           :connection_status, :connection_time_ms, :databases_accessible, :databases_count,
                           :tables_accessible, :tables_count, :query_test, :query_time_ms,
                           :connection_error, :database_error, :table_error, :query_error, :general_error, :checks_performed
                    FROM data_sources ds WHERE ds.name = :name
                """), {
                    "name": name,
                    "is_healthy": health_info.get('is_healthy', False),
                    "health_score": health_info.get('health_score', 0),
                    "connection_status": health_info.get('connection_status'),
                    "connection_time_ms": health_info.get('connection_time_ms'),
                    "databases_accessible": health_info.get('databases_accessible'),
                    "databases_count": health_info.get('databases_count'),
                    "tables_accessible": health_info.get('tables_accessible'),
                    "tables_count": health_info.get('tables_count'),
                    "query_test": health_info.get('query_test'),
                    "query_time_ms": health_info.get('query_time_ms'),
                    "connection_error": health_info.get('connection_error'),
                    "database_error": health_info.get('database_error'),
                    "table_error": health_info.get('table_error'),
                    "query_error": health_info.get('query_error'),
                    "general_error": health_info.get('error'),
                    "checks_performed": json.dumps(health_info.get('checks_performed', []))
                })

                db.commit()

            finally:
                db.close()

        except Exception as e:
            logger.error(f"è®°å½•å¥åº·æ£€æµ‹ç»“æœå¤±è´¥: {e}")
            if 'db' in locals():
                try:
                    db.rollback()
                    db.close()
                except:
                    pass


_service_instance = None

def get_optimized_data_integration_service():
    """è·å–æ•°æ®é›†æˆæœåŠ¡å®ä¾‹"""
    global _service_instance
    if _service_instance is None:
        _service_instance = OptimizedDataIntegrationService()
    return _service_instance


