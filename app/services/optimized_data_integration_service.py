# app/services/optimized_data_integration_service.py
"""
ä¼˜åŒ–åçš„æ•°æ®é›†æˆæœåŠ¡ - æ”¯æŒé«˜å¹¶å‘å’Œå¤šå±‚ç¼“å­˜
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from loguru import logger
import json
from pathlib import Path
import os
from app.utils.data_integration_clients import (
    DatabaseClientFactory,
    connection_manager,
)
from app.utils.integration_cache import (
    integration_cache,
    cache_connection_status,
    cache_table_schema,
    cache_data_preview
)
from app.models.data_source import DataSource, DataSourceConnection
from app.utils.database import get_db
from config.settings import settings


class OptimizedDataIntegrationService:
    """ä¼˜åŒ–åçš„æ•°æ®é›†æˆæœåŠ¡"""

    def __init__(self):
        self.connection_manager = connection_manager
        self.cache_manager = integration_cache
        self._initialize_default_connections()

    def _initialize_default_connections(self):
        """åˆå§‹åŒ–é»˜è®¤è¿æ¥é…ç½®"""
        if not settings.use_real_clusters:
            logger.info("Mockæ¨¡å¼ï¼šè·³è¿‡çœŸå®æ•°æ®åº“è¿æ¥åˆå§‹åŒ–")
            return

        # ä»æ•°æ®åº“åŠ è½½å·²ä¿å­˜çš„è¿æ¥é…ç½®
        self._load_saved_connections()

    def _load_saved_connections(self):
        """ä»æ•°æ®åº“åŠ è½½å·²ä¿å­˜çš„è¿æ¥é…ç½® - ä½¿ç”¨åŒæ­¥æ•°æ®åº“ä¼šè¯"""
        try:
            from app.utils.database import get_sync_db_session
            from app.models.data_source import DataSource
            import json

            # ä½¿ç”¨åŒæ­¥æ•°æ®åº“ä¼šè¯
            db = get_sync_db_session()
            try:
                saved_sources = db.query(DataSource).filter(DataSource.is_active == True).all()
                loaded_count = 0
                for source in saved_sources:
                    try:
                        if source.connection_config:
                            config = json.loads(source.connection_config) if isinstance(
                                source.connection_config, str) else source.connection_config
                            self.connection_manager.add_client(
                                source.name,
                                source.source_type,
                                config
                            )
                            logger.info(f"âœ… åŠ è½½å·²ä¿å­˜çš„æ•°æ®æº: {source.name}")
                            loaded_count += 1
                    except Exception as e:
                        logger.error(f"âŒ åŠ è½½æ•°æ®æº {source.name} å¤±è´¥: {e}")

                logger.info(f"ğŸ‰ æˆåŠŸåŠ è½½ {loaded_count} ä¸ªæ•°æ®æºè¿æ¥é…ç½®")

            finally:
                db.close()

        except Exception as e:
            logger.error(f"âŒ ä»æ•°æ®åº“åŠ è½½è¿æ¥é…ç½®å¤±è´¥: {e}")
            logger.info("ğŸ’¡ å°†ç»§ç»­å¯åŠ¨ï¼Œä½†éœ€è¦æ‰‹åŠ¨é…ç½®æ•°æ®æºè¿æ¥")

    @cache_table_schema(ttl=1800)  # 30åˆ†é’Ÿç¼“å­˜
    async def get_table_schema(self, source_name: str, table_name: str, database: str = None) -> Dict[str, Any]:
        """è·å–è¡¨ç»“æ„ - é•¿æœŸç¼“å­˜"""
        try:
            client = self.connection_manager.get_client(source_name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
                }

            schema = await client.get_table_schema(table_name, database)
            return {
                "success": True,
                "source_name": source_name,
                "database": database,
                "table_name": table_name,
                "schema": schema,
                "retrieved_at": datetime.now(),
                "cached": True
            }
        except Exception as e:
            logger.error(f"è·å–è¡¨ç»“æ„å¤±è´¥ {source_name}.{database}.{table_name}: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @cache_data_preview(ttl=300)  # 5åˆ†é’Ÿç¼“å­˜
    async def execute_query(self, source_name: str, query: str, database: str = None, schema: str = None,
                            limit: int = 100) -> Dict[str, Any]:
        """æ‰§è¡ŒæŸ¥è¯¢ - çŸ­æœŸç¼“å­˜"""
        try:
            client = self.connection_manager.get_client(source_name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
                }

            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æœ‰LIMITé™åˆ¶
            if limit and "limit" not in query.lower() and "select" in query.lower():
                if query.strip().endswith(';'):
                    query = query.strip()[:-1]
                query = f"{query} LIMIT {limit}"

            start_time = datetime.now()
            results = await client.execute_query(query, database, schema)
            end_time = datetime.now()

            return {
                "success": True,
                "source_name": source_name,
                "database": database,
                "schema": schema,
                "query": query,
                "results": results,
                "row_count": len(results),
                "execution_time_ms": int((end_time - start_time).total_seconds() * 1000),
                "executed_at": start_time,
                "cached": True
            }
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥ {source_name}: {e}")
            return {
                "success": False,
                "error": str(e),
                "query": query
            }

    async def get_data_sources_list(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ•°æ®æºåˆ—è¡¨ - æ··åˆç¼“å­˜ç­–ç•¥"""
        try:
            # ä»ç¼“å­˜è·å–åŸºç¡€åˆ—è¡¨
            cache_key = "sources_list"

            async def fetch_sources_data():
                return await self._fetch_sources_from_clients()

            sources = await self.cache_manager.get_cached_data(
                cache_key,
                fetch_sources_data,
                {'redis': 120}  # 2åˆ†é’Ÿç¼“å­˜
            )

            # å¼‚æ­¥æ›´æ–°æ•°æ®åº“ä¸­çš„ç»Ÿè®¡ä¿¡æ¯
            asyncio.create_task(self._update_sources_statistics(sources))

            return sources

        except Exception as e:
            logger.error(f"è·å–æ•°æ®æºåˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def _fetch_sources_from_clients(self) -> List[Dict[str, Any]]:
        """ä»å®¢æˆ·ç«¯è·å–æ•°æ®æºä¿¡æ¯"""
        clients = self.connection_manager.list_clients()

        if not clients and not settings.use_real_clusters:
            # è¿”å›Mockæ•°æ®
            return await self._get_mock_sources_list()

        # å¹¶è¡Œè·å–è¿æ¥çŠ¶æ€
        connection_results = await self._parallel_test_connections(clients)

        sources_list = []
        for name in clients:
            client = self.connection_manager.get_client(name)
            test_result = connection_results.get(name, {})

            source_info = {
                "name": name,
                "type": client.__class__.__name__.replace('Client', '').lower(),
                "status": "connected" if test_result.get('success') else "disconnected",
                "host": client.config.get('host', 'æœªçŸ¥'),
                "port": client.config.get('port', 0),
                "database": client.config.get('database', ''),
                "description": f"{test_result.get('database_type', 'æœªçŸ¥')} æ•°æ®åº“",
                "last_test": test_result.get('test_time', datetime.now()),
                "version": test_result.get('version', 'æœªçŸ¥'),
                "tables_count": 0  # å»¶è¿ŸåŠ è½½
            }

            if not test_result.get('success'):
                source_info["error"] = test_result.get('error', 'è¿æ¥å¤±è´¥')

            sources_list.append(source_info)

        return sources_list

    async def _get_mock_sources_list(self) -> List[Dict[str, Any]]:
        """è·å–Mockæ•°æ®æºåˆ—è¡¨"""
        return [
            {
                "name": "MySQL-Demo",
                "type": "mysql",
                "status": "connected",
                "host": "localhost",
                "port": 3306,
                "database": "demo",
                "description": "MySQLæ¼”ç¤ºæ•°æ®åº“",
                "tables_count": 15,
                "last_test": datetime.now() - timedelta(minutes=5),
                "created_at": datetime.now() - timedelta(days=7)
            },
            {
                "name": "Hive-Warehouse",
                "type": "hive",
                "status": "connected",
                "host": "hadoop101",
                "port": 10000,
                "database": "default",
                "description": "Hiveæ•°æ®ä»“åº“",
                "tables_count": 128,
                "last_test": datetime.now() - timedelta(minutes=3),
                "created_at": datetime.now() - timedelta(days=30)
            },
            # ... å…¶ä»–Mockæ•°æ®
        ]

    async def _update_sources_statistics(self, sources: List[Dict[str, Any]]):
        """å¼‚æ­¥æ›´æ–°æ•°æ®æºç»Ÿè®¡ä¿¡æ¯"""
        try:
            for source in sources:
                if source.get('status') == 'connected':
                    # å¼‚æ­¥è·å–è¡¨æ•°é‡ç­‰ç»Ÿè®¡ä¿¡æ¯
                    asyncio.create_task(self._update_single_source_stats(source['name']))
        except Exception as e:
            logger.error(f"æ›´æ–°æ•°æ®æºç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

    async def _update_single_source_stats(self, source_name: str):
        """æ›´æ–°å•ä¸ªæ•°æ®æºçš„ç»Ÿè®¡ä¿¡æ¯"""
        try:
            tables_result = await self.get_tables(source_name)
            if tables_result.get('success'):
                table_count = tables_result.get('count', 0)

                # æ›´æ–°æ•°æ®åº“è®°å½•
                db = next(get_db())
                data_source = db.query(DataSource).filter(DataSource.name == source_name).first()
                if data_source:
                    # è¿™é‡Œå¯ä»¥æ·»åŠ è¡¨æ•°é‡ç­‰ç»Ÿè®¡å­—æ®µåˆ°æ•°æ®æºæ¨¡å‹
                    # data_source.tables_count = table_count
                    db.commit()
                db.close()

        except Exception as e:
            logger.debug(f"æ›´æ–°æ•°æ®æºç»Ÿè®¡ä¿¡æ¯å¤±è´¥ {source_name}: {e}")

    async def batch_test_connections(self, source_names: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡æµ‹è¯•è¿æ¥ - é«˜æ•ˆå¹¶å‘ç‰ˆæœ¬"""
        try:
            # ä½¿ç”¨ç¼“å­˜å’Œå¹¶å‘å¤„ç†
            results = {}

            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…è¿‡è½½
            batch_size = 10
            for i in range(0, len(source_names), batch_size):
                batch = source_names[i:i + batch_size]
                batch_results = await self._parallel_test_connections(batch)
                results.update(batch_results)

            # ç»Ÿè®¡ç»“æœ
            total_tested = len(results)
            successful = sum(1 for result in results.values() if result.get('success'))
            failed = total_tested - successful

            # å¼‚æ­¥è®°å½•æµ‹è¯•ç»“æœ
            for name, result in results.items():
                asyncio.create_task(self._record_connection_test(name, result))

            summary = {
                "total_tested": total_tested,
                "successful": successful,
                "failed": failed,
                "success_rate": round((successful / total_tested) * 100, 1) if total_tested > 0 else 0,
                "test_results": results,
                "tested_at": datetime.now()
            }

            return summary
        except Exception as e:
            logger.error(f"æ‰¹é‡æµ‹è¯•è¿æ¥å¤±è´¥: {e}")
            raise

    async def get_health_status(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†æˆæ¨¡å—å¥åº·çŠ¶æ€"""
        try:
            # è·å–æ¦‚è§ˆä¿¡æ¯ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
            overview = await self.get_data_sources_overview()

            total_sources = overview.get('total_sources', 0)
            active_connections = overview.get('active_connections', 0)
            failed_connections = overview.get('failed_connections', 0)

            # è®¡ç®—å¥åº·åˆ†æ•°
            if total_sources == 0:
                health_score = 100
                status = "healthy"
            else:
                health_score = (active_connections / total_sources) * 100
                if health_score >= 90:
                    status = "healthy"
                elif health_score >= 70:
                    status = "warning"
                else:
                    status = "critical"

            # è·å–ç¼“å­˜ç»Ÿè®¡
            cache_stats = self.cache_manager.get_cache_stats()

            health_data = {
                "status": status,
                "health_score": round(health_score, 1),
                "total_sources": total_sources,
                "active_connections": active_connections,
                "failed_connections": failed_connections,
                "uptime_percentage": health_score,
                "last_check": datetime.now(),
                "issues": [],
                "cache_performance": cache_stats,
                "performance_metrics": {
                    "average_response_time": "< 100ms",
                    "cache_hit_rate": f"{cache_stats.get('memory_hit_rate', 0):.1f}%",
                    "concurrent_connections": len(self.connection_manager.list_clients())
                }
            }

            # æ·»åŠ é—®é¢˜æè¿°
            if failed_connections > 0:
                health_data["issues"].append(f"{failed_connections} ä¸ªæ•°æ®æºè¿æ¥å¤±è´¥")

            if total_sources == 0:
                health_data["issues"].append("æœªé…ç½®ä»»ä½•æ•°æ®æº")

            # æ£€æŸ¥ç¼“å­˜æ€§èƒ½
            if cache_stats.get('cache_miss_rate', 0) > 50:
                health_data["issues"].append("ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œå¯èƒ½å½±å“æ€§èƒ½")

            return health_data
        except Exception as e:
            logger.error(f"è·å–å¥åº·çŠ¶æ€å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e),
                "last_check": datetime.now()
            }

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜ï¼Œä½†å¯ä»¥æ·»åŠ é€‚å½“çš„ç¼“å­˜è£…é¥°å™¨
    async def get_databases(self, source_name: str) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“åˆ—è¡¨ - æ·»åŠ ç¼“å­˜"""
        cache_key = f"databases_{source_name}"

        async def fetch_databases():
            client = self.connection_manager.get_client(source_name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
                }

            databases = await client.get_databases()
            return {
                "success": True,
                "source_name": source_name,
                "databases": databases,
                "count": len(databases),
                "retrieved_at": datetime.now()
            }

        return await self.cache_manager.get_cached_data(
            cache_key,
            fetch_databases,
            {'redis': 600}  # 10åˆ†é’Ÿç¼“å­˜
        )

    async def get_tables(self, source_name: str, database: str = None) -> Dict[str, Any]:
        """è·å–è¡¨åˆ—è¡¨ - æ·»åŠ ç¼“å­˜"""
        cache_key = f"tables_{source_name}_{database or 'default'}"

        async def fetch_tables():
            client = self.connection_manager.get_client(source_name)
            if not client:
                return {
                    "success": False,
                    "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
                }

            tables = await client.get_tables(database)
            return {
                "success": True,
                "source_name": source_name,
                "database": database,
                "tables": tables,
                "count": len(tables),
                "retrieved_at": datetime.now()
            }

        return await self.cache_manager.get_cached_data(
            cache_key,
            fetch_tables,
            {'redis': 300}  # 5åˆ†é’Ÿç¼“å­˜
        )


# å…¨å±€ä¼˜åŒ–æœåŠ¡å®ä¾‹
optimized_data_integration_service = OptimizedDataIntegrationService()
cache_connection_status(ttl=60)


async def get_data_sources_overview(self) -> Dict[str, Any]:
    """è·å–æ•°æ®æºæ¦‚è§ˆ - è¿æ¥çœŸå®é›†ç¾¤"""
    try:
        clients = self.connection_manager.list_clients()

        # å¦‚æœæ²¡æœ‰é…ç½®çš„æ•°æ®æºï¼Œå…ˆåŠ è½½çœŸå®é›†ç¾¤è¿æ¥
        if not clients:
            await self._load_real_cluster_connections()
            clients = self.connection_manager.list_clients()

        # å¹¶è¡Œæµ‹è¯•æ‰€æœ‰è¿æ¥
        connection_results = await self._parallel_test_connections(clients)

        # ç»Ÿè®¡ç»“æœ
        total_sources = len(clients)
        active_connections = sum(1 for result in connection_results.values() if result.get('success'))
        failed_connections = total_sources - active_connections

        # æŒ‰ç±»å‹ç»Ÿè®¡
        sources_by_type = {}
        for client_name in clients:
            client = self.connection_manager.get_client(client_name)
            client_type = client.__class__.__name__.replace('Client', '').lower()
            sources_by_type[client_type] = sources_by_type.get(client_type, 0) + 1

        return {
            "total_sources": total_sources,
            "active_connections": active_connections,
            "failed_connections": failed_connections,
            "supported_types": DatabaseClientFactory.get_supported_types(),
            "sources_by_type": sources_by_type,
            "data_volume_estimate": await self._estimate_total_data_volume(clients),
            "last_sync": datetime.now(),
            "health_status": "è‰¯å¥½" if failed_connections == 0 else "éƒ¨åˆ†å¼‚å¸¸" if active_connections > 0 else "å¼‚å¸¸"
        }
    except Exception as e:
        logger.error(f"è·å–æ•°æ®æºæ¦‚è§ˆå¤±è´¥: {e}")
        # ç”Ÿäº§ç¯å¢ƒä¹Ÿè¦æœ‰å¤‡ç”¨æ•°æ®
        return await self._get_production_fallback_overview()


async def _parallel_test_connections(self, client_names: List[str]) -> Dict[str, Dict]:
    """å¹¶è¡Œæµ‹è¯•è¿æ¥"""

    async def test_single_connection(name):
        try:
            client = self.connection_manager.get_client(name)
            if client:
                return name, await client.test_connection()
            return name, {"success": False, "error": "Client not found"}
        except Exception as e:
            return name, {"success": False, "error": str(e)}

    semaphore = asyncio.Semaphore(5)  # é™åˆ¶å¹¶å‘æ•°

    async def test_with_semaphore(name):
        async with semaphore:
            return await test_single_connection(name)

    tasks = [test_with_semaphore(name) for name in client_names]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    return {name: result for name, result in results if not isinstance(result, Exception)}

async def _estimate_total_data_volume(self, client_names: List[str]) -> str:
    """ä¼°ç®—æ€»æ•°æ®é‡"""
    try:
        # å°è¯•ä»çœŸå®é›†ç¾¤è·å–æ•°æ®é‡
        from app.utils.hadoop_client import HDFSClient

        try:
            hdfs_client = HDFSClient()
            storage_info = hdfs_client.get_storage_info()
            if storage_info and storage_info.get('total_size', 0) > 0:
                total_gb = storage_info['total_size'] / (1024 ** 3)  # è½¬æ¢ä¸ºGB
                return f"{total_gb:.1f}GB"
        except:
            pass

        # å¤‡ç”¨ä¼°ç®—
        total_gb = len(client_names) * 50  # ç”Ÿäº§ç¯å¢ƒä¼°ç®—æ›´å¤§
        return f"{total_gb}GB"
    except:
        return "æœªçŸ¥"


async def _get_mock_overview(self) -> Dict[str, Any]:
    """è·å–Mockæ¦‚è§ˆæ•°æ®"""
    return {
        "total_sources": 6,
        "active_connections": 5,
        "failed_connections": 1,
        "supported_types": DatabaseClientFactory.get_supported_types(),
        "sources_by_type": {
            "mysql": 2,
            "hive": 1,
            "doris": 1,
            "kingbase": 1,
            "tidb": 1
        },
        "data_volume_estimate": "125.6GB",
        "last_sync": datetime.now(),
        "health_status": "è‰¯å¥½",
        "cache_info": self.cache_manager.get_cache_stats()
    }


async def add_data_source(self, name: str, db_type: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """æ·»åŠ æ•°æ®æº - æŒä¹…åŒ–åˆ°æ•°æ®åº“"""
    try:
        # éªŒè¯é…ç½®
        if db_type == "excel":
            required_fields = ['file_path']
        else:
            required_fields = ['host', 'username']

        missing_fields = [field for field in required_fields if field not in config]
        if missing_fields:
            return {
                "success": False,
                "error": f"ç¼ºå°‘å¿…è¦é…ç½®é¡¹: {', '.join(missing_fields)}"
            }

        # åˆ›å»ºå®¢æˆ·ç«¯
        self.connection_manager.add_client(name, db_type, config)

        # æµ‹è¯•è¿æ¥
        client = self.connection_manager.get_client(name)
        test_result = await client.test_connection()

        # ä¿å­˜åˆ°æ•°æ®åº“
        if test_result.get('success'):
            await self._save_data_source_to_db(name, db_type, config, test_result)

        # æ¸…é™¤ç›¸å…³ç¼“å­˜
        await self.cache_manager.invalidate_cache(pattern=name)
        await self.cache_manager.invalidate_cache(pattern="overview")

        return {
            "success": True,
            "name": name,
            "type": db_type,
            "test_result": test_result,
            "created_at": datetime.now()
        }
    except Exception as e:
        logger.error(f"æ·»åŠ æ•°æ®æºå¤±è´¥ {name}: {e}")
        return {
            "success": False,
            "error": str(e)
        }


async def _save_data_source_to_db(self, name: str, db_type: str, config: Dict[str, Any], test_result: Dict[str, Any]):
    """ä¿å­˜æ•°æ®æºé…ç½®åˆ°æ•°æ®åº“"""
    try:
        db = next(get_db())

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = db.query(DataSource).filter(DataSource.name == name).first()
        if existing:
            # æ›´æ–°ç°æœ‰è®°å½•
            existing.source_type = db_type
            existing.connection_config = json.dumps(config)
            existing.status = "online" if test_result.get('success') else "offline"
            existing.last_connection_test = datetime.now()
        else:
            # åˆ›å»ºæ–°è®°å½•
            data_source = DataSource(
                name=name,
                display_name=name,
                source_type=db_type,
                connection_config=json.dumps(config),
                status="online" if test_result.get('success') else "offline",
                is_active=True,
                last_connection_test=datetime.now()
            )
            db.add(data_source)

        # è®°å½•è¿æ¥æµ‹è¯•ç»“æœ
        connection_record = DataSourceConnection(
            data_source_id=existing.id if existing else None,
            connection_timestamp=datetime.now(),
            connection_type="initial_test",
            success=test_result.get('success', False),
            response_time_ms=test_result.get('response_time_ms', 0),
            error_message=test_result.get('error') if not test_result.get('success') else None
        )
        db.add(connection_record)

        db.commit()
        db.close()

    except Exception as e:
        logger.error(f"ä¿å­˜æ•°æ®æºåˆ°æ•°æ®åº“å¤±è´¥: {e}")


@cache_connection_status(ttl=30)
async def test_data_source(self, name: str) -> Dict[str, Any]:
    """æµ‹è¯•æ•°æ®æºè¿æ¥ - ç¼“å­˜ä¼˜åŒ–"""
    try:
        client = self.connection_manager.get_client(name)
        if not client:
            return {
                "success": False,
                "error": f"æ•°æ®æº {name} ä¸å­˜åœ¨"
            }

        result = await client.test_connection()

        # å¼‚æ­¥è®°å½•æµ‹è¯•ç»“æœåˆ°æ•°æ®åº“
        asyncio.create_task(self._record_connection_test(name, result))

        return result
    except Exception as e:
        logger.error(f"æµ‹è¯•æ•°æ®æºè¿æ¥å¤±è´¥ {name}: {e}")
        return {
            "success": False,
            "error": str(e),
            "test_time": datetime.now()
        }


async def _record_connection_test(self, name: str, test_result: Dict[str, Any]):
    """å¼‚æ­¥è®°å½•è¿æ¥æµ‹è¯•ç»“æœ"""
    try:
        db = next(get_db())
        data_source = db.query(DataSource).filter(DataSource.name == name).first()

        if data_source:
            # æ›´æ–°æ•°æ®æºçŠ¶æ€
            data_source.status = "online" if test_result.get('success') else "offline"
            data_source.last_connection_test = datetime.now()

            # è®°å½•è¿æ¥å†å²
            connection_record = DataSourceConnection(
                data_source_id=data_source.id,
                connection_timestamp=datetime.now(),
                connection_type="test",
                success=test_result.get('success', False),
                response_time_ms=test_result.get('response_time_ms', 0),
                error_message=test_result.get('error') if not test_result.get('success') else None
            )
            db.add(connection_record)
            db.commit()

        db.close()
    except Exception as e:
        logger.error(f"è®°å½•è¿æ¥æµ‹è¯•ç»“æœå¤±è´¥: {e}")


async def remove_data_source(self, name: str) -> Dict[str, Any]:
    """ç§»é™¤æ•°æ®æº - åŒæ—¶ä»å†…å­˜å’Œæ•°æ®åº“åˆ é™¤"""
    try:
        # ä»å†…å­˜ç§»é™¤
        if name not in self.connection_manager.list_clients():
            return {
                "success": False,
                "error": f"æ•°æ®æº {name} ä¸å­˜åœ¨"
            }

        self.connection_manager.remove_client(name)

        # ä»æ•°æ®åº“åˆ é™¤ï¼ˆè½¯åˆ é™¤ï¼‰
        db_success = await self._remove_data_source_from_db(name)

        # æ¸…é™¤ç›¸å…³ç¼“å­˜
        await self.cache_manager.invalidate_cache(pattern=name)
        await self.cache_manager.invalidate_cache(pattern="overview")

        return {
            "success": True,
            "message": f"æ•°æ®æº {name} å·²ç§»é™¤",
            "removed_from_memory": True,
            "removed_from_database": db_success
        }
    except Exception as e:
        logger.error(f"ç§»é™¤æ•°æ®æºå¤±è´¥ {name}: {e}")
        return {
            "success": False,
            "error": str(e)
        }


async def _remove_data_source_from_db(self, name: str) -> bool:
    """ä»æ•°æ®åº“åˆ é™¤æ•°æ®æºï¼ˆè½¯åˆ é™¤ï¼‰"""
    try:
        db = next(get_db())
        data_source = db.query(DataSource).filter(DataSource.name == name).first()

        if data_source:
            # è½¯åˆ é™¤ï¼šè®¾ç½®ä¸ºéæ´»è·ƒçŠ¶æ€
            data_source.is_active = False
            data_source.status = "deleted"
            db.commit()
            logger.info(f"æ•°æ®æº {name} å·²ä»æ•°æ®åº“è½¯åˆ é™¤")
            return True
        else:
            logger.warning(f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æ•°æ®æº {name}")
            return False

        db.close()

    except Exception as e:
        logger.error(f"ä»æ•°æ®åº“åˆ é™¤æ•°æ®æºå¤±è´¥: {e}")
        if 'db' in locals():
            db.rollback()
            db.close()
        return False


async def get_table_metadata(self, source_name: str, table_name: str, database: str = None) -> Dict[str, Any]:
    """è·å–è¡¨çš„å®Œæ•´å…ƒæ•°æ® - ç¼“å­˜ä¼˜åŒ–"""
    cache_key = f"metadata_{source_name}_{database or 'default'}_{table_name}"

    async def fetch_metadata():
        client = self.connection_manager.get_client(source_name)
        if not client:
            return {
                "success": False,
                "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
            }

        metadata = await client.get_table_metadata(table_name, database)
        return {
            "success": True,
            "source_name": source_name,
            "metadata": metadata,
            "retrieved_at": datetime.now()
        }

    return await self.cache_manager.get_cached_data(
        cache_key,
        fetch_metadata,
        {'redis': 1800}  # 30åˆ†é’Ÿç¼“å­˜
    )


async def search_tables(self, keyword: str = None, source_name: str = None, table_type: str = None) -> Dict[str, Any]:
    """æœç´¢è¡¨ - æ™ºèƒ½ç¼“å­˜"""
    cache_key = f"search_{keyword or 'all'}_{source_name or 'all'}_{table_type or 'all'}"

    async def fetch_search_results():
        all_tables = []
        clients_to_search = [source_name] if source_name else self.connection_manager.list_clients()

        # å¹¶è¡Œæœç´¢å¤šä¸ªæ•°æ®æº
        search_tasks = []
        for client_name in clients_to_search[:10]:  # é™åˆ¶å¹¶å‘æ•°
            search_tasks.append(self._search_single_source(client_name, keyword, table_type))

        search_results = await asyncio.gather(*search_tasks, return_exceptions=True)

        for result in search_results:
            if isinstance(result, list):
                all_tables.extend(result)

        return {
            "success": True,
            "tables": all_tables,
            "total_count": len(all_tables),
            "search_criteria": {
                "keyword": keyword,
                "source_name": source_name,
                "table_type": table_type
            },
            "searched_at": datetime.now()
        }

    return await self.cache_manager.get_cached_data(
        cache_key,
        fetch_search_results,
        {'redis': 600}  # 10åˆ†é’Ÿç¼“å­˜
    )


async def _search_single_source(self, client_name: str, keyword: str, table_type: str) -> List[Dict[str, Any]]:
    """æœç´¢å•ä¸ªæ•°æ®æº"""
    try:
        tables_result = await self.get_tables(client_name)
        if not tables_result.get('success'):
            return []

        tables = []
        for table in tables_result['tables']:
            table['source_name'] = client_name

            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            if keyword and keyword.lower() not in table['table_name'].lower():
                continue
            if table_type and table.get('table_type', '').lower() != table_type.lower():
                continue

            tables.append(table)

        return tables
    except Exception as e:
        logger.warning(f"æœç´¢æ•°æ®æº {client_name} æ—¶å‡ºé”™: {e}")
        return []


async def get_supported_database_types(self) -> List[Dict[str, Any]]:
    """è·å–æ”¯æŒçš„æ•°æ®åº“ç±»å‹ - é™æ€ç¼“å­˜"""
    cache_key = "supported_types"

    async def fetch_types():
        from app.utils.data_integration_clients import DatabaseClientFactory
        return DatabaseClientFactory.get_supported_types()

    types = await self.cache_manager.get_cached_data(
        cache_key,
        fetch_types,
        {'redis': 3600}  # 1å°æ—¶ç¼“å­˜
    )

    return types


async def preview_data_source(self, source_name: str, table_name: str = None, database: str = None, limit: int = 10) -> \
Dict[str, Any]:
    """é¢„è§ˆæ•°æ®æºæ•°æ®"""
    try:
        client = self.connection_manager.get_client(source_name)
        if not client:
            return {
                "success": False,
                "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
            }

        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¡¨åï¼Œè·å–ç¬¬ä¸€ä¸ªè¡¨
        if not table_name:
            tables_result = await self.get_tables(source_name, database)
            if not tables_result.get('success') or not tables_result.get('tables'):
                return {
                    "success": False,
                    "error": "æ²¡æœ‰å¯ç”¨çš„è¡¨è¿›è¡Œé¢„è§ˆ"
                }
            # è·å–ç¬¬ä¸€ä¸ªè¡¨çš„åç§°
            tables = tables_result['tables']
            if tables:
                table_name = tables[0]['table_name']  # ä¿®å¤ï¼šä»tablesç»“æœä¸­è·å–table_name
            else:
                return {
                    "success": False,
                    "error": "æ•°æ®æºä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨"
                }

        # æ‰§è¡ŒæŸ¥è¯¢é¢„è§ˆ
        query = f"SELECT * FROM {table_name} LIMIT {limit}"
        result = await self.execute_query(source_name, query, database, limit=limit)

        if result.get('success'):
            return {
                "success": True,
                "source_name": source_name,
                "table_name": table_name,
                "database": database,
                "preview_data": result.get('results', []),
                "row_count": len(result.get('results', [])),
                "limit": limit
            }
        else:
            return result

    except Exception as e:
        logger.error(f"é¢„è§ˆæ•°æ®æºå¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e)
        }


async def get_data_source_statistics(self, source_name: str) -> Dict[str, Any]:
    """è·å–æ•°æ®æºç»Ÿè®¡ä¿¡æ¯ - ç¼“å­˜ä¼˜åŒ–"""
    cache_key = f"statistics_{source_name}"

    async def fetch_statistics():
        # è·å–æ•°æ®åº“åˆ—è¡¨
        databases_result = await self.get_databases(source_name)
        if not databases_result.get('success'):
            return {
                "success": False,
                "error": databases_result.get('error', 'æ•°æ®æºä¸å­˜åœ¨')
            }

        databases = databases_result.get('databases', [])
        total_tables = 0
        tables_by_database = {}

        # å¹¶è¡Œç»Ÿè®¡æ¯ä¸ªæ•°æ®åº“çš„è¡¨æ•°é‡
        table_tasks = []
        for db in databases[:5]:  # é™åˆ¶å¹¶å‘æ•°ï¼Œåªç»Ÿè®¡å‰5ä¸ªæ•°æ®åº“
            table_tasks.append(self._count_tables_in_database(source_name, db))

        table_results = await asyncio.gather(*table_tasks, return_exceptions=True)

        for i, (db, result) in enumerate(zip(databases[:5], table_results)):
            if isinstance(result, dict) and result.get('success'):
                db_table_count = result.get('count', 0)
                total_tables += db_table_count
                tables_by_database[db] = db_table_count
            else:
                tables_by_database[db] = 0

        # æµ‹è¯•è¿æ¥çŠ¶æ€
        test_result = await self.test_data_source(source_name)

        statistics = {
            "source_name": source_name,
            "connection_status": "connected" if test_result.get('success') else "disconnected",
            "database_type": test_result.get('database_type', 'æœªçŸ¥'),
            "version": test_result.get('version', 'æœªçŸ¥'),
            "total_databases": len(databases),
            "total_tables": total_tables,
            "tables_by_database": tables_by_database,
            "last_test": test_result.get('test_time', datetime.now()),
            "response_time_ms": test_result.get('response_time_ms', 0),
            "collected_at": datetime.now()
        }

        if not test_result.get('success'):
            statistics["error"] = test_result.get('error', 'è¿æ¥å¤±è´¥')

        return {
            "success": True,
            **statistics
        }

    return await self.cache_manager.get_cached_data(
        cache_key,
        fetch_statistics,
        {'redis': 900}  # 15åˆ†é’Ÿç¼“å­˜
    )


async def _count_tables_in_database(self, source_name: str, database: str) -> Dict[str, Any]:
    """ç»Ÿè®¡å•ä¸ªæ•°æ®åº“çš„è¡¨æ•°é‡"""
    try:
        tables_result = await self.get_tables(source_name, database)
        return tables_result
    except Exception as e:
        return {"success": False, "error": str(e), "count": 0}


# Excelç›¸å…³æ–¹æ³•
async def upload_excel_source(self, name: str, file, description: str = None) -> Dict[str, Any]:
    """ä¸Šä¼ Excelæ–‡ä»¶åˆ›å»ºæ•°æ®æº"""
    try:
        from app.utils.data_integration_clients import excel_service

        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not file.filename.lower().endswith(('.xlsx', '.xls')):
            return {
                "success": False,
                "error": "ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œè¯·ä¸Šä¼ .xlsxæˆ–.xlsæ–‡ä»¶"
            }

        # éªŒè¯æ–‡ä»¶å¤§å° (é™åˆ¶ä¸º50MB)
        content = await file.read()
        file_size = len(content)
        await file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ

        if file_size > 50 * 1024 * 1024:  # 50MB
            return {
                "success": False,
                "error": "æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼ˆ50MBï¼‰"
            }

        # åˆ›å»ºExcelæ•°æ®æº
        result = await excel_service.create_excel_source(name, file, description)

        if result.get('success'):
            # æ·»åŠ åˆ°è¿æ¥ç®¡ç†å™¨
            config = result['config']
            add_result = await self.add_data_source(
                name=name,
                db_type='excel',
                config=config
            )

            if add_result.get('success'):
                # æ¸…é™¤ç›¸å…³ç¼“å­˜
                await self.cache_manager.invalidate_cache(pattern="overview")

                return {
                    "success": True,
                    "source_info": add_result,
                    "upload_info": result['upload_info']
                }
            else:
                # å¦‚æœæ·»åŠ æ•°æ®æºå¤±è´¥ï¼Œåˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶
                await excel_service.delete_excel_source(config)
                return {
                    "success": False,
                    "error": add_result.get('error', 'åˆ›å»ºæ•°æ®æºå¤±è´¥')
                }
        else:
            return result

    except Exception as e:
        logger.error(f"ä¸Šä¼ Excelæ–‡ä»¶å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e)
        }


async def list_excel_files(self) -> List[Dict[str, Any]]:
    """è·å–å·²ä¸Šä¼ çš„Excelæ–‡ä»¶åˆ—è¡¨"""
    try:
        upload_dir = Path(settings.UPLOAD_DIR)
        if not upload_dir.exists():
            upload_dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç›®å½•å¦‚æœä¸å­˜åœ¨
            return []

        excel_files = []
        # æœç´¢Excelæ–‡ä»¶
        for pattern in ["*.xlsx", "*.xls"]:
            for file_path in upload_dir.glob(pattern):
                try:
                    stat = file_path.stat()
                    excel_files.append({
                        "filename": file_path.name,
                        "size": stat.st_size,
                        "size_mb": round(stat.st_size / (1024 * 1024), 2),
                        "modified": datetime.fromtimestamp(stat.st_mtime),
                        "path": str(file_path),
                        "extension": file_path.suffix
                    })
                except Exception as e:
                    logger.warning(f"è¯»å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path}: {e}")
                    continue

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        excel_files.sort(key=lambda x: x['modified'], reverse=True)
        return excel_files

    except Exception as e:
        logger.error(f"è·å–Excelæ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return []


async def get_excel_sheets(self, source_name: str) -> Dict[str, Any]:
    """è·å–Excelå·¥ä½œè¡¨åˆ—è¡¨"""
    try:
        result = await self.get_tables(source_name)
        return result
    except Exception as e:
        logger.error(f"è·å–Excelå·¥ä½œè¡¨åˆ—è¡¨å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e)
        }


async def preview_excel_sheet(self, source_name: str, sheet_name: str, limit: int = 10) -> Dict[str, Any]:
    """é¢„è§ˆExcelå·¥ä½œè¡¨æ•°æ®"""
    try:
        client = self.connection_manager.get_client(source_name)
        if not client:
            return {
                "success": False,
                "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
            }

        # æ£€æŸ¥æ˜¯å¦ä¸ºExcelå®¢æˆ·ç«¯
        if not hasattr(client, 'get_data_preview'):
            return {
                "success": False,
                "error": "è¯¥æ•°æ®æºä¸æ”¯æŒé¢„è§ˆåŠŸèƒ½"
            }

        result = await client.get_data_preview(sheet_name, limit)
        return result
    except Exception as e:
        logger.error(f"é¢„è§ˆExcelå·¥ä½œè¡¨å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e)
        }


async def delete_excel_source(self, source_name: str) -> Dict[str, Any]:
    """åˆ é™¤Excelæ•°æ®æºå’Œå¯¹åº”çš„æ–‡ä»¶"""
    try:
        client = self.connection_manager.get_client(source_name)
        if not client:
            return {
                "success": False,
                "error": f"æ•°æ®æº {source_name} ä¸å­˜åœ¨"
            }

        # è·å–æ–‡ä»¶é…ç½®
        config = client.config if hasattr(client, 'config') else {}

        # åˆ é™¤æ–‡ä»¶
        from app.utils.data_integration_clients import excel_service
        delete_result = await excel_service.delete_excel_source(config)

        # åˆ é™¤æ•°æ®æº
        source_result = await self.remove_data_source(source_name)

        return {
            "success": True,
            "file_deleted": delete_result.get('success', False),
            "source_deleted": source_result.get('success', False),
            "file_message": delete_result.get('message', ''),
            "source_message": source_result.get('message', '')
        }
    except Exception as e:
        logger.error(f"åˆ é™¤Excelæ•°æ®æºå¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e)
        }


async def export_excel_sheet(self, source_name: str, sheet_name: str, export_format: str = "json", limit: int = None) -> \
Dict[str, Any]:
    """å¯¼å‡ºExcelå·¥ä½œè¡¨æ•°æ®"""
    try:
        # æ„å»ºæŸ¥è¯¢
        query = f"SELECT * FROM {sheet_name}"
        if limit:
            query += f" LIMIT {limit}"

        result = await self.execute_query(
            source_name=source_name,
            query=query,
            limit=limit or 1000
        )

        if result.get('success'):
            data = result['results']

            return {
                "success": True,
                "sheet_name": sheet_name,
                "export_format": export_format,
                "data": data,
                "row_count": len(data),
                "exported_at": datetime.now()
            }
        else:
            return result

    except Exception as e:
        logger.error(f"å¯¼å‡ºExcelå·¥ä½œè¡¨å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e)
        }

async def _load_real_cluster_connections(self):
    """åŠ è½½çœŸå®é›†ç¾¤è¿æ¥"""
    try:
        # åŠ è½½Hiveè¿æ¥
        if settings.HIVE_SERVER_HOST:
            hive_config = {
                "host": settings.HIVE_SERVER_HOST,
                "port": settings.HIVE_SERVER_PORT,
                "username": settings.HIVE_USERNAME,
                "password": settings.HIVE_PASSWORD,
                "database": settings.HIVE_DATABASE or "default"
            }
            self.connection_manager.add_client("Production-Hive", "hive", hive_config)
            logger.info("âœ… åŠ è½½Hiveç”Ÿäº§è¿æ¥")

        # åŠ è½½HDFSè¿æ¥ï¼ˆå¦‚æœæœ‰å®¢æˆ·ç«¯æ”¯æŒï¼‰
        if settings.HDFS_NAMENODE:
            hdfs_config = {
                "namenode": settings.HDFS_NAMENODE,
                "user": settings.HDFS_USER
            }
            # å¦‚æœæœ‰HDFSå®¢æˆ·ç«¯å®ç°ï¼Œåœ¨è¿™é‡Œæ·»åŠ 
            logger.info("âœ… HDFSé…ç½®å·²è¯»å–")

    except Exception as e:
        logger.error(f"åŠ è½½çœŸå®é›†ç¾¤è¿æ¥å¤±è´¥: {e}")


async def _get_production_fallback_overview(self) -> Dict[str, Any]:
    """ç”Ÿäº§ç¯å¢ƒå¤‡ç”¨æ¦‚è§ˆæ•°æ®"""
    return {
        "total_sources": 2,
        "active_connections": 1,
        "failed_connections": 1,
        "supported_types": DatabaseClientFactory.get_supported_types(),
        "sources_by_type": {"hive": 1, "hdfs": 1},
        "data_volume_estimate": "100.0GB",
        "last_sync": datetime.now(),
        "health_status": "éƒ¨åˆ†å¼‚å¸¸",
        "error": "éƒ¨åˆ†æ•°æ®æºè¿æ¥å¤±è´¥",
        "cluster_info": {
            "hive_host": settings.HIVE_SERVER_HOST,
            "hdfs_namenode": settings.HDFS_NAMENODE
        }
    }